[
    {
        "Y": "True or False",
        "X": "What does is_tensor return ?",
        "Z": "is_tensor Returns True if objis a PyTorch tensor."
    },
    {
        "Y": "True",
        "X": "Is_tensor returns true or false if obj is  a PyTorch tensor?",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor."
    },
    {
        "Y": "True or False",
        "X": "Is_tensor Returns what?",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor."
    },
	{
        "Y": "True or False",
        "X": " What  does Is_tensor Return ?",
        "Z": "is_tensor Returns True if obj is  a PyTorch tensor."
    },
	 {
        "Y": "True",
        "X": "Is_storage Returns True or False if obj is  a PyTorch storage object?",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object."
    },
    {
        "Y": " if the  object is a PyTorch storage object.",
        "X": "When is_storage return True?",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object."
    },
    {
        "Y": "True or False",
        "X": " What does Is_storage Return?",
        "Z": "is_storage Returns True if obj is  a PyTorch storage object."
    },
    {
        "Y": "is_complex Returns True",
        "X": "What happens if the data type of input is a complex data type?",
        "Z": "is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128."
    },
    {
        "Y": "is_complex",
        "X": "What Returns True if the data type of input is a complex data type?",
        "Z": "is_complex Returns True if the data type of input is a complex data type i.e., one of torch.complex64, andtorch.complex128."
    },
    {
        "Y": "is_floating_point Returns True",
        "X": "What happens if the data type of input is a floating point data type?",
        "Z": "is_floating_point Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16."
    },
    {
        "Y": "is_floating_point",
        "X": "What Returns True if the data type of input is a floating point data type?",
        "Z": "is_floating_point Returns True if the data type of input is a floating point data type i.e., one of torch.float64,torch.float32,torch.float16, andtorch.bfloat16."
    },
    {
        "Y": "is_nonzero Returns True",
        "X": "What happens if the input is a single element tensor which is not equal to zero after type conversions?",
        "Z": "is_nonzero Returns True if the input is a single element tensor which is not equal to zero after type conversions."
    },
    {
        "Y": "is_nonzero",
        "X": "What Returns True if the input is a single element tensor which is not equal to zero after type conversions?",
        "Z": "is_nonzero Returns True if the input is a single element tensor which is not equal to zero after type conversions."
    },
    {
        "Y": "set_default_dtype",
        "X": "What sets the default floating point dtype to d?",
        "Z": "set_default_dtype Sets the default floating point dtype to d."
    },
    {
        "Y": "use  get_default_dtype",
        "X": "How to get  the current default floating point torch.dtype?",
        "Z": "get_default_dtype Get the current default floating point torch.dtype."
    },
    {
        "Y": "get_default_dtype",
        "X": "What is function that returns the current floating point torch.dtype?",
        "Z": "get_default_dtype Get the current default floating point torch.dtype."
    },
    {
        "Y": "get_default_dtype",
        "X": "What is fuction to  get current floating point torch.dtype?",
        "Z": "get_default_dtype Get the current default floating point torch.dtype."
    },
    {
        "Y": "set_default_tensor_type",
        "X": "What sets the default torch.Tensor type to floating point tensor type t?",
        "Z": "set_default_tensor_type Sets the default torch.Tensor type to floating point tensor type t."
    },
    {
        "Y": "floating point",
        "X": "What  type of  Tensor Type  does  set_default_tensor_type Sets the default torch.Tensor type to?",
        "Z": "set_default_tensor_type Sets the default torch.Tensor type to floating point tensor type t."
    },
    {
        "Y": "the input tensor",
        "X": "numel Returns the total number of elements in what?",
        "Z": "numel Returns the total number of elements in the input tensor."
    },
    {
        "Y": "numel",
        "X": "What returns the total number of elements in the input tensor?",
        "Z": "numel Returns the total number of elements in the input tensor."
    },
    {
        "Y": "set_printoptions",
        "X": "What is method  that sets options for printing?",
        "Z": "set_printoptions Set options for printing."
    },
    {
        "Y": "set_flush_denormal",
        "X": "What disables denormal floating numbers on the CPU?",
        "Z": "set_flush_denormal Disables denormal floating numbers on CPU."
    },
    {
        "Y": "set_flush_denormal Disables denormal floating numbers on CPU.",
        "X": "What does  set_flush_denormal do",
        "Z": "set_flush_denormal Disables denormal floating numbers on CPU."
    },
    {
        "Y": "tensor Constructs a tensor with data.",
        "X": "What is the use of tensor?",
        "Z": "tensor Constructs a tensor withdata."
    },
    {
        "Y": "tensor",
        "X": " what is  the method  to Constructs a tensor withdata?",
        "Z": "tensor Constructs a tensor withdata."
    },
    {
        "Y": "a sparse tensor",
        "X": "What does sparse_coo_tensor construct?",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices."
    },
    {
        "Y": "sparse_coo_tensor",
        "X": "What Constructs asparse tensor in COO(rdinate) format?",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices."
    },
    {
        "Y": "COO(rdinate)",
        "X": "In what format does sparse_coo_tensor construct asparse tensor?",
        "Z": "sparse_coo_tensor Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices."
    },
    {
        "Y": "a torch.Tensor",
        "X": "What does as_tensor convert the data into?",
        "Z": "as_tensor Convert the data into a torch.Tensor."
    },
    {
        "Y": "as_tensor",
        "X": "What converts the data into a torch.Tensor?",
        "Z": "as_tensor Convert the data into a torch.Tensor."
    },
    {
        "Y": "as_strided",
        "X": "What type of view creates a view of an existingtorch.Tensorinput with specifiedsize,stride andstor",
        "Z": "as_strided Create a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_offset."
    },
    {
        "Y": "as_strided",
        "X": "What creates a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_off",
        "Z": "as_strided Create a view of an existingtorch.Tensorinput with specifiedsize,strideandstorage_offset."
    },
    {
        "Y": "a numpy.ndarray",
        "X": "What does from_numpy create a Tensor from?",
        "Z": "from_numpy Creates a Tensor froma numpy.ndarray."
    },
    {
        "Y": "a Tensor",
        "X": "What does from_numpy create?",
        "Z": "from_numpy Creates a Tensor froma numpy.ndarray."
    },
    {
        "Y": "zeros_like",
        "X": "What returns a tensor filled with the scalar value 0?",
        "Z": "zeros_like Returns a tensor filled with the scalar value 0, with the same size as input."
    },
    {
        "Y": "variable argument size",
        "X": "What defines the shape of the tensor?",
        "Z": "ones Returns a tensor filled with the scalar value 1  , with the shape defined by the variable argument size."
    },
    {
        "Y": "a tensor filled with the scalar value 0",
        "X": "What does zeros_like return?",
        "Z": "zeros_like Returns a tensor filled with the scalar value 0, with the same size as input."
    },
    {
        "Y": "tensor filled with the scalar value 1",
        "X": "what type of function does one return?",
        "Z": "ones Returns a tensor filled with the scalar value 1  , with the shape defined by the variable argument size."
    },
    {
        "Y": "a tensor",
        "X": "what does ones Return return?",
        "Z": "ones Returns a tensor filled with the scalar value 1  , with the shape defined by the variable argument size."
    },
    {
        "Y": "a tensor",
        "X": "What does one return that is filled with the scalar value 1  ?",
        "Z": "ones_like Returns a tensor filled with the scalar value 1  , with the same size as input."
    },
    {
        "Y": "a tensor",
        "X": "What does ones_like return?",
        "Z": "ones_like Returns a tensor filled with the scalar value 1  , with the same size as input."
    },
    {
        "Y": "a 1-D tensor",
        "X": "What does arange return?",
        "Z": "arange Returns a 1-D tensor of size  ceilling((end -start) /step) with values from the interval[start,end)taken with common difference step beginning from start."
    },
    {
        "Y": " a 1-D tensor",
        "X": "What is the use of  ops range?",
        "Z": "range Returns a 1-D tensor of size  ceilling((end -start) /step)  +1 with values from start to end with step step."
    },
    {
        "Y": "a 1-D tensor",
        "X": " What  does range Return?",
        "Z": "range Returns a 1-D tensor of size  ceilling((end -start) /step)  +1 with values from start to end with step step."
    },
    {
        "Y": "logspace",
        "X": "What creates a one-dimensional tensor of size steps?",
        "Z": "logspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from base to the power start  to  base to the power end, inclusive, on a logarithmic scale with base base."
    },
    {
        "Y": "linspace",
        "X": "What creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end?",
        "Z": "linspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive."
    },
    {
        "Y": "basebase",
        "X": "What is logarithmic scale used to create a one-dimensional tensor of size steps?",
        "Z": "logspace Creates a one-dimensional tensor of size steps whose values are evenly spaced from base to the power start  to  base to the power end, inclusive, on a logarithmic scale with base base."
    },
    {
        "Y": "eye",
        "X": "What returns a 2-D tensor with ones on the diagonal and zeros elsewhere?",
        "Z": "eye Returns a 2-D tensor with ones on the diagonal and zeros elsewhere."
    },
    {
        "Y": "uninitialized data",
        "X": "empty Returns a tensor filled with what?",
        "Z": "empty Returns a tensor filled with uninitialized data."
    },
    {
        "Y": "empty_strided",
        "X": "What returns a tensor filled with uninitialized data?",
        "Z": "empty_strided Returns a tensor filled with uninitialized data."
    },
	{
        "Y": "empty_strided Returns a tensor filled with uninitialized data.",
        "X": "What does empty_strided Return ?",
        "Z": "empty_strided Returns a tensor filled with uninitialized data."
    },
    {
        "Y": "empty_like",
        "X": "What returns an uninitialized tensor with the same size as input?",
        "Z": "empty_like Returns an uninitialized tensor with the same size as input."
    },
    {
        "Y": "empty_like",
        "X": "Returns an uninitialized tensor with the same size as input?",
        "Z": "empty_like Returns an uninitialized tensor with the same size as input."
    },
    {
        "Y": "uninitialized data",
        "X": "What type of data does empty_strided return?",
        "Z": "empty_strided Returns a tensor filled with uninitialized data."
    },
    {
        "Y": "uninitialized data",
        "X": "empty_strided Returns a tensor filled with what?",
        "Z": "empty_strided Returns a tensor filled with uninitialized data."
    },
    {
        "Y": "with fill_value",
        "X": "What does full create a tensor of?",
        "Z": "full Creates a tensor of  size filled with fill_value."
    },
    {
        "Y": "tensor",
        "X": "What is the  size filled with fill_value?",
        "Z": "full Creates a tensor of  size filled with fill_value."
    },
	{
        "Y": "full",
        "X": "What does Create a tensor of  size filled with fill_value.?",
        "Z": "full Creates a tensor of  size filled with fill_value."
    },
    {
        "Y": "full_like",
        "X": "What returns a tensor with the same size as input filled with fill_value?",
        "Z": "full_like Returns a tensor with the same size as input filled with fill_value."
    },
    {
        "Y": "tensor",
        "X": "full_like Returns a what with the same size as input filled with fill_value?",
        "Z": "full_like Returns a tensor with the same size as input filled with fill_value."
    },
    {
        "Y": "float tensor",
        "X": "What type of tensor does quantize_per_tensor convert to?",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point."
    },
    {
        "Y": "zero point",
        "X": "What is the scale of a quantized tensor?",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point."
    },
    {
        "Y": "quantized tensor with given scale and zero point.",
        "X": "What type of tensor does quantize_per_tensor convert a float tensor to?",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point."
    },
    {
        "Y": "float",
        "X": "What type of tensor does quantize_per_tensor convert?",
        "Z": "quantize_per_tensor Converts a float tensor to a quantized tensor with given scale and zero point."
    },
    {
        "Y": "per-channel quantized tensor with given scales and zero points.",
        "X": "Quantize_per_channel Converts a float tensor to what?",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points."
    },
    {
        "Y": "zero points",
        "X": "What is the value of the scales used to convert a float tensor to a per-channel quantized tensor",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points."
    },
    {
        "Y": "quantized tensor with given scales and zero points.",
        "X": "What does quantize_per_channel convert a float tensor to?",
        "Z": "quantize_per_channel Converts a float tensor to a per-channel quantized tensor with given scales and zero points."
    },
    {
        "Y": "dequantize",
        "X": "What returns an fp32 Tensor by dequantizing a quantized Tensor?",
        "Z": "dequantize Returns an fp32 Tensor by dequantizing a quantized Tensor"
    },
    {
        "Y": "real part",
        "X": "what part of a complex tensor is equal to real?",
        "Z": "complex Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag."
    },
    {
        "Y": "imaginary part",
        "X": "what part of a complex tensor is equal to imag?",
        "Z": "complex Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag."
    },
    {
        "Y": "Cartesian coordinates",
        "X": "What are the elements of polar Construct a complex tensor?",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle."
    },
    {
        "Y": "polar Constructs",
        "X": "What is a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle."
    },
    {
        "Y": "Cartesian coordinates",
        "X": "What are the elements of polar Constructs a complex tensor?",
        "Z": "polar Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle."
    },
    {
        "Y": "Heaviside step function",
        "X": "What does heaviside compute for each element in input?",
        "Z": "heaviside Computes the Heaviside step function for each element in input."
    },
    {
        "Y": "heaviside",
        "X": "What Computes the Heaviside step function for each element in input?",
        "Z": "heaviside Computes the Heaviside step function for each element in input."
    },
    {
        "Y": "cat",
        "X": "What concatenates the given sequence of seq tensors in the given dimension?",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension."
    },
    {
        "Y": "given sequence of seq tensors ",
        "X": "What does cat concatenate in a given dimension?",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension."
    },
    {
        "Y": "given dimension",
        "X": "cat Concatenates the given sequence of seq tensors in what?",
        "Z": "cat Concatenates the given sequence of seq tensors in the given dimension."
    },
    {
        "Y": "Splits a tensor into a specific number of chunks",
        "X": "What does a chunk do?",
        "Z": "chunk Splits a tensor into a specific number of chunks."
    },
    {
        "Y": "chunk ",
        "X": "What splits a tensor into a specific number of chunks?",
        "Z": "chunk Splits a tensor into a specific number of chunks."
    },
    {
        "Y": "indices_or_sections",
        "X": "According to What ,does dsplit Split input's depthwise tensors ?",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections."
    },
    {
        "Y": "three or more dimensions",
        "X": "How many dimensions does dsplit Splits input have?",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections."
    },
    {
        "Y": "indices",
        "X": "dsplit Splits input is divided into multiple tensors according to what?",
        "Z": "dsplit Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections."
    },
    {
        "Y": "column_stack",
        "X": "What creates a new tensor by horizontally stacking the tensors in tensors?",
        "Z": "column_stack Creates a new tensor by horizontally stacking the tensors in tensors."
    },
    {
        "Y": "horizontally stacking",
        "X": "How does column_stack create a new tensor?",
        "Z": "column_stack Creates a new tensor by horizontally stacking the tensors in tensors."
    },
    {
        "Y": "third axis",
        "X": "Along what axis are dstack Stack tensors in sequence depthwise?",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis)."
    },
    {
        "Y": "dstack ",
        "X": "What is  method  that  stack tensors in sequence depthwise?",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis)."
    },
    {
        "Y": "dstack Stack tensors in sequence depthwise",
        "X": "What does dstack do?",
        "Z": "dstack Stack tensors in sequence depthwise (along third axis)."
    },
    {
        "Y": "Gathers values along an axis specified bydim",
        "X": "What does gather do?",
        "Z": "gather Gathers values along an axis specified bydim."
    },
    {
        "Y": "bydim",
        "X": "What is the axis specified by gather?",
        "Z": "gather Gathers values along an axis specified bydim."
    },
    {
        "Y": "gather",
        "X": "What gathers values along an axis specified bydim?",
        "Z": "gather Gathers values along an axis specified bydim."
    },
    {
        "Y": "hsplit ",
        "X": "What is the method  that  splits a tensor horizontally?",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections."
    },
    {
        "Y": "A tensr with one or more dimensions",
        "X": "What does hsplit Splits input have?",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections."
    },
    {
        "Y": "horizontally into multiple tensors",
        "X": "How does hsplit Splits input split into multiple tensors?",
        "Z": "hsplit Splits input, a tensor with one or more dimensions, into multiple tensors horizontally according to indices_or_sections."
    },
    {
        "Y": "horizontally",
        "X": "How are hstack Stack tensors sequenced?",
        "Z": "hstack Stack tensors in sequence horizontally (column wise)."
    },
    {
        "Y": "horizontally",
        "X": "In what way hstack Stack tensors in sequence ?",
        "Z": "hstack Stack tensors in sequence horizontally (column wise)."
    },
    {
        "Y": "index_select",
        "X": "What returns a new tensor which indexes the input tensor along dimension dim using the entries inindex?",
        "Z": "index_select Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a Long Tensor."
    },
    {
        "Y": "masked_select",
        "X": "What returns a new 1-D tensor which indexes the input tensor according to the boolean mask?",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor ."
    },
    {
        "Y": "1-D tensor which indexes the input tensor according to the boolean mask ",
        "X": "What type of tensor does masked_select return?",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor ."
    },
    {
        "Y": "a Bool Tensor ",
        "X": "What is the boolean mask?",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor ."
    },
    {
        "Y": "1-D tensor",
        "X": "What does masked_select return?",
        "Z": "masked_select Returns a new 1-D tensor which indexes the input tensor according to the booleanmask which using is a Bool Tensor ."
    },
    {
        "Y": "Alias for torch.movedim",
        "X": "What is moveaxis?",
        "Z": "moveaxis Alias for torch.movedim()."
    },
    {
        "Y": "moveaxis",
        "X": "What is the Alias for torch.movedim()?",
        "Z": "moveaxis Alias for torch.movedim()."
    },
    {
        "Y": "narrow",
        "X": "What returns a new tensor that is a narrowed version of input tensor?",
        "Z": "narrow Returns a new tensor that is a narrowed version of input tensor."
    },
    {
        "Y": "a new tensor",
        "X": "What is a narrowed version of input tensor?",
        "Z": "narrow Returns a new tensor that is a narrowed version of input tensor."
    },
    {
        "Y": "tensor with the same data and number of elements as input, but with the specified shape.",
        "X": "What type of return does reshape return?",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape."
    },
    {
        "Y": "reshape",
        "X": "What returns a tensor with the same data and number of elements as input?",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape."
    },
    {
        "Y": "a tensor",
        "X": "What does reshape return with the same data and number of elements as input?",
        "Z": "reshape Returns a tensor with the same data and number of elements as input, but with the specified shape."
    },
    {
        "Y": "row_stack",
        "X": "What is the Alias of torch.vstack()?",
        "Z": "row_stack Alias of torch.vstack()."
    },
    {
        "Y": "Alias of torch.vstack",
        "X": "What is row_stack?",
        "Z": "row_stack Alias of torch.vstack()."
    },
    {
        "Y": "row_stack",
        "X": "What is the Alias for torch.vstack()?",
        "Z": "row_stack Alias of torch.vstack()."
    },
    {
        "Y": "Out-of-place version of torch.Tensor.scatter_()",
        "X": "What is out-of-place version of torch.Tensor.scatter_()?",
        "Z": "scatter Out-of-place version of torch.Tensor.scatter_()"
    },
    {
        "Y": "scatter Out-of-place",
        "X": "What is a version of of torch.Tensor.scatter_()?",
        "Z": "scatter Out-of-place version of torch.Tensor.scatter_()"
    },
    {
        "Y": "Out-of-place version of torch.Tensor.scatter_add_()",
        "X": "What is out-of-place version of torch.Tensor.scatter_add_()?",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()"
    },
    {
        "Y": "scatter_add Out-of-place",
        "X": "What version of of torch.Tensor.scatter_add_()?",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()"
    },
    {
        "Y": "scatter_add",
        "X": "What is the Out-of-place version of of torch.Tensor.scatter_add_()?",
        "Z": "scatter_add Out-of-place version of torch.Tensor.scatter_add_()"
    },
    {
        "Y": "split Splits the tensor into chunks",
        "X": "What does split do to a tensor?",
        "Z": "split Splits the tensor into chunks."
    },
    {
        "Y": "split",
        "X": "What splits the tensor into chunks?",
        "Z": "split Splits the tensor into chunks."
    },
    {
        "Y": "chunks",
        "X": "split Splits the tensor into what?",
        "Z": "split Splits the tensor into chunks."
    },
    {
        "Y": "size 1",
        "X": "The tensor returns a tensor with all the dimensions of input of what?",
        "Z": "squeeze Returns a tensor with all the dimensions of input of size 1 removed."
    },
    {
        "Y": "a tensor",
        "X": "What does squeeze Returns return?",
        "Z": "squeeze Returns a tensor with all the dimensions of input ofsize 1 removed."
    },
    {
        "Y": "stack",
        "X": "What Concatenates a sequence of tensors along a new dimension?",
        "Z": "stack Concatenates a sequence of tensors along a new dimension."
    },
    {
        "Y": "tensors",
        "X": "stack Concatenates a sequence of what?",
        "Z": "stack Concatenates a sequence of tensors along a new dimension."
    },
    {
        "Y": "stack",
        "X": "What concatenates a sequence of tensors along a new dimension?",
        "Z": "stack Concatenates a sequence of tensors along a new dimension."
    },
    {
        "Y": "swapdims",
        "X": "What is Alias for torch.transpose()?",
        "Z": "swapdims Alias for torch.transpose()."
    },
    {
        "Y": "Alias for torch.transpose()",
        "X": "What is  swapaxes?",
        "Z": "swapaxes Alias for torch.transpose()."
    },
    {
        "Y": "swapdims  is Alias for torch.transpose()",
        "X": "What does swapdims do?",
        "Z": "swapdims Alias for torch.transpose()."
    },
    {
        "Y": " Alias for torch.transpose()",
        "X": "What does swapdims do?",
        "Z": "swapdims Alias for torch.transpose()."
    },
    {
        "Y": "2-D tensor",
        "X": " what  input  tensor does   t Expect o be and transposes dimensions 0 and 1?",
        "Z": "t Expects input to be <= 2-D tensor and transposes dimensions 0 and 1."
    },
    {
        "Y": "the given indices",
        "X": "What are the elements of input to  ake?",
        "Z": "take Returns a new tensor with the elements of input at  the given indices."
    },
    {
        "Y": "1-dimensional indices",
        "X": "Take_along_dim Selects values from what indices from the given dim?",
        "Z": "take_along_dim Selects values from input at the 1-dimensional indices from indices along the given dim."
    },
    {
        "Y": "take_along_dim",
        "X": "What selects values from input at the 1-dimensional indices from indices along the given dim?",
        "Z": "take_along_dim Selects values from input at the 1-dimensional indices from indices along the given dim."
    },
    {
        "Y": "views of input tensor  where tensor_split can be applied",
        "X": "What are the sub-tensors of a tensor?",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections."
    },
    {
        "Y": "tensor_split Splits a tensor into multiple sub-tensors",
        "X": "What does tensor_split do?",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections."
    },
    {
        "Y": "views of input",
        "X": "What are all sub-tensors of a tensor for tensor_split?",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections."
    },
    {
        "Y": "indices  or number of sections specified by indices_or_sections",
        "X": "What does tensor_split use to split a tensor into multiple sub-tensors?",
        "Z": "tensor_split Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections."
    },
    {
        "Y": "tile",
        "X": "What constructs a tensor by repeating the elements of input?",
        "Z": "tile Constructs a tensor by repeating the elements of input."
    },
    {
        "Y": "tile",
        "X": "What Constructs a tensor by repeating the elements of input?",
        "Z": "tile Constructs a tensor by repeating the elements of input."
    },
    {
        "Y": "tensor which is transpose of input tensor",
        "X": "What does transpose return?",
        "Z": "transpose Returns a tensor that is a transposed version of input."
    },
    {
        "Y": "transpose",
        "X": "What returns a tensor that is a transposed version of input?",
        "Z": "transpose Returns a tensor that is a transposed version of input."
    },
    {
        "Y": "tensor",
        "X": "What does transpose return that is a transposed version of input?",
        "Z": "transpose Returns a tensor that is a transposed version of input."
    },
    {
        "Y": "unbind",
        "X": "What removes a tensor dimension?",
        "Z": "unbind Removes a tensor dimension."
    },
	{
        "Y": " use unbind",
        "X": "How to remove a tensor dimension?",
        "Z": "unbind Removes a tensor dimension."
    },
    {
        "Y": "a dimension of size one inserted at the specified position",
        "X": "What does unsqueeze return a new tensor with?",
        "Z": "unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position."
    },
    {
        "Y": "a new tensor",
        "X": "What does unsqueeze Return?",
        "Z": "unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position."
    },
    {
        "Y": "indices_or_sections",
        "X": "What does vsplit Split input's tensors vertically vary according to?",
        "Z": "vsplit Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections."
    },
    {
        "Y": "a tensor with two or more dimensions",
        "X": "What is input to vsplit?",
        "Z": "vsplit Splits input, a tensor with two or more dimensions, into multiple tensors vertically according to indices_or_sections."
    },
    {
        "Y": "row wise",
        "X": "How are vstack Stack tensors organized?",
        "Z": "vstack Stack tensors in sequence vertically (row wise)."
    },
    {
        "Y": "vertically",
        "X": "How are vstack Stack tensors sequenced?",
        "Z": "vstack Stack tensors in sequence vertically (row wise)."
    },
    {
        "Y": "vstack Stack tensors in sequence vertically",
        "X": "What does  vstack do?",
        "Z": "vstack Stack tensors in sequence vertically (row wise)."
    },
    {
        "Y": "row wise",
        "X": "vstack Stack tensors in sequence vertically based on what?",
        "Z": "vstack Stack tensors in sequence vertically (row wise)."
    },
    {
        "Y": "tensor",
        "X": "What is the return value of elements selected from either x or y?",
        "Z": "where Return a tensor of elements selected from either x or y, depending on condition."
    },
    {
        "Y": "either x or y",
        "X": "Where are the tensor of elements selected from?",
        "Z": "where Return a tensor of elements selected from either x or y, depending on condition."
    },
    {
        "Y": "pseudo random numbers",
        "X": "What does the generator object produce?",
        "Z": "Generator Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers."
    },
    {
        "Y": "generator object",
        "X": "What does Generator create and return that manages the state of the algorithm?",
        "Z": "Generator Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers."
    },
    {
        "Y": "seed",
        "X": "What sets the seed for generating random numbers to a non-deterministic random number?",
        "Z": "seed Sets the seed for generating random numbers to a non-deterministic random number."
    },
    {
        "Y": "non-deterministic random number",
        "X": "To what  does seed Set the seed for generating random numbers?",
        "Z": "seed Sets the seed for generating random numbers to a non-deterministic random number."
    },
    {
        "Y": "manual_seed",
        "X": "What does set the seed for generating random numbers?",
        "Z": "manual_seed Sets the seed for generating random numbers."
    },
    {
        "Y": "for generating random numbers",
        "X": "what for manual_seed Set the seed ?",
        "Z": "manual_seed Sets the seed for generating random numbers."
    },
    {
        "Y": "Python long",
        "X": "How long is the initial seed for generating random numbers?",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long."
    },
    {
        "Y": "initial_seed",
        "X": "What does return the initial seed for generating random numbers as a Python  long?",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long."
    },
    {
        "Y": "Python long",
        "X": "initial_seed Returns the initial seed for generating random numbers as  what?",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long."
    },
    {
        "Y": "initial seed",
        "X": "What does initial_seed return for generating random numbers as a Python  long?",
        "Z": "initial_seed Returns the initial seed for generating random numbers as a Python  long."
    },
    {
        "Y": "get_rng_state",
        "X": "What returns the random number generator state as a torch.ByteTensor?",
        "Z": "get_rng_state Returns the random number generator state as a torch.ByteTensor."
    },
    {
        "Y": "torch.ByteTensor",
        "X": "What is random number generator state returned by get_rng_state?",
        "Z": "get_rng_state Returns the random number generator state as a torch.ByteTensor."
    },
    {
        "Y": "set_rng_state",
        "X": "What sets the random number generator state?",
        "Z": "set_rng_state Sets the random number generator state."
    },
    {
        "Y": "random number generator state",
        "X": "What does set_rng_state set?",
        "Z": "set_rng_state Sets the random number generator state."
    },
    {
        "Y": "bernoulli",
        "X": "What draws binary random numbers from a Bernoulli distribution?",
        "Z": "bernoulli Draws binary random numbers (0 or 1) from a Bernoulli distribution."
    },
    {
        "Y": "binary random numbers",
        "X": "What does bernoulli draw from a Bernoulli distribution?",
        "Z": "bernoulli Draws binary random numbers (0 or 1) from a Bernoulli distribution."
    },
    {
        "Y": "multinomial",
        "X": "What returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located",
        "Z": "multinomial Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput."
    },
    {
        "Y": "tensor",
        "X": "What does multinomial return if each row containsnum_samplesindices sampled from the multinomial probability distribution?",
        "Z": "multinomial Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput."
    },
    {
        "Y": "mean and standard deviation",
        "X": "What are given to the tensor of random numbers drawn from separate normal distributions?",
        "Z": "normal Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given."
    },
    {
        "Y": "normal",
        "X": "What returns a tensor of random numbers drawn from separate normal distributions?",
        "Z": "normal Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given."
    },
    {
        "Y": "rate parameter given by the corresponding element in input i",
        "X": "What does poisson use to return a tensor of the same size?",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"
    },
    {
        "Y": "Poisson distribution",
        "X": "What distribution is the tensor sampled from by poisson?",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"
    },
    {
        "Y": "rate parameter",
        "X": "What parameter is given by the corresponding element in input for poisson?",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"
    },
    {
        "Y": "Poisson distribution",
        "X": "From what distribution is each element sampled by poisson?",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"
    },
    {
        "Y": "a tensor",
        "X": "What does poisson return for each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.",
        "Z": "poisson Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,"
    },
    {
        "Y": "uniform distribution",
        "X": "What kind of distribution is  referred by rand to return tensor?",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)"
    },
    {
        "Y": "rand",
        "X": "What returns a tensor filled with random numbers from a uniform distribution?",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)"
    },
    {
        "Y": "random numbers",
        "X": "rand Returns a tensor filled with what?",
        "Z": "rand Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)"
    },
    {
        "Y": "randn_like",
        "X": "What returns a tensor with the same size as input?",
        "Z": "randn_like Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1."
    },
    {
        "Y": "a tensor",
        "X": "What does rand_like return?",
        "Z": "rand_like Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)."
    },
    {
        "Y": "randint",
        "X": "What returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Z": "randint Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)."
    },
    {
        "Y": "randint_like",
        "X": "What returns a tensor with the same shape as Tensorinput filled with random integers generated uniformly betweenlow(inclusive",
        "Z": "randint_like Returns a tensor with the same shape as Tensorinput filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)."
    },
    {
        "Y": "mean0and variance1",
        "X": "What is the standard normal distribution?",
        "Z": "randn Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution)."
    },
    {
        "Y": "tensor",
        "X": "What is randn's return value?",
        "Z": "randn Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution)."
    },
    {
        "Y": "tensor",
        "X": "What does randn return?",
        "Z": "randn Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution)."
    },
    {
        "Y": "normal distribution",
        "X": "Which distribution  randn_like  refers for returning  a tensor ?",
        "Z": "randn_like Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1."
    },
    {
        "Y": "a tensor",
        "X": "What does randn_like return?",
        "Z": "randn_like Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1."
    },
    {
        "Y": "randperm",
        "X": "What returns a random permutation of integers from 0 to -1?",
        "Z": "randperm Returns a random permutation of integers from 0 to -1."
    },
    {
        "Y": "The torch.quasirandom.SobolEngine",
        "X": "What is engine that generates Sobol sequences?",
        "Z": "quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences."
    },
    {
        "Y": "quasirandom  (scrambled) Sobol sequences.",
        "X": "What type of sequence is generated by the SobolEngine ?",
        "Z": "quasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences."
    },
    {
        "Y": "save Saves an object to a disk file",
        "X": "What does save do?",
        "Z": "save Saves an object to a disk file."
    },
    {
        "Y": "save",
        "X": "What saves an object to a disk file?",
        "Z": "save Saves an object to a disk file."
    },
    {
        "Y": "disk",
        "X": "save Saves an object to what type of file?",
        "Z": "save Saves an object to a disk file."
    },
    {
        "Y": "with torch.save()",
        "X": "load Loads an object saved from a file  saved using what method?",
        "Z": "load Loads an object saved with torch.save() from a file."
    },
    {
        "Y": "the object from a file  earlier saved with torch.save()",
        "X": "What does load Load  from a file?",
        "Z": "load Loads an object saved with torch.save() from a file."
    },
    {
        "Y": "get_num_threads",
        "X": "What returns the number of threads used for parallelizing CPU operations?",
        "Z": "get_num_threads Returns the number of threads used for parallelizing CPU operations"
    },
    {
        "Y": "set_num_threads",
        "X": "What sets the number of threads used for intraop parallelism on CPU?",
        "Z": "set_num_threads Sets the number of threads used for intraop parallelism on CPU."
    },
    {
        "Y": "get_num_interop_threads",
        "X": "What returns the number of threads used for inter-op parallelism on CPU?",
        "Z": "get_num_interop_threads Returns the number of threads used for inter-op parallelism on CPU (e.g."
    },
    {
        "Y": "the number of threads used for inter-op parallelism on CPU",
        "X": "What does get_num_interop_threads return?",
        "Z": "get_num_interop_threads Returns the number of threads used for inter-op parallelism on CPU (e.g."
    },
    {
        "Y": "set_num_interop_threads",
        "X": "What sets the number of threads used for interop parallelism?",
        "Z": "set_num_interop_threads Sets the number of threads used for interop parallelism (e.g."
    },
    {
        "Y": "no_grad Context-manager",
        "X": "What disables gradient calculation?",
        "Z": "no_grad Context-manager that disabled gradient calculation."
    },
    {
        "Y": "no_grad Context-manager",
        "X": "What is program that disabled gradient calculation?",
        "Z": "no_grad Context-manager that disabled gradient calculation."
    },
    {
        "Y": "gradient calculation",
        "X": "What did no_grad Context-manager disable?",
        "Z": "no_grad Context-manager that disabled gradient calculation."
    },
    {
        "Y": "gradient calculation",
        "X": "What does the enable_grad Context-manager enable?",
        "Z": "enable_grad Context-manager that enables gradient calculation."
    },
    {
        "Y": "enable_grad",
        "X": "What is Context-manager that enables gradient calculation?",
        "Z": "enable_grad Context-manager that enables gradient calculation."
    },
    {
        "Y": "set_grad_enabled Context-manager",
        "X": "What sets gradient calculation to on or off?",
        "Z": "set_grad_enabled Context-manager that sets gradient calculation to on or off."
    },
    {
        "Y": "set_grad_enabled",
        "X": "What is Context-manager that sets gradient calculation to on or off?",
        "Z": "set_grad_enabled Context-manager that sets gradient calculation to on or off."
    },
    {
        "Y": "Context-manager",
        "X": "What is set_grad_enabled?",
        "Z": "set_grad_enabled Context-manager that sets gradient calculation to on or off."
    },
    {
        "Y": " is_grad_enabled Returns True",
        "X": "What happens if grad mode is currently enabled?",
        "Z": "is_grad_enabled Returns True if grad mode is currently enabled."
    },
    {
        "Y": "grad mode",
        "X": "What mode is currently enabled by is_grad_enabled?",
        "Z": "is_grad_enabled Returns True if grad mode is currently enabled."
    },
    {
        "Y": "inference_mode Context-manager",
        "X": "What enables or disables inference mode?",
        "Z": "inference_mode Context-manager that enables or disables inference mode"
    },
    {
        "Y": "inference mode",
        "X": "What does inference_mode Context-manager enable or disable?",
        "Z": "inference_mode Context-manager that enables or disables inference mode"
    },
    {
        "Y": "is_inference_mode_enabled",
        "X": "What Returns True if inference mode is currently enabled?",
        "Z": "is_inference_mode_enabled Returns True if inference mode is currently enabled."
    },
    {
        "Y": "absolute value",
        "X": "What does abs compute of each element in input?",
        "Z": "abs Computes the absolute value of each element in input."
    },
    {
        "Y": "abs",
        "X": "What computes the absolute value of each element in input?",
        "Z": "abs Computes the absolute value of each element in input."
    },
    {
        "Y": "Alias for torch.abs()",
        "X": "What is the absolute ?",
        "Z": "absolute Alias for torch.abs()"
    },
    {
        "Y": "absolute ",
        "X": "What is the  Alias for torch.abs()?",
        "Z": "absolute Alias for torch.abs()"
    },
    {
        "Y": "inverse cosine",
        "X": "What does acos compute?",
        "Z": "acos Computes the inverse cosine of each element in input."
    },
    {
        "Y": "acos",
        "X": "What computes the inverse cosine of each element in input?",
        "Z": "acos Computes the inverse cosine of each element in input."
    },
    {
        "Y": "Alias for torch.acos",
        "X": "What is arccos?",
        "Z": "arccos Alias for torch.acos()."
    },
    {
        "Y": "arccos",
        "X": "What is Alias for torch.acos?",
        "Z": "arccos Alias for torch.acos()."
    },
    {
        "Y": "inverse hyperbolic cosine",
        "X": "acosh Returns a new tensor with what of the elements of input?",
        "Z": "acosh Returns a new tensor with the inverse hyperbolic cosine of the elements of input."
    },
    {
        "Y": "acosh",
        "X": "What returns a new tensor with the inverse hyperbolic cosine of the elements of input?",
        "Z": "acosh Returns a new tensor with the inverse hyperbolic cosine of the elements of input."
    },
    {
        "Y": "acosh",
        "X": "What returns a new tensor with the inverse hyperbolic cosine of the elements of input?",
        "Z": "acosh Returns a new tensor with the inverse hyperbolic cosine of the elements of input."
    },
    {
        "Y": "Alias for torch.acosh",
        "X": "What is  arccosh?",
        "Z": "arccosh Alias for torch.acosh()."
    },
    {
        "Y": "arccosh",
        "X": "What is the Alias for torch.acosh()?",
        "Z": "arccosh Alias for torch.acosh()."
    },
    {
        "Y": "add Adds the scalar other to each element of the input input and returns a new resulting tensor",
        "X": "What does add do to the scalar other to each element of the input input and return a new resulting tensor?",
        "Z": "add Adds the scalar other to each element of the input input and returns a new resulting tensor."
    },
    {
        "Y": "add",
        "X": "What adds the scalar other to each element of the input input and returns a new resulting tensor?",
        "Z": "add Adds the scalar other to each element of the input input and returns a new resulting tensor."
    },
    {
        "Y": "addcdiv",
        "X": "What performs the element-wise division of tensor 1 by tensor 2 , multiply the result by the scalarvalueand add it to input?",
        "Z": "addcdiv Performs the element-wise division of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input."
    },
    {
        "Y": "multiply the result by the scalarvalue",
        "X": "How does addcdiv perform the element-wise division of of tensor 1 by tensor 2?",
        "Z": "addcdiv Performs the element-wise division of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input."
    },
    {
        "Y": "addcmul",
        "X": "What performs the element-wise multiplication of tensor 1 by tensor 2?",
        "Z": "addcmul Performs the element-wise multiplication of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input."
    },
    {
        "Y": "element-wise multiplication of tensor 1 by tensor 2",
        "X": "to what inputs the element-wise multiplication performed by addcmul?",
        "Z": "addcmul Performs the element-wise multiplication of tensor 1 by tensor 2, multiply the result by the scalarvalueand add it to input."
    },
    {
        "Y": "the element-wise angle",
        "X": "What does angle compute?",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor."
    },
    {
        "Y": "radians",
        "X": "In what units does angle compute the element-wise angle of the given input tensor?",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor."
    },
    {
        "Y": "radians",
        "X": "In what units is the element-wise angle of a given input tensor computed?",
        "Z": "angle Computes the element-wise angle (in radians) of the given input tensor."
    },
    {
        "Y": "asin",
        "X": "What returns a new tensor with the arcsine of the elements of input?",
        "Z": "asin Returns a new tensor with the arcsine  of the elements of input."
    },
    {
        "Y": "arcsine",
        "X": "What returns a new tensor with arcsine of each element ?",
        "Z": "asin Returns a new tensor with the arcsine  of the elements of input."
    },
    {
        "Y": "torch.asin",
        "X": "What is arcsin?",
        "Z": "arcsin Alias for torch.asin()."
    },
    {
        "Y": "arcsin",
        "X": "What is Alias for torch.asin()?",
        "Z": "arcsin Alias for torch.asin()."
    },
    {
        "Y": "asinh",
        "X": "What returns a new tensor with the inverse hyperbolic sine of the elements of input?",
        "Z": "asinh Returns a new tensor with the inverse hyperbolic sine of the elements of input."
    },
    {
        "Y": "hyperbolic",
        "X": "What type of sine does asinh return?",
        "Z": "asinh Returns a new tensor with the inverse hyperbolic sine of the elements of input."
    },
    {
        "Y": "inverse hyperbolic sine",
        "X": "Asinh Returns a new tensor with what of the elements of input?",
        "Z": "asinh Returns a new tensor with the inverse hyperbolic sine of the elements of input."
    },
    {
        "Y": "Alias for torch.asinh",
        "X": "What is arcsinh?",
        "Z": "arcsinh Alias for torch.asinh()."
    },
    {
        "Y": " Alias for torch.asinh",
        "X": "What is arcsinh?",
        "Z": "arcsinh Alias for torch.asinh()."
    },
    {
        "Y": "atan ",
        "X": "What returns a new tensor with the arctangent of the elements of input?",
        "Z": "atan Returns a new tensor with the arctangent  of the elements of input."
    },
    {
        "Y": "arctangent",
        "X": "Atan returns a new tensor with what of the elements of input?",
        "Z": "atan Returns a new tensor with the arctangent  of the elements of input."
    },
    {
        "Y": "Alias for torch.atan",
        "X": "What is arctan?",
        "Z": "arctan Alias for torch.atan()."
    },
    {
        "Y": "arctan",
        "X": "What is Alias for torch.atan?",
        "Z": "arctan Alias for torch.atan()."
    },
    {
        "Y": "atanh",
        "X": "What returns a new tensor with the inverse hyperbolic tangent of the elements of input?",
        "Z": "atanh Returns a new tensor with the inverse hyperbolic tangent of the elements of input."
    },
    {
        "Y": "hyperbolic",
        "X": "Atanh Returns a new tensor with the inverse of what type of tangent of the elements of input?",
        "Z": "atanh Returns a new tensor with the inverse hyperbolic tangent of the elements of input."
    },
    {
        "Y": "inverse hyperbolic tangent",
        "X": "Atanh Returns a new tensor with what of the elements of input?",
        "Z": "atanh Returns a new tensor with the inverse hyperbolic tangent of the elements of input."
    },
    {
        "Y": "Alias for torch.atanh",
        "X": "What is arctanh?",
        "Z": "arctanh Alias for torch.atanh()."
    },
    {
        "Y": "arctanh",
        "X": "What is the  Alias for torch.atanh()?",
        "Z": "arctanh Alias for torch.atanh()."
    },
    {
        "Y": "atan2",
        "X": "What is the element-wise arctangent ofinput i/otheri?",
        "Z": "atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant."
    },
    {
        "Y": "the quadrant",
        "X": "With consideration of what is the arctangent ofinput i/otheri / textotheritextotheri",
        "Z": "atan2 Element-wise arctangent ofinput i/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant."
    },
    {
        "Y": "bitwise_not",
        "X": " What Computes the bitwise NOT of the given input tensor?",
        "Z": "bitwise_not Computes the bitwise NOT of the given input tensor."
    },
    {
        "Y": "bitwise_and",
        "X": "What computes the bitwise AND of input and other?",
        "Z": "bitwise_and Computes the bitwise AND of input and other."
    },
    {
        "Y": "Computes the bitwise OR of input and other",
        "X": "What is bitwise_or for?",
        "Z": "bitwise_or Computes the bitwise OR of input and other."
    },
    {
        "Y": "bitwise_xor",
        "X": "What computes the bitwise XOR of input and other?",
        "Z": "bitwise_xor Computes the bitwise XOR of input and other."
    },
    {
        "Y": "a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element",
        "X": "What does ceil return with the ceil of the elements of input?",
        "Z": "ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element."
    },
    {
        "Y": "the ceil",
        "X": "What is the smallest integer greater than or equal to each element?",
        "Z": "ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element."
    },
    {
        "Y": "the smallest integer greater than or equal to each element",
        "X": "What is the ceil of the elements of input?",
        "Z": "ceil Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element."
    },
    {
        "Y": "clamp",
        "X": "What clamps all elements in input into the range?",
        "Z": "clamp Clamps all elements in input into the range[min,max]."
    },
    {
        "Y": "clamp Clamps inputs to a range bentween min and max",
        "X": "What are all elements in input into the range?",
        "Z": "clamp Clamps all elements in input into the range[min,max]."
    },
    {
        "Y": "Alias for torch.clamp()",
        "X": "What is clip?",
        "Z": "clip Alias for torch.clamp()."
    },
    {
        "Y": "clip",
        "X": "What is the Alias for torch.clamp()?",
        "Z": "clip Alias for torch.clamp()."
    },
    {
        "Y": "for torch.clamp()",
        "X": "What is the name of clip Alias?",
        "Z": "clip Alias for torch.clamp()."
    },
    {
        "Y": "the element-wise conjugate of the given input tensor",
        "X": "What does conj compute?",
        "Z": "conj Computes the element-wise conjugate of the given input tensor."
    },
    {
        "Y": "the element-wise conjugate of the given input tensor",
        "X": "conj Computes what?",
        "Z": "conj Computes the element-wise conjugate of the given input tensor."
    },
    {
        "Y": "conj",
        "X": "What Computes the element-wise conjugate of the given input tensor?",
        "Z": "conj Computes the element-wise conjugate of the given input tensor."
    },
    {
        "Y": "the magnitude of input",
        "X": "What is the magnitude of a floating-point tensor?",
        "Z": "copysign Create a new floating-point tensor with the magnitude of input and the sign of other, element wise."
    },
    {
        "Y": "cos",
        "X": "What returns a new tensor with the cosine of the elements of input?",
        "Z": "cos Returns a new tensor with the cosine  of the elements of input."
    },
    {
        "Y": "cos",
        "X": "What Returns a new tensor with the cosine of the elements of input?",
        "Z": "cos Returns a new tensor with the cosine  of the elements of input."
    },
    {
        "Y": "hyperbolic",
        "X": "cosh Returns a new tensor with what type of cosine?",
        "Z": "cosh Returns a new tensor with the hyperbolic cosine  of the elements of input."
    },
    {
        "Y": "cosh",
        "X": "What returns a new tensor with the hyperbolic cosine of the elements of input?",
        "Z": "cosh Returns a new tensor with the hyperbolic cosine  of the elements of input."
    },
    {
        "Y": "deg2rad",
        "X": "What returns a new tensor with each element of input converted from angles in degrees to radians?",
        "Z": "deg2rad Returns a new tensor with each of the elements of input converted from angles in degrees to radians."
    },
    {
        "Y": "deg2rad",
        "X": "What returns a new tensor with each of the elements of input converted from angles in degrees to radians?",
        "Z": "deg2rad Returns a new tensor with each of the elements of input converted from angles in degrees to radians."
    },
    {
        "Y": "div",
        "X": "What divides each element of the input input by the corresponding element of the other?",
        "Z": "div Divides each element of the input input by the corresponding element of other."
    },
    {
        "Y": "div",
        "X": "What divides each element of the input input by the corresponding element of other?",
        "Z": "div Divides each element of the input input by the corresponding element of other."
    },
    {
        "Y": "Alias for torch.div()",
        "X": "What is Alias for torch.div() divide?",
        "Z": "divide Alias for torch.div()."
    },
    {
        "Y": "divide Alias for torch.div()",
        "X": "What is Alias for torch.div()?",
        "Z": "divide Alias for torch.div()."
    },
    {
        "Y": "logarithmic",
        "X": "digamma Computes the derivative of the gamma function on input?",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input."
    },
    {
        "Y": "gamma function",
        "X": "digamma Computes the logarithmic derivative of what on input?",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input."
    },
    {
        "Y": "logarithmic derivative",
        "X": "digamma Computes what derivative of the gamma function on input?",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input."
    },
    {
        "Y": "digamma",
        "X": "What computes the logarithmic derivative of the gamma function on input?",
        "Z": "digamma Computes the logarithmic derivative of the gamma function on input."
    },
    {
        "Y": "Alias for torch.special.erf()",
        "X": "What does erf?",
        "Z": "erf Alias for torch.special.erf()."
    },
    {
        "Y": "Alias for torch.special.erf",
        "X": "What is erf?",
        "Z": "erf Alias for torch.special.erf()."
    },
    {
        "Y": "Alias for torch.special.erfc()",
        "X": "What is erfc function?",
        "Z": "erfc Alias for torch.special.erfc()."
    },
    {
        "Y": "for torch.special.erfc()",
        "X": "What is erfc Alias?",
        "Z": "erfc Alias for torch.special.erfc()."
    },
    {
        "Y": "Alias for torch.special.erfinv()",
        "X": "What does erfinv do?",
        "Z": "erfinv Alias for torch.special.erfinv()."
    },
    {
        "Y": "Alias for torch.special.erfinv",
        "X": "What does erfinv stand for?",
        "Z": "erfinv Alias for torch.special.erfinv()."
    },
    {
        "Y": "for torch.special.erfinv",
        "X": "What is erfinv Alias?",
        "Z": "erfinv Alias for torch.special.erfinv()."
    },
    {
        "Y": "exp",
        "X": "What returns a new tensor with the exponential of the elements of the input tensorinput?",
        "Z": "exp Returns a new tensor with the exponential of the elements of the input tensorinput."
    },
    {
        "Y": "exponential",
        "X": "exp Returns a new tensor with what of the elements of the input tensorinput?",
        "Z": "exp Returns a new tensor with the exponential of the elements of the input tensorinput."
    },
    {
        "Y": "exp2",
        "X": "What is the Alias for torch.special.exp2()?",
        "Z": "exp2 Alias for torch.special.exp2()."
    },
    {
        "Y": "expm1",
        "X": "What is the Alias for torch.special.expm1?",
        "Z": "expm1 Alias for torch.special.expm1()."
    },
    {
        "Y": "Alias for torch.special.expm1",
        "X": "What does expm1 stand for?",
        "Z": "expm1 Alias for torch.special.expm1()."
    },
    {
        "Y": "expm1",
        "X": "What is the Alias for torch.special.expm1()?",
        "Z": "expm1 Alias for torch.special.expm1()."
    },
    {
        "Y": "scale,zero_point,quant_minandquant_max",
        "X": "What methods are used to get a new tensor with the data in inputfake quantized per channel?",
        "Z": "fake_quantize_per_channel_affine Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis."
    },
    {
        "Y": "byaxis",
        "X": "The new tensor returns a new tensor with the data in inputfake quantized per channel usingscale,zer",
        "Z": "fake_quantize_per_channel_affine Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis."
    },
    {
        "Y": "fake_quantize_per_channel_affine",
        "X": "What returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant",
        "Z": "fake_quantize_per_channel_affine Returns a new tensor with the data in inputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis."
    },
    {
        "Y": "scale,zero_point,quant_minandquant_max",
        "X": "What methods are used to fake the quantization of a tensor?",
        "Z": "fake_quantize_per_tensor_affine Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max."
    },
    {
        "Y": "fake_quantize_per_tensor_affine",
        "X": "What returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_min",
        "Z": "fake_quantize_per_tensor_affine Returns a new tensor with the data in inputfake quantized usingscale,zero_point,quant_minandquant_max."
    },
    {
        "Y": "Alias for torch.trunc()",
        "X": "What is fix Alias for ?",
        "Z": "fix Alias for torch.trunc()"
    },
    {
        "Y": "fix Alias for torch.trunc()",
        "X": "What does fix Alias for torch.trunc() do?",
        "Z": "fix Alias for torch.trunc()"
    },
    {
        "Y": "double precision",
        "X": "float_power Raises input to the power of exponent, element wise, in what precision?",
        "Z": "float_power Raises input to the power of exponent, element wise, in double precision."
    },
    {
        "Y": "float_power",
        "X": "What Raises input to the power of exponent, element wise?",
        "Z": "float_power Raises input to the power of exponent, element wise, in double precision."
    },
    {
        "Y": "floor",
        "X": "What returns a new tensor with the floor of the elements of input?",
        "Z": "floor Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element."
    },
    {
        "Y": "floor_divide",
        "X": "What is the term for a floor division?",
        "Z": "floor_divide "
    },
    {
        "Y": "element-wise remainder",
        "X": " What does fmod Compute?",
        "Z": "fmod Computes the element-wise remainder of division."
    },
    {
        "Y": "remainder",
        "X": "What computes the element-wise remainder of division?",
        "Z": "remainder Computes the element-wise remainder of division."
    },
    {
        "Y": "fractional portion",
        "X": "frac Computes what portion of each element in input?",
        "Z": "frac Computes the fractional portion of each element in input."
    },
    {
        "Y": "fractional",
        "X": "frac Computes the what portion of each element in input?",
        "Z": "frac Computes the fractional portion of each element in input."
    },
    {
        "Y": "frac",
        "X": "What computes the fractional portion of each element in input?",
        "Z": "frac Computes the fractional portion of each element in input."
    },
    {
        "Y": "frexp",
        "X": "What Decomposes input into mantissa and exponent tensors?",
        "Z": "frexp Decomposes input into mantissa and exponent tensors such thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent."
    },
    {
        "Y": "gradient",
        "X": "What is function that is analogous to NumPy's gradient function?",
        "Z": "gradient This function is analogous to NumPy\u2019s gradient function."
    },
    {
        "Y": "NumPy",
        "X": "The gradient function is analogous to what other function?",
        "Z": "gradient This function is analogous to NumPy\u2019s gradient function."
    },
    {
        "Y": "NumPy\u2019s gradient function",
        "X": "What is the gradient function analogous to?",
        "Z": "gradient This function is analogous to NumPy\u2019s gradient function."
    },
    {
        "Y": "imag",
        "X": "What returns a new tensor containing imaginary values of the self tensor?",
        "Z": "imag Returns a new tensor containing imaginary values of the self tensor."
    },
    {
        "Y": "imaginary values of the self tensor",
        "X": "imag Returns a new tensor containing what?",
        "Z": "imag Returns a new tensor containing imaginary values of the self tensor."
    },
    {
        "Y": "Multiplies input by 2**:attr:other",
        "X": "What is ldexp?",
        "Z": "ldexp Multiplies input by 2**:attr:other."
    },
    {
        "Y": "ldexp",
        "X": "What does Multiplies input by 2**:attr:other?",
        "Z": "ldexp Multiplies input by 2**:attr:other."
    },
    {
        "Y": "lerp",
        "X": "What does a linear interpolation of two tensors start(given by input) and end based on a s",
        "Z": "lerp Does a linear interpolation of two tensors start(given by input) and end based on a scalar or tensor weight and  returns the resulting out tensor."
    },
    {
        "Y": "a scalar or tensor weight",
        "X": "What does lerp base its interpolation of two tensors on?",
        "Z": "lerp Does a linear interpolation of two tensors start(given by input) and end based on a scalar or tensor weight and  returns the resulting out tensor."
    },
    {
        "Y": "a scalar or tensor weight",
        "X": "What does lerp do a linear interpolation of two tensors start(given by input) and end based on?",
        "Z": "lerp Does a linear interpolation of two tensors start(given by input) and end based on a scalar or tensor weight and  returns the resulting out tensor."
    },
    {
        "Y": "lgamma",
        "X": "What computes the natural logarithm of the absolute value of the gamma function on input?",
        "Z": "lgamma Computes the natural logarithm of the absolute value of the gamma function on input."
    },
    {
        "Y": "logarithm",
        "X": "lgamma Computes the natural what of the value of the gamma function on input?",
        "Z": "lgamma Computes the natural logarithm of the absolute value of the gamma function on input."
    },
    {
        "Y": "lgamma",
        "X": "What Computes the natural logarithm of the absolute value of the gamma function on input?",
        "Z": "lgamma Computes the natural logarithm of the absolute value of the gamma function on input."
    },
    {
        "Y": "the natural logarithm of the absolute value of the gamma function on input",
        "X": "What does lgamma compute?",
        "Z": "lgamma Computes the natural logarithm of the absolute value of the gamma function on input."
    },
    {
        "Y": "log",
        "X": "What returns a new tensor with the natural logarithm of the elements of input?",
        "Z": "log Returns a new tensor with the natural logarithm of the elements of input."
    },
    {
        "Y": "log10",
        "X": "What returns a new tensor with the logarithm to the base 10 of the elements of input?",
        "Z": "log10 Returns a new tensor with the logarithm to the base 10 of the elements of input."
    },
    {
        "Y": "logarithm",
        "X": "The new tensor returns a new tensor with what?",
        "Z": "log10 Returns a new tensor with the logarithm to the base 10 of the elements of input."
    },
    {
        "Y": "log10",
        "X": "What returns a new tensor with the logarithm to the base 10 of the elements of input?",
        "Z": "log10 Returns a new tensor with the logarithm to the base 10 of the elements of input."
    },
    {
        "Y": "log1p",
        "X": "What returns a new tensor with the natural logarithm of (1 + input)?",
        "Z": "log1p Returns a new tensor with the natural logarithm of (1 +input)."
    },
    {
        "Y": "1",
        "X": "What is the natural logarithm of p?",
        "Z": "log1p Returns a new tensor with the natural logarithm of (1 +input)."
    },
    {
        "Y": "log2",
        "X": "What returns a new tensor with the logarithm to the base 2 of the elements of input?",
        "Z": "log2 Returns a new tensor with the logarithm to the base 2 of the elements of input."
    },
    {
        "Y": "logaddexp",
        "X": "What is the logarithm of the sum of exponentiations of the inputs?",
        "Z": "logaddexp Logarithm of the sum of exponentiations of the inputs."
    },
    {
        "Y": "logaddexp2",
        "X": "What is the logarithm of the sum of exponentiations of the inputs in base-2?",
        "Z": "logaddexp2 Logarithm of the sum of exponentiations of the inputs in base-2."
    },
    {
        "Y": "base-2",
        "X": "What is the base of the logaddexp2 logarithm of the sum of exponentiations of the inputs in?",
        "Z": "logaddexp2 Logarithm of the sum of exponentiations of the inputs in base-2."
    },
    {
        "Y": "logical_and Computes the element-wise logical AND of the given input tensors",
        "X": "What is function that computes the element-wise logical AND of the given input tensors?",
        "Z": "logical_and Computes the element-wise logical AND of the given input tensors."
    },
    {
        "Y": "logical_and",
        "X": "What computes the element-wise logical AND of the given input tensors?",
        "Z": "logical_and Computes the element-wise logical AND of the given input tensors."
    },
    {
        "Y": "given input tensors",
        "X": "logical_and Computes the element-wise logical AND of the given what?",
        "Z": "logical_and Computes the element-wise logical AND of the given input tensors."
    },
    {
        "Y": "logical_not Computes the element-wise logical NOT of the given input tensor",
        "X": "What does not compute the element-wise logical NOT of the given input tensor?",
        "Z": "logical_not Computes the element-wise logical NOT of the given input tensor."
    },
    {
        "Y": "logical_not",
        "X": "What computes the element-wise logical NOT of the given input tensor?",
        "Z": "logical_not Computes the element-wise logical NOT of the given input tensor."
    },
    {
        "Y": "logical_or",
        "X": "What computes the element-wise logical OR of the given input tensors?",
        "Z": "logical_or Computes the element-wise logical OR of the given input tensors."
    },
    {
        "Y": " given input tensors",
        "X": "logical_or Computes the element-wise logical OR of the given what?",
        "Z": "logical_or Computes the element-wise logical OR of the given input tensors."
    },
    {
        "Y": "logical_xor",
        "X": "What computes the element-wise logical XOR of the given input tensors?",
        "Z": "logical_xor Computes the element-wise logical XOR of the given input tensors."
    },
    {
        "Y": "Alias for torch.special.logit()",
        "X": "What is logit?",
        "Z": "logit Alias for torch.special.logit()."
    },
    {
        "Y": "logit",
        "X": "What is the Alias for torch.special.logit()?",
        "Z": "logit Alias for torch.special.logit()."
    },
    {
        "Y": "hypotenuse",
        "X": "What is leg of a right triangle?",
        "Z": "hypot Given the legs of a right triangle, return its hypotenuse."
    },
    {
        "Y": "hypotenuse",
        "X": "What do the legs of a right triangle return?",
        "Z": "hypot Given the legs of a right triangle, return its hypotenuse."
    },
    {
        "Y": "zeroth order",
        "X": "i0 Computes the modified Bessel function of the first kind for each element of input?",
        "Z": "i0 Computes the zeroth order modified Bessel function of the first kind for each element of input."
    },
    {
        "Y": "i0",
        "X": "What computes the zeroth order modified Bessel function of the first kind for each element of input?",
        "Z": "i0 Computes the zeroth order modified Bessel function of the first kind for each element of input."
    },
    {
        "Y": "igamma",
        "X": "What computes the regularized lower incomplete gamma function?",
        "Z": "igamma Computes the regularized lower incomplete gamma function:"
    },
    {
        "Y": "igammac",
        "X": "What compiler computes the regularized upper incomplete gamma function?",
        "Z": "igammac Computes the regularized upper incomplete gamma function:"
    },
    {
        "Y": "igammac",
        "X": "What Computes the regularized upper incomplete gamma function?",
        "Z": "igammac Computes the regularized upper incomplete gamma function:"
    },
    {
        "Y": "mul Multiplies each element of the input input",
        "X": "What does mul do with the scalarotherand return a new tensor?",
        "Z": "mul Multiplies each element of the input input with the scalarotherand returns a new resulting tensor."
    },
    {
        "Y": "mul",
        "X": "What Multiplies each element of the input input with the scalarotherand returns a new resulting tensor?",
        "Z": "mul Multiplies each element of the input input with the scalarotherand returns a new resulting tensor."
    },
    {
        "Y": "Alias for torch.mul",
        "X": "What is Alias fortorch?",
        "Z": "multiply Alias for torch.mul()."
    },
    {
        "Y": "multiply Alias for torch.mul()",
        "X": "What is Alias for torch.mul()?",
        "Z": "multiply Alias for torch.mul()."
    },
    {
        "Y": "dimensionpppelement",
        "X": "mvlgamma Computes themultivariate log-gamma function) with what -wise?",
        "Z": "mvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given by"
    },
    {
        "Y": "multivariate log-gamma function",
        "X": "What does mvlgamma compute?",
        "Z": "mvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given by"
    },
    {
        "Y": "dimensionpppelement-wise",
        "X": "What is given by mvlgamma?",
        "Z": "mvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given by"
    },
    {
        "Y": "mvlgamma",
        "X": "What Computes themultivariate log-gamma function?",
        "Z": "mvlgamma Computes themultivariate log-gamma function) with dimensionpppelement-wise, given by"
    },
    {
        "Y": "bynan,posinf, andneginf",
        "X": "What are the values specified by nan_to_num?",
        "Z": "nan_to_num ReplacesNaN, positive infinity, and negative infinity values in input with the values specified bynan,posinf, andneginf, respectively."
    },
    {
        "Y": "nan_to_num",
        "X": "What replacesNaN, positive infinity, and negative infinity values in input with the values specified bynan,posinf,",
        "Z": "nan_to_num ReplacesNaN, positive infinity, and negative infinity values in input with the values specified bynan,posinf, andneginf, respectively."
    },
    {
        "Y": "neg",
        "X": "What returns a new tensor with the negative of the elements of input?",
        "Z": "neg Returns a new tensor with the negative of the elements of input."
    },
    {
        "Y": "Alias for torch.neg",
        "X": "What is website that is negative for Alias for torch.neg?",
        "Z": "negative Alias for torch.neg()"
    },
    {
        "Y": "negative",
        "X": "Is Alias for torch.neg() positive or negative?",
        "Z": "negative Alias for torch.neg()"
    },
    {
        "Y": "Alias for torch.neg",
        "X": "What is Alias for torch.neg stand for?",
        "Z": "negative Alias for torch.neg()"
    },
    {
        "Y": "floating-point value",
        "X": "Afterinputtowardsother Return the next what?",
        "Z": "nextafter Return the next floating-point value afterinputtowardsother, element wise."
    },
    {
        "Y": "polygamma",
        "X": "What is function that computes the digamma function on input?",
        "Z": "polygamma Computes thenthn^{th}nthderivative of the digamma function on input."
    },
    {
        "Y": "polygamma",
        "X": "What is function that computes the secondthnthnthderivative of the digamma function onin",
        "Z": "polygamma Computes thenthn^{th}nthderivative of the digamma function on input."
    },
    {
        "Y": "positive Returnsinput",
        "X": "What is the result of the Returnsinput?",
        "Z": "positive Returnsinput."
    },
    {
        "Y": "positive Returnsinput",
        "X": "What is input?",
        "Z": "positive Returnsinput."
    },
    {
        "Y": "tensor",
        "X": "What does pow return with the result?",
        "Z": "pow Takes the power of each element in inputwithexponentand returns a tensor with the result."
    },
    {
        "Y": "pow",
        "X": "What takes the power of each element in inputwithexponentand returns a tensor with the result?",
        "Z": "pow Takes the power of each element in inputwithexponentand returns a tensor with the result."
    },
    {
        "Y": "radians",
        "X": "rad2deg Returns a new tensor with each of the elements of input converted from angles in what?",
        "Z": "rad2deg Returns a new tensor with each of the elements of input converted from angles in radians to degrees."
    },
    {
        "Y": "rad2deg",
        "X": "What returns a new tensor with each element of input converted from angles in radians to degrees?",
        "Z": "rad2deg Returns a new tensor with each of the elements of input converted from angles in radians to degrees."
    },
    {
        "Y": "rad2deg",
        "X": "What returns a new tensor with each of the elements of input converted from angles in radians to degrees?",
        "Z": "rad2deg Returns a new tensor with each of the elements of input converted from angles in radians to degrees."
    },
    {
        "Y": "real Returns a new tensor containing real values of the self tensor",
        "X": "What returns a new tensor containing real values of the self tensor?",
        "Z": "real Returns a new tensor containing real values of the self tensor."
    },
    {
        "Y": "the self tensor",
        "X": "real Returns a new tensor containing real values of what?",
        "Z": "real Returns a new tensor containing real values of the self tensor."
    },
    {
        "Y": "tensor",
        "X": "reciprocal Returns a new what with the reciprocal of the elements of input?",
        "Z": "reciprocal Returns a new tensor with the reciprocal of the elements of input"
    },
    {
        "Y": "Computes the element-wise remainder of division",
        "X": "What does remainder do?",
        "Z": "remainder Computes the element-wise remainder of division."
    },
    {
        "Y": "tensor",
        "X": "round Returns a new what?",
        "Z": "round Returns a new tensor with each of the elements of inputrounded to the closest integer."
    },
    {
        "Y": "round",
        "X": "What returns a new tensor with each of the elements of inputrounded to the closest integer?",
        "Z": "round Returns a new tensor with each of the elements of inputrounded to the closest integer."
    },
    {
        "Y": "rsqrt",
        "X": "What returns a new tensor with the reciprocal of the square-root of each of the elements of input?",
        "Z": "rsqrt Returns a new tensor with the reciprocal of the square-root of each of the elements of input."
    },
    {
        "Y": "rsqrt",
        "X": "What returns a new tensor with the reciprocal of the square-root of each of the elements of input?",
        "Z": "rsqrt Returns a new tensor with the reciprocal of the square-root of each of the elements of input."
    },
    {
        "Y": "Alias for torch.special.expit()",
        "X": "What is sigmoid?",
        "Z": "sigmoid Alias for torch.special.expit()."
    },
    {
        "Y": "tensor",
        "X": "What does sign return with the signs of the elements of input?",
        "Z": "sign Returns a new tensor with the signs of the elements of input."
    },
    {
        "Y": "tensor",
        "X": "What does sign Return with the signs of the elements of input?",
        "Z": "sign Returns a new tensor with the signs of the elements of input."
    },
    {
        "Y": "torch.sign()",
        "X": "sgn is an extension of what function?",
        "Z": "sgn This function is an extension of torch.sign() to complex tensors."
    },
    {
        "Y": "sgn",
        "X": "What is an extension of torch.sign() to complex tensors?",
        "Z": "sgn This function is an extension of torch.sign() to complex tensors."
    },
    {
        "Y": "less than zero",
        "X": "What is the sign bit set of each element of input?",
        "Z": "signbit Tests if each element of inputhas its sign bit set (is less than zero) or not."
    },
    {
        "Y": "signbit Tests",
        "X": "What tests determine if each element of input has its sign bit set (is less than zero) or not?",
        "Z": "signbit Tests if each element of inputhas its sign bit set (is less than zero) or not."
    },
    {
        "Y": "sine",
        "X": "The new tensor returns a new tensor with what of the elements of input?",
        "Z": "sin Returns a new tensor with the sine of the elements of input."
    },
    {
        "Y": "tensor",
        "X": "Sin Returns a new what with the sine of the elements of input?",
        "Z": "sin Returns a new tensor with the sine of the elements of input."
    },
    {
        "Y": "Computes",
        "X": "What does sinc do to the normalized sinc of input?",
        "Z": "sinc Computes the normalized sinc of input."
    },
    {
        "Y": "sinc",
        "X": "What Computes the normalized sinc of input?",
        "Z": "sinc Computes the normalized sinc of input."
    },
    {
        "Y": "sinh",
        "X": "What returns a new tensor with the hyperbolic sine of the elements of input?",
        "Z": "sinh Returns a new tensor with the hyperbolic sine of the elements of input."
    },
    {
        "Y": "hyperbolic",
        "X": "What type of sine does sinh return?",
        "Z": "sinh Returns a new tensor with the hyperbolic sine of the elements of input."
    },
    {
        "Y": "sinh",
        "X": "What returns a new tensor with the hyperbolic sine of the elements of input?",
        "Z": "sinh Returns a new tensor with the hyperbolic sine of the elements of input."
    },
    {
        "Y": "sqrt",
        "X": "What returns a new tensor with the square-root of the elements of input?",
        "Z": "sqrt Returns a new tensor with the square-root of the elements of input."
    },
    {
        "Y": "square",
        "X": "What returns a new tensor with the square of the elements of input?",
        "Z": "square Returns a new tensor with the square of the elements of input."
    },
    {
        "Y": "square",
        "X": "What Returns a new tensor with the square of the elements of input?",
        "Z": "square Returns a new tensor with the square of the elements of input."
    },
    {
        "Y": "sub Subtracts other",
        "X": "What is sub that is scaled byalpha?",
        "Z": "sub Subtracts other, scaled byalpha, from input."
    },
    {
        "Y": "byalpha",
        "X": "How is sub Subtracts other scaled?",
        "Z": "sub Subtracts other, scaled byalpha, from input."
    },
    {
        "Y": "from input",
        "X": "Where is sub Subtracts other scaled byalpha?",
        "Z": "sub Subtracts other, scaled byalpha, from input."
    },
    {
        "Y": "subtract Alias for torch.sub()",
        "X": "What is the function that subtracts Alias for torch.sub()?",
        "Z": "subtract Alias for torch.sub()."
    },
    {
        "Y": "Alias for torch.sub()",
        "X": "What is subtract?",
        "Z": "subtract Alias for torch.sub()."
    },
    {
        "Y": "tangent",
        "X": "tan Returns a new tensor with what of the elements of input?",
        "Z": "tan Returns a new tensor with the tangent of the elements of input."
    },
    {
        "Y": "tan",
        "X": "What returns a new tensor with the tangent of the elements of input?",
        "Z": "tan Returns a new tensor with the tangent of the elements of input."
    },
    {
        "Y": "tanh",
        "X": "What returns a new tensor with the hyperbolic tangent of the elements of input?",
        "Z": "tanh Returns a new tensor with the hyperbolic tangent of the elements of input."
    },
    {
        "Y": "hyperbolic tangent",
        "X": "tanh Returns a new tensor with what element?",
        "Z": "tanh Returns a new tensor with the hyperbolic tangent of the elements of input."
    },
    {
        "Y": "Alias for torch.div()",
        "X": "What does true_divide do?",
        "Z": "true_divide Alias for torch.div()withrounding_mode=None."
    },
    {
        "Y": "true_divide",
        "X": "What is the Alias for torch.div() with rounding_mode=None?",
        "Z": "true_divide Alias for torch.div()withrounding_mode=None."
    },
    {
        "Y": "trunc",
        "X": "What returns a new tensor with the truncated integer values of the elements of input?",
        "Z": "trunc Returns a new tensor with the truncated integer values of the elements of input."
    },
    {
        "Y": "tensor",
        "X": "What does trunc return with the truncated integer values of the elements of input?",
        "Z": "trunc Returns a new tensor with the truncated integer values of the elements of input."
    },
    {
        "Y": "Computesinput*log",
        "X": "What is xlogy?",
        "Z": "xlogy Computesinput*log(other)with the following cases."
    },
    {
        "Y": "xlogy",
        "X": "Computes input*log(other) ?",
        "Z": "xlogy Computes input*log(other) ."
    },
    {
        "Y": "argmax",
        "X": "What returns the indices of the maximum value of all elements in the input tensor?",
        "Z": "argmax Returns the indices of the maximum value of all elements in the input tensor."
    },
    {
        "Y": "argmin",
        "X": "What returns the indices of the minimum value(s) of the flattened tensor?",
        "Z": "argmin Returns the indices of the minimum value(s) of the flattened tensor or along a dimension"
    },
    {
        "Y": "argmin",
        "X": "What returns the indices of the minimum value(s) of the flattened tensor or along a dimension?",
        "Z": "argmin Returns the indices of the minimum value(s) of the flattened tensor or along a dimension"
    },
    {
        "Y": "amax",
        "X": "What returns the maximum value of each slice of the input tensor in the given dimension(s)dim?",
        "Z": "amax Returns the maximum value of each slice of the input tensor in the given dimension(s)dim."
    },
    {
        "Y": "the maximum value of each slice of the input tensor",
        "X": "What does amax return?",
        "Z": "amax Returns the maximum value of each slice of the input tensor in the given dimension(s)dim."
    },
    {
        "Y": "amin",
        "X": "What returns the minimum value of each slice of the input tensor in the given dimension(s)dim?",
        "Z": "amin Returns the minimum value of each slice of the input tensor in the given dimension(s)dim."
    },
    {
        "Y": "all Tests",
        "X": "What happens if all elements in inputevaluate toTrue?",
        "Z": "all Tests if all elements in inputevaluate toTrue."
    },
    {
        "Y": "maximum value",
        "X": "max Returns what value of all elements in the input tensor?",
        "Z": "max Returns the maximum value of all elements in the input tensor."
    },
    {
        "Y": "the input tensor",
        "X": "max Returns the maximum value of all elements in what?",
        "Z": "max Returns the maximum value of all elements in the input tensor."
    },
    {
        "Y": "max",
        "X": "What returns the maximum value of all elements in the input tensor?",
        "Z": "max Returns the maximum value of all elements in the input tensor."
    },
    {
        "Y": "the input tensor",
        "X": "min Returns the minimum value of all elements in what?",
        "Z": "min Returns the minimum value of all elements in the input tensor."
    },
    {
        "Y": "min",
        "X": "What returns the minimum value of all elements in the input tensor?",
        "Z": "min Returns the minimum value of all elements in the input tensor."
    },
    {
        "Y": "p-norm",
        "X": "What does dist return?",
        "Z": "dist Returns the p-norm of (input-other)"
    },
    {
        "Y": "input-other",
        "X": "What is the p-norm of?",
        "Z": "dist Returns the p-norm of (input-other)"
    },
    {
        "Y": "logsumexp",
        "X": "What returns the log of summed exponentials of each row of the input tensor in the given dimension dim?",
        "Z": "logsumexp Returns the log of summed exponentials of each row of the input tensor in the given dimension dim."
    },
    {
        "Y": "mean",
        "X": "What returns the mean value of all elements in the input tensor?",
        "Z": "mean Returns the mean value of all elements in the input tensor."
    },
    {
        "Y": "the mean value of all elements in the input tensor",
        "X": "mean Returns what?",
        "Z": "mean Returns the mean value of all elements in the input tensor."
    },
    {
        "Y": "nanmedian",
        "X": "What returns the median of the values in input?",
        "Z": "nanmedian Returns the median of the values in input, ignoringNaNvalues."
    },
    {
        "Y": "median",
        "X": "What Returns the median of the values in input?",
        "Z": "median Returns the median of the values in input."
    },
    {
        "Y": "the mode value of each row of the input tensor",
        "X": "What does values represent in the given dimension dim?",
        "Z": "mode Returns a named tuple(values,indices)where values is the mode value of each row of the input tensor in the given dimension dim, i.e."
    },
    {
        "Y": "mode",
        "X": "What returns a named tuple(values,indices)where values is the mode value of each row of the input tensor",
        "Z": "mode Returns a named tuple(values,indices)where values is the mode value of each row of the input tensor in the given dimension dim, i.e."
    },
    {
        "Y": "matrix  norm or vector norm",
        "X": "What is the return value of a given tensor?",
        "Z": "norm Returns the matrix  norm or vector norm of a given tensor."
    },
    {
        "Y": "matrix  norm",
        "X": "What type of norm returns a given tensor?",
        "Z": "norm Returns the matrix  norm or vector norm of a given tensor."
    },
    {
        "Y": "vector norm",
        "X": "Returns the matrix  norm or what else of a given tensor?",
        "Z": "norm Returns the matrix  norm or vector norm of a given tensor."
    },
    {
        "Y": "zero",
        "X": "What does nansum treat Not a Numbers as?",
        "Z": "nansum Returns the sum of all elements, treating Not a Numbers (NaNs) as zero."
    },
    {
        "Y": "nansum",
        "X": "What returns the sum of all elements?",
        "Z": "nansum Returns the sum of all elements, treating Not a Numbers (NaNs) as zero."
    },
    {
        "Y": "the input tensor",
        "X": "prod Returns the product of all elements in what?",
        "Z": "prod Returns the product of all elements in the input tensor."
    },
    {
        "Y": "the product of all elements in the input tensor",
        "X": "what does prod Return?",
        "Z": "prod Returns the product of all elements in the input tensor."
    },
    {
        "Y": "the dimension dim",
        "X": "The quantile computes the q-th quantiles of each row of the input tensor along what?",
        "Z": "quantile Computes the q-th quantiles of each row of the input tensor along the dimension dim."
    },
    {
        "Y": "quantile",
        "X": "What computes the q-th quantiles of each row of the input tensor along the dimension dim?",
        "Z": "quantile Computes the q-th quantiles of each row of the input tensor along the dimension dim."
    },
    {
        "Y": "nanquantile",
        "X": "What is a variant of of torch.quantile() that \"ignores\"NaNvalues?",
        "Z": "nanquantile This is a variant of torch.quantile()that \u201cignores\u201dNaNvalues, computing the quantiles q as if NaN values in input didnot exist."
    },
    {
        "Y": "quantiles q as",
        "X": "What is computed if NaN values in input didnot exist?",
        "Z": "nanquantile This is a variant of torch.quantile()that \u201cignores\u201dNaNvalues, computing the quantiles q as if NaN values in input didnot exist."
    },
    {
        "Y": "if unbiased is True",
        "X": "What is the name of Bessel's correction?",
        "Z": "std if unbiased is True, Bessel\u2019s correction will be used."
    },
    {
        "Y": "Bessel\u2019s",
        "X": "Which correction will be used if std if unbiased is True?",
        "Z": "std if unbiased is True, Bessel\u2019s correction will be used."
    },
    {
        "Y": "Bessel\u2019s correction",
        "X": "What will be used to calculate the standard deviation?",
        "Z": "std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation."
    },
    {
        "Y": "standard deviation",
        "X": "What will Bessel\u2019s correction be used to calculate?",
        "Z": "std_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the standard deviation."
    },
    {
        "Y": "the input tensor",
        "X": "sum Returns the sum of all elements in what?",
        "Z": "sum Returns the sum of all elements in the input tensor."
    },
    {
        "Y": "the sum of all elements in the input tensor",
        "X": "sum Returns what?",
        "Z": "sum Returns the sum of all elements in the input tensor."
    },
    {
        "Y": "unique",
        "X": "What returns the unique elements of the input tensor?",
        "Z": "unique Returns the unique elements of the input tensor."
    },
    {
        "Y": "unique_consecutive",
        "X": "What eliminates all but the first element from every group of equivalent elements?",
        "Z": "unique_consecutive Eliminates all but the first element from every consecutive group of equivalent elements."
    },
    {
        "Y": "unique_consecutive",
        "X": "What eliminates all but the first element from every consecutive group of equivalent elements?",
        "Z": "unique_consecutive Eliminates all but the first element from every consecutive group of equivalent elements."
    },
    {
        "Y": "var if unbiased is True",
        "X": "What is the default value for Bessel's correction?",
        "Z": "var if unbiased is True, Bessel\u2019s correction will be used."
    },
    {
        "Y": "Bessel\u2019s",
        "X": "What type of correction will be used?",
        "Z": "var if unbiased is True, Bessel\u2019s correction will be used."
    },
    {
        "Y": "Bessel\u2019s correction",
        "X": "What will be used to calculate the variance?",
        "Z": "var_mean if unbiased is True, Bessel\u2019s correction will be used to calculate the variance."
    },
    {
        "Y": "count_nonzero",
        "X": "What Counts the number of non-zero values in the tensor input along the given dim?",
        "Z": "count_nonzero Counts the number of non-zero values in the tensor input along the given dim."
    },
    {
        "Y": "the given dim",
        "X": "What is the tensor input along?",
        "Z": "count_nonzero Counts the number of non-zero values in the tensor input along the given dim."
    },
    {
        "Y": "allinput and othersatisfy",
        "X": "What condition does allclose check?",
        "Z": "allclose This function checks if allinput and othersatisfy the condition:"
    },
    {
        "Y": "argsort",
        "X": "What returns the indices that sort a tensor along a given dimension in ascending order by value?",
        "Z": "argsort Returns the indices that sort a tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "ascending",
        "X": "argsort Returns the indices that sort a tensor along a given dimension in what order by value?",
        "Z": "argsort Returns the indices that sort a tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "ascending order",
        "X": "argsort Returns the indices that sort a tensor along a given dimension in what order?",
        "Z": "argsort Returns the indices that sort a tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "element-wise equality",
        "X": "What does eq compute?",
        "Z": "eq Computes element-wise equality"
    },
    {
        "Y": "element-wise equality",
        "X": "eq Computes what?",
        "Z": "eq Computes element-wise equality"
    },
    {
        "Y": "eq",
        "X": "What Computes element-wise equality?",
        "Z": "eq Computes element-wise equality"
    },
    {
        "Y": "equal",
        "X": "What method returns  true if two tensors have the same size and elements?",
        "Z": "equal True if two tensors have the same size and elements,False otherwise."
    },
    {
        "Y": "Computesinput",
        "X": "What is anothertextinput geq textotherelement-wise?",
        "Z": "ge Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise."
    },
    {
        "Y": "Alias for torch.ge()",
        "X": "What is greater_equal?",
        "Z": "greater_equal Alias for torch.ge()."
    },
    {
        "Y": "greater_equal",
        "X": "What is Alias for torch.ge()?",
        "Z": "greater_equal Alias for torch.ge()."
    },
    {
        "Y": "Computesinput",
        "X": "What is gt?",
        "Z": "gt Computesinput>other\\text{input} > \\text{other}input>otherelement-wise."
    },
    {
        "Y": "gt Computesinput",
        "X": "What is anothertextinput > textotherinput>otherelement-wise?",
        "Z": "gt Computesinput>other\\text{input} > \\text{other}input>otherelement-wise."
    },
    {
        "Y": "Alias for torch.gt()",
        "X": "What is Alias for torch.gt?",
        "Z": "greater Alias for torch.gt()."
    },
    {
        "Y": "greater Alias for torch.gt()",
        "X": "What is Alias for torch.gt()?",
        "Z": "greater Alias for torch.gt()."
    },
    {
        "Y": "boolean elements",
        "X": "What does isclose return a new tensor with?",
        "Z": "isclose Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other."
    },
    {
        "Y": "if each element of input is \u201cclose\u201d to the corresponding element of other",
        "X": "What does the isclose return a new tensor with boolean elements representing?",
        "Z": "isclose Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other."
    },
    {
        "Y": "if each element of input is \u201cclose\u201d to the corresponding element of other",
        "X": "What do boolean elements represent?",
        "Z": "isclose Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other."
    },
    {
        "Y": "boolean elements",
        "X": "What does isfinite return if each element isfiniteor?",
        "Z": "isfinite Returns a new tensor with boolean elements representing if each element isfiniteor not."
    },
    {
        "Y": "boolean elements",
        "X": "What does isfinite return if each element isfiniteor not?",
        "Z": "isfinite Returns a new tensor with boolean elements representing if each element isfiniteor not."
    },
    {
        "Y": "if each element isfiniteor not",
        "X": "What does isfinite return a new tensor with boolean elements represent?",
        "Z": "isfinite Returns a new tensor with boolean elements representing if each element isfiniteor not."
    },
    {
        "Y": "infinite",
        "X": "Is each element of input infinite or negative infinity?",
        "Z": "isinf Tests if each element of input is infinite (positive or negative infinity) or not."
    },
    {
        "Y": "isinf",
        "X": "What Tests if each element ofinput is infinite?",
        "Z": "isinf Tests if each element of input is infinite (positive or negative infinity) or not."
    },
    {
        "Y": "isposinf Tests",
        "X": "What tests determine if each element of input is positive infinity or not?",
        "Z": "isposinf Tests if each element of input is positive infinity or not."
    },
    {
        "Y": "isposinf",
        "X": "What test determines if each element of input is positive infinity or not?",
        "Z": "isposinf Tests if each element of input is positive infinity or not."
    },
    {
        "Y": "isneginf Tests",
        "X": "What is used to determine if each element of input is negative infinity or not?",
        "Z": "isneginf Tests if each element of input is negative infinity or not."
    },
    {
        "Y": "isneginf",
        "X": "What test determines if each element of input is negative infinity or not?",
        "Z": "isneginf Tests if each element of input is negative infinity or not."
    },
    {
        "Y": "boolean elements",
        "X": "isnan Returns a new tensor with what?",
        "Z": "isnan Returns a new tensor with boolean elements representing if each element of input is NaN or not."
    },
    {
        "Y": "if each element of input is NaN or not",
        "X": "Isnan Returns a new tensor with boolean elements representing what?",
        "Z": "isnan Returns a new tensor with boolean elements representing if each element of input is NaN or not."
    },
    {
        "Y": "boolean elements",
        "X": "isnan Returns a new tensor with what elements?",
        "Z": "isnan Returns a new tensor with boolean elements representing if each element of input is NaN or not."
    },
    {
        "Y": "boolean elements",
        "X": "What does isreal return a new tensor with?",
        "Z": "isreal Returns a new tensor with boolean elements representing if each element of input is real-valued or not."
    },
    {
        "Y": "boolean elements",
        "X": "isreal Returns a new tensor with what elements?",
        "Z": "isreal Returns a new tensor with boolean elements representing if each element of input is real-valued or not."
    },
    {
        "Y": "if each element of input is real-valued or not",
        "X": "Isreal Returns a new tensor with boolean elements representing?",
        "Z": "isreal Returns a new tensor with boolean elements representing if each element of input is real-valued or not."
    },
    {
        "Y": "kthvalue",
        "X": "What returns a named tuple(values,indices)?",
        "Z": "kthvalue Returns a named tuple(values,indices)where values is thekth smallest element of each row of the input tensor in the given dimension dim."
    },
    {
        "Y": "thekth smallest element",
        "X": "What is the value of each row of the input tensor in the given dimension dim?",
        "Z": "kthvalue Returns a named tuple(values,indices)where values is thekth smallest element of each row of the input tensor in the given dimension dim."
    },
    {
        "Y": "kthvalue",
        "X": "What Returns a named tuple(values,indices)where values is thekth smallest element of each row of the",
        "Z": "kthvalue Returns a named tuple(values,indices)where values is thekth smallest element of each row of the input tensor in the given dimension dim."
    },
    {
        "Y": "Computesinput",
        "X": "What is othertextinput leq textotherelement-wise?",
        "Z": "le Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise."
    },
    {
        "Y": "Alias for torch.le()",
        "X": "What is less_equal?",
        "Z": "less_equal Alias for torch.le()."
    },
    {
        "Y": "less_equal",
        "X": "What is the difference between Alias for torch.le() and Alias for torch.le()?",
        "Z": "less_equal Alias for torch.le()."
    },
    {
        "Y": "Computesinput",
        "X": "What is anothertextinput textotherinputotherelement-wise?",
        "Z": "lt Computesinput<other\\text{input} < \\text{other}input<otherelement-wise."
    },
    {
        "Y": "Computesinput",
        "X": "What is othertextinput  textotherelement-wise?",
        "Z": "lt Computesinput<other\\text{input} < \\text{other}input<otherelement-wise."
    },
    {
        "Y": "Alias for torch.lt()",
        "X": "What is less?",
        "Z": "less Alias for torch.lt()."
    },
    {
        "Y": "less",
        "X": "What is Alias for torch.lt ?",
        "Z": "less Alias for torch.lt()."
    },
    {
        "Y": "the element-wise maximum of input and other",
        "X": "maximum Computes what?",
        "Z": "maximum Computes the element-wise maximum of input and other."
    },
    {
        "Y": "fmin",
        "X": "What computes the element-wise minimum of input and other?",
        "Z": "fmin Computes the element-wise minimum of input and other."
    },
    {
        "Y": "element-wise minimum",
        "X": "minimum Computes what of input and other?",
        "Z": "minimum Computes the element-wise minimum of input and other."
    },
    {
        "Y": "fmax",
        "X": "What computes the element-wise maximum of input and other?",
        "Z": "fmax Computes the element-wise maximum of input and other."
    },
    {
        "Y": "fmin",
        "X": "What Computes the element-wise minimum of input and other?",
        "Z": "fmin Computes the element-wise minimum of input and other."
    },
    {
        "Y": "not_equal",
        "X": "What is Alias for torch.ne()?",
        "Z": "not_equal Alias for torch.ne()."
    },
    {
        "Y": "Alias for torch.ne()",
        "X": "What is not_equal?",
        "Z": "not_equal Alias for torch.ne()."
    },
    {
        "Y": "the elements of the input tensor",
        "X": "Sort Sorts what along a given dimension in ascending order by value?",
        "Z": "sort Sorts the elements of the input tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "ascending order by value",
        "X": "Sort Sorts the elements of the input tensor along a given dimension in what order?",
        "Z": "sort Sorts the elements of the input tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "ascending order",
        "X": "Sorts the elements of the input tensor along a given dimension in what order?",
        "Z": "sort Sorts the elements of the input tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "sort",
        "X": "What Sorts the elements of the input tensor along a given dimension in ascending order by value?",
        "Z": "sort Sorts the elements of the input tensor along a given dimension in ascending order by value."
    },
    {
        "Y": "the k largest elements of the given input tensor",
        "X": "What does topk return along a given dimension?",
        "Z": "topk Returns the k largest elements of the given input tensor along a given dimension."
    },
    {
        "Y": "the k largest",
        "X": "topk Returns what element of the given input tensor along a given dimension?",
        "Z": "topk Returns the k largest elements of the given input tensor along a given dimension."
    },
    {
        "Y": "the input tensor",
        "X": "msort Sorts the elements of what?",
        "Z": "msort Sorts the elements of the input tensor along its first dimension in ascending order by value."
    },
    {
        "Y": "ascending order",
        "X": "msort Sorts the elements of the input tensor along its first dimension in what order?",
        "Z": "msort Sorts the elements of the input tensor along its first dimension in ascending order by value."
    },
    {
        "Y": "Short-time Fourier transform",
        "X": "What is STFT?",
        "Z": "stft Short-time Fourier transform (STFT)."
    },
    {
        "Y": "Fourier Transform",
        "X": "What is istft Inverse short time?",
        "Z": "istft Inverse short time Fourier Transform."
    },
    {
        "Y": "Inverse short time",
        "X": "What is the Fourier Transform?",
        "Z": "istft Inverse short time Fourier Transform."
    },
    {
        "Y": "evaluates bartlett_window",
        "X": "What is Bartlett window function?",
        "Z": "bartlett_window Bartlett window function."
    },
    {
        "Y": " evaluates blackman_window",
        "X": "What is Blackman window function?",
        "Z": "blackman_window Blackman window function."
    },
    {
        "Y": "evaluates Hamming window function",
        "X": "What does hamming_window function do?",
        "Z": "hamming_window Hamming window function."
    },
    {
        "Y": "evaluates Hamming window function",
        "X": "What is hamming_window?",
        "Z": "hamming_window Hamming window function."
    },
    {
        "Y": "evaluates hann_window",
        "X": "What is Hann window function?",
        "Z": "hann_window Hann window function."
    },
    {
        "Y": "evaluates Hann window function",
        "X": "What does hann_window do?",
        "Z": "hann_window Hann window function."
    },
    {
        "Y": "window length window_length and shape parameter beta",
        "X": "What does kaiser_window compute the Kaiser window with?",
        "Z": "kaiser_window Computes the Kaiser window with window length window_length and shape parameter beta."
    },
    {
        "Y": "kaiser_window",
        "X": "What computes the Kaiser window with window length window_length and shape parameter beta?",
        "Z": "kaiser_window Computes the Kaiser window with window length window_length and shape parameter beta."
    },
    {
        "Y": "kaiser_window",
        "X": "What Computes the Kaiser window with window length window_length and shape parameter beta?",
        "Z": "kaiser_window Computes the Kaiser window with window length window_length and shape parameter beta."
    },
    {
        "Y": "zero",
        "X": "Atleast_1d Returns a 1-dimensional view of each input tensor with what dimensions?",
        "Z": "atleast_1d Returns a 1-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "1-dimensional",
        "X": "At least_1d Returns a view of each input tensor with zero dimensions that is what?",
        "Z": "atleast_1d Returns a 1-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "atleast_1d",
        "X": "What returns a 1-dimensional view of each input tensor with zero dimensions?",
        "Z": "atleast_1d Returns a 1-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "zero",
        "X": "Atleast_2d Returns a 2-dimensional view of each input tensor with what dimensions?",
        "Z": "atleast_2d Returns a 2-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "atleast_2d",
        "X": "What returns a 2-dimensional view of each input tensor with zero dimensions?",
        "Z": "atleast_2d Returns a 2-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "atleast_3d",
        "X": "What returns a 3-dimensional view of each input tensor with zero dimensions?",
        "Z": "atleast_3d Returns a 3-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "zero",
        "X": "Atleast_3d Returns a 3-dimensional view of each input tensor with what dimensions?",
        "Z": "atleast_3d Returns a 3-dimensional view of each input tensor with zero dimensions."
    },
    {
        "Y": "bincount",
        "X": "What counts the frequency of each value in an array of non-negative ints?",
        "Z": "bincount Count the frequency of each value in an array of non-negative ints."
    },
    {
        "Y": "block_diag",
        "X": "What creates a block diagonal matrix  from provided tensors?",
        "Z": "block_diag Create a block diagonal matrix  from provided tensors."
    },
    {
        "Y": "provided tensors",
        "X": "block_diag Create a block diagonal matrix  from what?",
        "Z": "block_diag Create a block diagonal matrix  from provided tensors."
    },
    {
        "Y": "Broadcasting semantics",
        "X": "What is broadcast_tensors based on?",
        "Z": "broadcast_tensors Broadcasts the given tensors according toBroadcasting semantics."
    },
    {
        "Y": "shape shape",
        "X": "What shape is broadcast_to Broadcasts input to?",
        "Z": "broadcast_to Broadcasts input to the shape shape."
    },
    {
        "Y": "Broadcasts input ",
        "X": "What does broadcast_to the shape shape?",
        "Z": "broadcast_to Broadcasts input to the shape shape."
    },
    {
        "Y": "shapes",
        "X": "What type of broadcast is similar to broadcast_tensors?",
        "Z": "broadcast_shapes Similar to broadcast_tensors() but for shapes."
    },
    {
        "Y": "broadcast_tensors",
        "X": "What are broadcast_shapes similar to?",
        "Z": "broadcast_shapes Similar to broadcast_tensors() but for shapes."
    },
    {
        "Y": "shapes",
        "X": "What are broadcast_tensors?",
        "Z": "broadcast_shapes Similar to broadcast_tensors() but for shapes."
    },
    {
        "Y": "the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries",
        "X": "How are the boundaries of the buckets set by bucketize?",
        "Z": "bucketize Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries."
    },
    {
        "Y": "bucketize",
        "X": "What returns the indices of the buckets to which each value in the input belongs?",
        "Z": "bucketize Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries."
    },
    {
        "Y": " given sequence of tensors",
        "X": "Do cartesian product of the given sequence of what?",
        "Z": "cartesian_prod Do cartesian product of the given sequence of tensors."
    },
    {
        "Y": "cartesian product",
        "X": "What does cartesian_prod do of the given sequence of tensors?",
        "Z": "cartesian_prod Do cartesian product of the given sequence of tensors."
    },
    {
        "Y": "p-norm distance",
        "X": "What does cdist compute between each pair of the two collections of row vectors?",
        "Z": "cdist Computes batched the p-norm distance between each pair of the two collections of row vectors."
    },
    {
        "Y": "cdist",
        "X": "What Computes batched the p-norm distance between each pair of two collections of row vectors?",
        "Z": "cdist Computes batched the p-norm distance between each pair of the two collections of row vectors."
    },
    {
        "Y": "clone",
        "X": "What returns a copy of input?",
        "Z": "clone Returns a copy of input."
    },
    {
        "Y": "combinations",
        "X": "Compute combinations of lengthrrof the given tensor?",
        "Z": "combinations Compute combinations of lengthrrrof the given tensor."
    },
    {
        "Y": "vectors",
        "X": "cross Returns the cross product of what?",
        "Z": "cross Returns the cross product of vectors in dimension dimof input and other."
    },
    {
        "Y": "cummin",
        "X": "What returns a named tuple?",
        "Z": "cummin Returns a named tuple(values,indices)where values is the cumulative minimum of elements ofinput in the dimension dim."
    },
    {
        "Y": "cummax",
        "X": "What Returns a named tuple(values,indices)where values is the cumulative maximum of elements ofinput in the dimensiond",
        "Z": "cummax Returns a named tuple(values,indices)where values is the cumulative maximum of elements ofinput in the dimension dim."
    },
    {
        "Y": "cummin",
        "X": "What Returns a named tuple(values,indices)where values is the cumulative minimum of elements ofinput in the dimensiond",
        "Z": "cummin Returns a named tuple(values,indices)where values is the cumulative minimum of elements ofinput in the dimension dim."
    },
    {
        "Y": "cumulative product",
        "X": "What does cumprod return?",
        "Z": "cumprod Returns the cumulative product of elements ofinput in the dimension dim."
    },
    {
        "Y": "cumprod",
        "X": "What returns the cumulative product of elements ofinput in the dimension dim?",
        "Z": "cumprod Returns the cumulative product of elements ofinput in the dimension dim."
    },
    {
        "Y": "cumsum",
        "X": "What returns the cumulative sum of elements ofinput in the dimension dim?",
        "Z": "cumsum Returns the cumulative sum of elements ofinput in the dimension dim."
    },
    {
        "Y": "2-D square",
        "X": "diag Ifinputis a vector (1-D tensor), then returns a what tensor?",
        "Z": "diag Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "2-D square tensor",
        "X": "diag Ifinputis a vector (1-D tensor), then returns a what?",
        "Z": "diag Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "1-D tensor",
        "X": "What is a vector?",
        "Z": "diagflat Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "by input",
        "X": "How are the diagonals of certain 2D planes filled?",
        "Z": "diag_embed Creates a tensor whose diagonals of certain 2D planes (specified bydim1 anddim2) are filled by input."
    },
    {
        "Y": "diag_embed",
        "X": "What creates a tensor whose diagonals of certain 2D planes are filled by input?",
        "Z": "diag_embed Creates a tensor whose diagonals of certain 2D planes (specified bydim1 anddim2) are filled by input."
    },
    {
        "Y": "by input",
        "X": "diag_embed Creates a tensor whose diagonals of certain 2D planes are filled what?",
        "Z": "diag_embed Creates a tensor whose diagonals of certain 2D planes (specified bydim1 anddim2) are filled by input."
    },
    {
        "Y": "2-D square",
        "X": "diagflat Ifinputis a vector (1-D tensor), then returns a what tensor?",
        "Z": "diagflat Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "diagflat",
        "X": "Ifinputis a vector (1-D tensor), then returns a 2-D square tensor?",
        "Z": "diagflat Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "2-D square tensor",
        "X": "Ifinputis a vector (1-D tensor), then returns a what?",
        "Z": "diagflat Ifinputis a vector (1-D tensor), then returns a 2-D square tensor"
    },
    {
        "Y": "a partial view",
        "X": "What does diagonal Return return?",
        "Z": "diagonal Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape."
    },
    {
        "Y": "n-th",
        "X": "What is the forward difference along a given dimension?",
        "Z": "diff Computes the n-th forward difference along the given dimension."
    },
    {
        "Y": "n-th",
        "X": "What is the forward difference along the given dimension?",
        "Z": "diff Computes the n-th forward difference along the given dimension."
    },
    {
        "Y": "input operands along dimensions",
        "X": "Einsum Sums the product of the elements of what dimension specified using a notation based on the Einstein summation convention?",
        "Z": "einsum Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention."
    },
    {
        "Y": "Einstein summation convention",
        "X": "Einsum Sums the product of the elements of the input operands along dimensions specified using a notation based on what convention?",
        "Z": "einsum Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention."
    },
    {
        "Y": "input operands along",
        "X": "Einsum Sums the product of the elements of what dimensions specified using a notation based on the Einstein summation convention?",
        "Z": "einsum Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention."
    },
    {
        "Y": "flatten",
        "X": "What does Flattens input do by reshaping it into a one-dimensional tensor?",
        "Z": "flatten Flattens input by reshaping it into a one-dimensional tensor."
    },
    {
        "Y": "one-dimensional",
        "X": "What kind of tensor is Flattens input?",
        "Z": "flatten Flattens input by reshaping it into a one-dimensional tensor."
    },
    {
        "Y": "Flattens input into a one-dimensional tensor",
        "X": "What is done by reshaping it into a one-dimensional tensor?",
        "Z": "flatten Flattens input by reshaping it into a one-dimensional tensor."
    },
    {
        "Y": "reshaping it into a one-dimensional tensor",
        "X": "How does flatten Flattens input work?",
        "Z": "flatten Flattens input by reshaping it into a one-dimensional tensor."
    },
    {
        "Y": "dims",
        "X": "flip Reverse the order of a n-D tensor along given axis in what?",
        "Z": "flip Reverse the order of a n-D tensor along given axis in dims."
    },
    {
        "Y": "n-D",
        "X": "What is the order of a tensor along given axis in dims?",
        "Z": "flip Reverse the order of a n-D tensor along given axis in dims."
    },
    {
        "Y": "dims",
        "X": "In what order does flip Reverse the order of a n-D tensor along given axis?",
        "Z": "flip Reverse the order of a n-D tensor along given axis in dims."
    },
    {
        "Y": "fliplr",
        "X": "What flips tensor in the left/right direction?",
        "Z": "fliplr Flip tensor in the left/right direction, returning a new tensor."
    },
    {
        "Y": "tensor",
        "X": "What do fliplr Flip in the left/right direction?",
        "Z": "fliplr Flip tensor in the left/right direction, returning a new tensor."
    },
    {
        "Y": "flipud",
        "X": "What flips tensor in the up/down direction?",
        "Z": "flipud Flip tensor in the up/down direction, returning a new tensor."
    },
    {
        "Y": "flipud Flip tensor",
        "X": "What returns a new tensor?",
        "Z": "flipud Flip tensor in the up/down direction, returning a new tensor."
    },
    {
        "Y": "kron",
        "X": "What computes the Kronecker product, denoted byotimes, of input and other?",
        "Z": "kron Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of input and other."
    },
    {
        "Y": "kron",
        "X": "What denotes the Kronecker product?",
        "Z": "kron Computes the Kronecker product, denoted by\u2297\\otimes\u2297, of input and other."
    },
    {
        "Y": "n-D",
        "X": "rot90 Rotate a tensor by 90 degrees in the plane specified by dims axis?",
        "Z": "rot90 Rotate a n-D tensor by 90 degrees in the plane specified by dims axis."
    },
    {
        "Y": "dims axis",
        "X": "What specifies the plane of a n-D tensor?",
        "Z": "rot90 Rotate a n-D tensor by 90 degrees in the plane specified by dims axis."
    },
    {
        "Y": "dims",
        "X": "Rotate a n-D tensor by 90 degrees in the plane specified by what axis?",
        "Z": "rot90 Rotate a n-D tensor by 90 degrees in the plane specified by dims axis."
    },
    {
        "Y": "90",
        "X": "Rotate a n-D tensor by what degree in the plane specified by dims axis?",
        "Z": "rot90 Rotate a n-D tensor by 90 degrees in the plane specified by dims axis."
    },
    {
        "Y": "element-wise greatest common divisor",
        "X": "What does gcd stand for?",
        "Z": "gcd Computes the element-wise greatest common divisor (GCD) of input and other."
    },
    {
        "Y": "Computes the histogram of a tensor",
        "X": "What does histc do?",
        "Z": "histc Computes the histogram of a tensor."
    },
    {
        "Y": "histogram",
        "X": "What does histc compute of a tensor?",
        "Z": "histc Computes the histogram of a tensor."
    },
    {
        "Y": "tensor",
        "X": "histc Computes the histogram of what?",
        "Z": "histc Computes the histogram of a tensor."
    },
    {
        "Y": "theiiithgrid",
        "X": "What is defined by expanding the iiith input over dimensions defined by other inputs?",
        "Z": "meshgrid Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNNN-dimensional grids, where theiiithgrid is defined by expanding the iiith input over dimensions defined by other inputs."
    },
    {
        "Y": "scalar or 1-dimensional vector",
        "X": "What can each of the Take NNN tensors be?",
        "Z": "meshgrid Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNNN-dimensional grids, where theiiithgrid is defined by expanding the iiith input over dimensions defined by other inputs."
    },
    {
        "Y": "meshgrid",
        "X": "What are Take NNN tensors?",
        "Z": "meshgrid Take NNN tensors, each of which can be either scalar or 1-dimensional vector, and create NNNN-dimensional grids, where theiiithgrid is defined by expanding the iiith input over dimensions defined by other inputs."
    },
    {
        "Y": "lcm",
        "X": "What computes the element-wise least common multiple (LCM) of input and other?",
        "Z": "lcm Computes the element-wise least common multiple (LCM) of input and other."
    },
    {
        "Y": "least common multiple",
        "X": "What does lcm stand for?",
        "Z": "lcm Computes the element-wise least common multiple (LCM) of input and other."
    },
    {
        "Y": "logcumsumexp",
        "X": "What returns the logarithm of the cumulative summation of the exponentiation of elements ofinput in the dimension dim?",
        "Z": "logcumsumexp Returns the logarithm of the cumulative summation of the exponentiation of elements ofinput in the dimension dim."
    },
    {
        "Y": "tensor",
        "X": "ravel Return a contiguous flattened what?",
        "Z": "ravel Return a contiguous flattened tensor."
    },
    {
        "Y": "ravel",
        "X": "What animal returns a contiguous flattened tensor?",
        "Z": "ravel Return a contiguous flattened tensor."
    },
    {
        "Y": "ravel",
        "X": "What is the term for a contiguous flattened tensor?",
        "Z": "ravel Return a contiguous flattened tensor."
    },
    {
        "Y": "the value max norm",
        "X": "Thep-norm of the sub-tensor is lower than what?",
        "Z": "renorm Returns a tensor where each sub-tensor of inputalong dimension dimis normalized such that thep-norm of the sub-tensor is lower than the value max norm"
    },
    {
        "Y": "thep-norm",
        "X": "What of the sub-tensor is lower than the value max norm?",
        "Z": "renorm Returns a tensor where each sub-tensor of inputalong dimension dimis normalized such that thep-norm of the sub-tensor is lower than the value max norm"
    },
    {
        "Y": "repeat_interleave",
        "X": "What is repeat elements of a tensor?",
        "Z": "repeat_interleave Repeat elements of a tensor."
    },
    {
        "Y": "roll",
        "X": "What is the term for rolling the tensor along a given dimension?",
        "Z": "roll Roll the tensor along the given dimension(s)."
    },
    {
        "Y": "roll",
        "X": "What roll the tensor along the given dimension(s)?",
        "Z": "roll Roll the tensor along the given dimension(s)."
    },
    {
        "Y": "the order of the correspondinginnermostdimension withinsorted_sequence",
        "X": "What would be preserved if the corresponding values invalueswere inserted before the indices?",
        "Z": "searchsorted Find the indices from theinnermostdimension ofsorted_sequencesuch that, if the corresponding values invalueswere inserted before the indices, the order of the correspondinginnermostdimension withinsorted_sequencewould be preserved."
    },
    {
        "Y": "the order of the correspondinginnermostdimension withinsorted_sequence",
        "X": "What would be preserved if the corresponding values invalues were inserted before the indices?",
        "Z": "searchsorted Find the indices from theinnermostdimension ofsorted_sequencesuch that, if the corresponding values invalueswere inserted before the indices, the order of the correspondinginnermostdimension withinsorted_sequencewould be preserved."
    },
    {
        "Y": "tensordot",
        "X": "What returns a contraction of a and b over multiple dimensions?",
        "Z": "tensordot Returns a contraction of a and b over multiple dimensions."
    },
    {
        "Y": "a and b  over multiple dimensions",
        "X": "tensordot Returns a contraction of what?",
        "Z": "tensordot Returns a contraction of a and b over multiple dimensions."
    },
    {
        "Y": "tril",
        "X": "What returns the lower triangular part of the matrix ?",
        "Z": "tril Returns the lower triangular part of the matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "triangular",
        "X": "tril Returns the lower what part of the matrix ?",
        "Z": "tril Returns the lower triangular part of the matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "tril_indices",
        "X": "What returns the indices of the lower triangular part of arow-by-colmatrix  in a 2-by-N",
        "Z": "tril_indices Returns the indices of the lower triangular part of arow-by-colmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."
    },
    {
        "Y": "column coordinates",
        "X": "What does the second row of tril_indices contain?",
        "Z": "tril_indices Returns the indices of the lower triangular part of arow-by-colmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."
    },
    {
        "Y": "arow-by-colmatrix ",
        "X": "tril_indices Returns the indices of the lower triangular part of what?",
        "Z": "tril_indices Returns the indices of the lower triangular part of arow-by-colmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."
    },
    {
        "Y": "tensoroutare",
        "X": "What returns the other elements of the result?",
        "Z": "triu Returns the upper triangular part of a matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "2",
        "X": "How many tensors does triu return?",
        "Z": "triu Returns the upper triangular part of a matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "triu",
        "X": "What returns the upper triangular part of a matrix ?",
        "Z": "triu Returns the upper triangular part of a matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "triu",
        "X": "What returns the upper triangular part of a matrix  or batch of matrices?",
        "Z": "triu Returns the upper triangular part of a matrix  (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0."
    },
    {
        "Y": "triu_indices",
        "X": "What returns the indices of the upper triangular part of arowbycolmatrix  in a 2-by-N Tens",
        "Z": "triu_indices Returns the indices of the upper triangular part of arowbycolmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."
    },
    {
        "Y": "row coordinates",
        "X": "What does the first row of triu_indices contain?",
        "Z": "triu_indices Returns the indices of the upper triangular part of arowbycolmatrix  in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates."
    },
    {
        "Y": "Vandermonde",
        "X": "What type of matrix  does vander generate?",
        "Z": "vander Generates a Vandermonde matrix ."
    },
    {
        "Y": "vander",
        "X": "What generates a Vandermonde matrix ?",
        "Z": "vander Generates a Vandermonde matrix ."
    },
    {
        "Y": "view_as_real",
        "X": "What returns a view of iinput as a real tensor?",
        "Z": "view_as_real Returns a view of iinput as a real tensor."
    },
    {
        "Y": "a real tensor",
        "X": "What real value does view_as_real return?",
        "Z": "view_as_real Returns a view of iinput as a real tensor."
    },
    {
        "Y": "tensor",
        "X": "Returns a view of iinput as a complex what?",
        "Z": "view_as_complex Returns a view of iinput as a complex tensor."
    },
    {
        "Y": "view_as_complex",
        "X": "What returns a view of iinput as a complex tensor?",
        "Z": "view_as_complex Returns a view of iinput as a complex tensor."
    },
    {
        "Y": "tensor",
        "X": "What type of complex does view_as_complex return?",
        "Z": "view_as_complex Returns a view of iinput as a complex tensor."
    },
    {
        "Y": "addbmm",
        "X": "What performs a batch matrix -matrix  product of matrices stored in batch1 and batch2?",
        "Z": "addbmm Performs a batch matrix -matrix  product of matrices stored in batch1 and batch2, with a reduced add step (all matrix  multiplications get accumulated along the first dimension)."
    },
    {
        "Y": "in batch1 and batch2",
        "X": "Where are matrices stored?",
        "Z": "addbmm Performs a batch matrix -matrix  product of matrices stored in batch1 and batch2, with a reduced add step (all matrix  multiplications get accumulated along the first dimension)."
    },
    {
        "Y": "addbmm",
        "X": "What performs a batch matrix -matrix  product of matrices stored in batch1 and batch2 with a reduced add",
        "Z": "addbmm Performs a batch matrix -matrix  product of matrices stored in batch1 and batch2, with a reduced add step (all matrix  multiplications get accumulated along the first dimension)."
    },
    {
        "Y": "matrix  multiplication",
        "X": "What does addmm perform on the matricesmat1 and mat2?",
        "Z": "addmm Performs a matrix  multiplication of the matricesmat1 and mat2."
    },
    {
        "Y": "addmm",
        "X": "What performs a matrix  multiplication of the matricesmat1 and mat2?",
        "Z": "addmm Performs a matrix  multiplication of the matricesmat1 and mat2."
    },
    {
        "Y": "matrix  multiplication",
        "X": "addmm performs what of the matricesmat1 and mat2?",
        "Z": "addmm Performs a matrix  multiplication of the matricesmat1 and mat2."
    },
    {
        "Y": "matrix -vector product",
        "X": "addmv performs what of the matrix  mat and the vector vec?",
        "Z": "addmv Performs a matrix -vector product of the matrix  mat and the vector vec."
    },
    {
        "Y": "addmv",
        "X": "What performs a matrix -vector product of the matrix  mat and the vector vec?",
        "Z": "addmv Performs a matrix -vector product of the matrix  mat and the vector vec."
    },
    {
        "Y": "matrix  mat and",
        "X": "addmv Performs a matrix -vector product of what?",
        "Z": "addmv Performs a matrix -vector product of the matrix  mat and the vector vec."
    },
    {
        "Y": "addr",
        "X": "What performs the outer-product of vectorsvec1 andvec2and adds it to the matrix input?",
        "Z": "addr Performs the outer-product of vectorsvec1 andvec2and adds it to the matrix input."
    },
    {
        "Y": "baddbmm",
        "X": "What performs a batch matrix -matrix  product of matrices in batch1 and batch2?",
        "Z": "baddbmm Performs a batch matrix -matrix  product of matrices in batch1 and batch2."
    },
    {
        "Y": "bmm",
        "X": "What performs a batch matrix -matrix  product of matrices stored in input and mat2?",
        "Z": "bmm Performs a batch matrix -matrix  product of matrices stored in input and mat2."
    },
    {
        "Y": "matrices stored in input and mat2",
        "X": "bmm Performs a batch matrix -matrix  product of what?",
        "Z": "bmm Performs a batch matrix -matrix  product of matrices stored in input and mat2."
    },
    {
        "Y": "chain_matmul",
        "X": "What returns the matrix  product of the NNN2-D tensors?",
        "Z": "chain_matmul Returns the matrix  product of the NNN2-D tensors."
    },
    {
        "Y": "cholesky",
        "X": "What computes the Cholesky decomposition of a symmetric positive-definite matrix  AAA orfor batches of symmetric positive-definite ",
        "Z": "cholesky Computes the Cholesky decomposition of a symmetric positive-definite matrix  AAA orfor batches of symmetric positive-definite matrices."
    },
    {
        "Y": "cholesky_inverse",
        "X": "What computes the inverse of a symmetric positive-definite matrix ?",
        "Z": "cholesky_inverse Computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu: returns matric inv."
    },
    {
        "Y": "cholesky_inverse",
        "X": "What computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu?",
        "Z": "cholesky_inverse Computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu: returns matric inv."
    },
    {
        "Y": "matric inv",
        "X": "What does cholesky_inverse return?",
        "Z": "cholesky_inverse Computes the inverse of a symmetric positive-definite matirx AAA using its Cholesky factoruuu: returns matric inv."
    },
    {
        "Y": "Cholesky factor matrix uuu",
        "X": "cholesky_solve Solves a linear system of equations with a positive semidefinite matrix  to be inverted given what?",
        "Z": "cholesky_solve Solves a linear system of equations with a positive semidefinite matrix  to be inverted given its Cholesky factor matrix uuu."
    },
    {
        "Y": "cholesky_solve Solves",
        "X": "What is a linear system of equations with a positive semidefinite matrix  called?",
        "Z": "cholesky_solve Solves a linear system of equations with a positive semidefinite matrix  to be inverted given its Cholesky factor matrix uuu."
    },
    {
        "Y": "cholesky_solve",
        "X": "What Solves a linear system of equations with a positive semidefinite matrix  to be inverted given its Cholesky factor matrix uu",
        "Z": "cholesky_solve Solves a linear system of equations with a positive semidefinite matrix  to be inverted given its Cholesky factor matrix uuu."
    },
    {
        "Y": "dot product",
        "X": "What does dot compute of two 1D tensors?",
        "Z": "dot Computes the dot product of two 1D tensors."
    },
    {
        "Y": "dot",
        "X": "What computes the dot product of two 1D tensors?",
        "Z": "dot Computes the dot product of two 1D tensors."
    },
    {
        "Y": "two",
        "X": "How many 1D tensors does dot Compute?",
        "Z": "dot Computes the dot product of two 1D tensors."
    },
    {
        "Y": "eigenvalues and eigenvectors",
        "X": "eig Computes what of a real square matrix ?",
        "Z": "eig Computes the eigenvalues and eigenvectors of a real square matrix ."
    },
    {
        "Y": "eig",
        "X": "What Computes the eigenvalues and eigenvectors of a real square matrix ?",
        "Z": "eig Computes the eigenvalues and eigenvectors of a real square matrix ."
    },
    {
        "Y": "LAPACK",
        "X": "What is function that calls geqrf?",
        "Z": "geqrf This is a low-level function for calling LAPACK\u2019s geqrf directly."
    },
    {
        "Y": "calling LAPACK\u2019s geqrf directly",
        "X": "What is geqrf a low-level function for?",
        "Z": "geqrf This is a low-level function for calling LAPACK\u2019s geqrf directly."
    },
    {
        "Y": "low-level function",
        "X": "What is geqrf?",
        "Z": "geqrf This is a low-level function for calling LAPACK\u2019s geqrf directly."
    },
    {
        "Y": "Alias of torch.outer",
        "X": "What is ger?",
        "Z": "ger Alias of torch.outer()."
    },
    {
        "Y": "of torch.outer()",
        "X": "What does ger Alias do?",
        "Z": "ger Alias of torch.outer()."
    },
    {
        "Y": "1D tensors",
        "X": "inner Computes the dot product for what?",
        "Z": "inner Computes the dot product for 1D tensors."
    },
    {
        "Y": "inner",
        "X": "What computes the dot product for 1D tensors?",
        "Z": "inner Computes the dot product for 1D tensors."
    },
    {
        "Y": "Alias for torch.linalg.inv",
        "X": "What is the inverse of what?",
        "Z": "inverse Alias for torch.linalg.inv()"
    },
    {
        "Y": "Alias for torch.linalg.inv()",
        "X": "What is the inverse ?",
        "Z": "inverse Alias for torch.linalg.inv()"
    },
    {
        "Y": "Alias for torch.linalg.det",
        "X": "What is det?",
        "Z": "det Alias for torch.linalg.det()"
    },
    {
        "Y": "Alias for torch.linalg.det()",
        "X": "What is det?",
        "Z": "det Alias for torch.linalg.det()"
    },
    {
        "Y": "logdet",
        "X": "What calculates the log determinant of a square matrix  or batches of square matrices?",
        "Z": "logdet Calculates log determinant of a square matrix  or batches of square matrices."
    },
    {
        "Y": "log determinant",
        "X": "What does logdet calculate?",
        "Z": "logdet Calculates log determinant of a square matrix  or batches of square matrices."
    },
    {
        "Y": "Alias for torch.linalg.slogdet",
        "X": "What does slogdet stand for?",
        "Z": "slogdet Alias for torch.linalg.slogdet()"
    },
    {
        "Y": "for torch.linalg.slogdet",
        "X": "What is slogdet Alias?",
        "Z": "slogdet Alias for torch.linalg.slogdet()"
    },
    {
        "Y": "lstsq",
        "X": "What computes the solution to the least squares and least norm problems for a full rank matirx AAA ?",
        "Z": "lstsq Computes the solution to the least squares and least norm problems for a full rank matirx AAA of size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrix BBBof size(m\u00d7k)(m \\times k)(m\u00d7k)."
    },
    {
        "Y": "lstsq",
        "X": "What computes the solution to the least squares and least norm problems for a full rank matirx AAA of size(mn)(m",
        "Z": "lstsq Computes the solution to the least squares and least norm problems for a full rank matirx AAA of size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrix BBBof size(m\u00d7k)(m \\times k)(m\u00d7k)."
    },
    {
        "Y": "LU factorization",
        "X": "lu Computes what of a matrix  or batches of matricesA?",
        "Z": "lu Computes the LU factorization of a matrix  or batches of matricesA."
    },
    {
        "Y": "LU",
        "X": "lu Computes what factorization of a matrix  or batches of matricesA?",
        "Z": "lu Computes the LU factorization of a matrix  or batches of matricesA."
    },
    {
        "Y": "LU factorization of A fromtorch.lu()",
        "X": "What is the LU solve of the linear systemAx=bAx = bAx=busing?",
        "Z": "lu_solve Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu()."
    },
    {
        "Y": "LU solve",
        "X": "What does lu_solve return?",
        "Z": "lu_solve Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu()."
    },
    {
        "Y": "LU solve",
        "X": "What returns the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromt",
        "Z": "lu_solve Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu()."
    },
    {
        "Y": "lu_solve",
        "X": "What returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factor",
        "Z": "lu_solve Returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu()."
    },
    {
        "Y": "LU factorization",
        "X": "lu_unpack Unpacks the data and pivots from what of a tensor into tensorsLand",
        "Z": "lu_unpack Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu()."
    },
    {
        "Y": "LU_pivots",
        "X": "What does lu_unpack pivot from a LU factorization of a tensor into a permutation ten",
        "Z": "lu_unpack Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu()."
    },
    {
        "Y": "matmul matrix ",
        "X": "What is the product of two tensors?",
        "Z": "matmul matrix  product of two tensors."
    },
    {
        "Y": "two tensors",
        "X": "What is the matmul matrix  product of?",
        "Z": "matmul matrix  product of two tensors."
    },
    {
        "Y": "matmul",
        "X": "What is the matrix  product of two tensors?",
        "Z": "matmul matrix  product of two tensors."
    },
    {
        "Y": "Alias for torch.linalg.matrix _power",
        "X": "What does matrix _power stand for?",
        "Z": "matrix _power Alias for torch.linalg.matrix _power()"
    },
    {
        "Y": "matrix _power",
        "X": "What is the Alias for torch.linalg.matrix _power()?",
        "Z": "matrix _power Alias for torch.linalg.matrix _power()"
    },
    {
        "Y": "matrix _rank",
        "X": "What returns the numerical rank of a 2-D tensor?",
        "Z": "matrix _rank Returns the numerical rank of a 2-D tensor."
    },
    {
        "Y": "matrix _exp",
        "X": "What computes the matrix  exponential of a square matrix  or of each square matrix  in a batch?",
        "Z": "matrix _exp Computes the matrix  exponential of a square matrix  or of each square matrix  in a batch."
    },
    {
        "Y": "matrix  exponential",
        "X": "What does matrix _exp compute?",
        "Z": "matrix _exp Computes the matrix  exponential of a square matrix  or of each square matrix  in a batch."
    },
    {
        "Y": "mm",
        "X": "What performs a matrix  multiplication of the matricesinput and mat2?",
        "Z": "mm Performs a matrix  multiplication of the matricesinput and mat2."
    },
    {
        "Y": "matricesinput and mat2.",
        "X": "What does mm perform a matrix  multiplication of?",
        "Z": "mm Performs a matrix  multiplication of the matricesinput and mat2."
    },
    {
        "Y": "matrix -vector product",
        "X": "What does mv perform of the matrix input and the vector vec?",
        "Z": "mv Performs a matrix -vector product of the matrix input and the vector vec."
    },
    {
        "Y": "vector vec",
        "X": "mv Performs a matrix -vector product of the matrix input and what?",
        "Z": "mv Performs a matrix -vector product of the matrix input and the vector vec."
    },
    {
        "Y": "matrix -vector",
        "X": "mv Performs what product of the matrix input and the vector vec?",
        "Z": "mv Performs a matrix -vector product of the matrix input and the vector vec."
    },
    {
        "Y": "orgqr",
        "X": "What is Alias for torch.linalg.householder_product()?",
        "Z": "orgqr Alias for torch.linalg.householder_product()."
    },
    {
        "Y": "for torch.linalg.householder_product()",
        "X": "What is orgqr Alias?",
        "Z": "orgqr Alias for torch.linalg.householder_product()."
    },
    {
        "Y": "ormqr",
        "X": "What computes the matrix -matrix  multiplication of a product of Householder matrices with a general matrix ?",
        "Z": "ormqr Computes the matrix -matrix  multiplication of a product of Householder matrices with a general matrix ."
    },
    {
        "Y": "matrix -matrix ",
        "X": "What is the multiplication of a product of Householder matrices with a general matrix ?",
        "Z": "ormqr Computes the matrix -matrix  multiplication of a product of Householder matrices with a general matrix ."
    },
    {
        "Y": "Outer product of input andvec2",
        "X": "What is the outer product of input andvec2?",
        "Z": "outer Outer product of input andvec2."
    },
    {
        "Y": "Alias for torch.linalg.pinv",
        "X": "What is file that contains the name Alias for torch.linalg.pinv?",
        "Z": "pinverse Alias for torch.linalg.pinv()"
    },
    {
        "Y": "pinverse",
        "X": "What is Alias for torch.linalg.pinv()?",
        "Z": "pinverse Alias for torch.linalg.pinv()"
    },
    {
        "Y": "qr",
        "X": "What computes the QR decomposition of a matrix  or a batch of matricesinput?",
        "Z": "qr Computes the QR decomposition of a matrix  or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix  or batch of orthogonal matrices andRRRbeing an upper triangular matrix  or batch of upper triangular matrices."
    },
    {
        "Y": "a named tuple",
        "X": "What does qr return of tensors?",
        "Z": "qr Computes the QR decomposition of a matrix  or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix  or batch of orthogonal matrices andRRRbeing an upper triangular matrix  or batch of upper triangular matrices."
    },
    {
        "Y": "qr",
        "X": "What Computes the QR decomposition of a matrix  or a batch of matricesinput?",
        "Z": "qr Computes the QR decomposition of a matrix  or a batch of matricesinput, and returns a named tuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix  or batch of orthogonal matrices andRRRbeing an upper triangular matrix  or batch of upper triangular matrices."
    },
    {
        "Y": "LU",
        "X": "What is the factorization of A?",
        "Z": "solve This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a named tuplesolution, LU."
    },
    {
        "Y": "the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A",
        "X": "What does solve This function return the solution to?",
        "Z": "solve This function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A, in order as a named tuplesolution, LU."
    },
    {
        "Y": "svd",
        "X": "What computes the singular value decomposition of either a matrix  or batch of matricesinput?",
        "Z": "svd Computes the singular value decomposition of either a matrix  or batch of matricesinput."
    },
    {
        "Y": "svd_lowrank",
        "X": "What returns the singular value decomposition(U,S,V)of a matrix , batches of matrices, or a spar",
        "Z": "svd_lowrank Return the singular value decomposition(U,S,V)of a matrix , batches of matrices, or a sparse matirx AAA such thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT."
    },
    {
        "Y": "pca_lowrank",
        "X": "What performs linear Principal Component Analysis on a low-rank matrix ?",
        "Z": "pca_lowrank Performs linear Principal Component Analysis (PCA) on a low-rank matrix , batches of such matrices, or sparse matrix ."
    },
    {
        "Y": "linear Principal Component Analysis",
        "X": "What does PCA stand for?",
        "Z": "pca_lowrank Performs linear Principal Component Analysis (PCA) on a low-rank matrix , batches of such matrices, or sparse matrix ."
    },
    {
        "Y": "a named tuple",
        "X": "What does symeig return eigenvalues and eigenvectors of a real symmetric or complex Hermit",
        "Z": "symeig This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor a batch thereof, represented by a named tuple (eigenvalues, eigenvectors)."
    },
    {
        "Y": "symmetric or complex",
        "X": "symeig returns eigenvalues and eigenvectors of what kind of real Hermitian matrix input",
        "Z": "symeig This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor a batch thereof, represented by a named tuple (eigenvalues, eigenvectors)."
    },
    {
        "Y": "symeig",
        "X": "What function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor",
        "Z": "symeig This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor a batch thereof, represented by a named tuple (eigenvalues, eigenvectors)."
    },
    {
        "Y": "a batch",
        "X": "symeig returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian",
        "Z": "symeig This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor a batch thereof, represented by a named tuple (eigenvalues, eigenvectors)."
    },
    {
        "Y": "eigenvalues, eigenvectors",
        "X": "What does a named tuple return?",
        "Z": "symeig This function returns eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrix inputor a batch thereof, represented by a named tuple (eigenvalues, eigenvectors)."
    },
    {
        "Y": "matrix -free LOBPCG methods",
        "X": "What is used to find the k largest eigenvalues and the corresponding eigenvectors of a symmetric positive defined",
        "Z": "lobpcg Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix -free LOBPCG methods."
    },
    {
        "Y": "k largest (or smallest) eigenvalues",
        "X": "What does lobpcg find in a symmetric positive defined generalized eigenvalue problem?",
        "Z": "lobpcg Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix -free LOBPCG methods."
    },
    {
        "Y": "matrix -free",
        "X": "What type of LOBPCG methods are used to find the k largest (or smallest) eigenvalues?",
        "Z": "lobpcg Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix -free LOBPCG methods."
    },
    {
        "Y": "eigenvalues",
        "X": "lobpcg Find the k largest (or smallest) what?",
        "Z": "lobpcg Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix -free LOBPCG methods."
    },
    {
        "Y": "trapezoid rule",
        "X": "What rule is used to Estimateydxint y,dxydxalongdim?",
        "Z": "trapz Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule."
    },
    {
        "Y": "triangular_solve",
        "X": "What Solves a system of equations with a triangular coefficient matirx AAA ?",
        "Z": "triangular_solve Solves a system of equations with a triangular coefficient matirx AAA and multiple right-hand sidesbbb."
    },
    {
        "Y": "triangular",
        "X": "What _solve Solves a system of equations with a triangular coefficient matirx AAA and multiple right-hand sidesb",
        "Z": "triangular_solve Solves a system of equations with a triangular coefficient matirx AAA and multiple right-hand sidesbbb."
    },
    {
        "Y": "dot product",
        "X": "What does vdot compute of two 1D tensors?",
        "Z": "vdot Computes the dot product of two 1D tensors."
    },
    {
        "Y": "vdot",
        "X": "What Computes the dot product of two 1D tensors?",
        "Z": "vdot Computes the dot product of two 1D tensors."
    },
    {
        "Y": "_GLIBCXX_USE_CXX11_ABI=1",
        "X": "What does PyTorch return if it was built with?",
        "Z": "compiled_with_cxx11_abi Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"
    },
    {
        "Y": "PyTorch",
        "X": "What was built with _GLIBCXX_USE_CXX11_ABI=1?",
        "Z": "compiled_with_cxx11_abi Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"
    },
    {
        "Y": "_GLIBCXX_USE_CXX11_ABI=1",
        "X": "What does compiled_with_cxx11_abi return if PyTorch was built with?",
        "Z": "compiled_with_cxx11_abi Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"
    },
    {
        "Y": "result_type",
        "X": "What returns The torch.dtype that would result from performing an arithmetic operation on the provided input tensors?",
        "Z": "result_type Returns The torch.dtypethat would result from performing an arithmetic operation on the provided input tensors."
    },
    {
        "Y": "can_cast",
        "X": "What determines if a type conversion is allowed under PyTorch casting rules?",
        "Z": "can_cast Determines if a type conversion is allowed under PyTorch casting rules described in the type promotiondocumentation."
    },
    {
        "Y": "type promotiondocumentation",
        "X": "What document describes PyTorch casting rules?",
        "Z": "can_cast Determines if a type conversion is allowed under PyTorch casting rules described in the type promotiondocumentation."
    },
    {
        "Y": "smallest size and scalar kind",
        "X": "What kind of type does The torch.dtype have?",
        "Z": "promote_types Returns The torch.dtypewith the smallest size and scalar kind that is not smaller nor of lower kind than eithertype1ortype2."
    },
    {
        "Y": "eithertype1ortype2",
        "X": "The torch.dtype returns the smallest size and scalar kind that is not smaller nor lower than what?",
        "Z": "promote_types Returns The torch.dtypewith the smallest size and scalar kind that is not smaller nor of lower kind than eithertype1ortype2."
    },
    {
        "Y": "use_deterministic_algorithms",
        "X": "What sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms?",
        "Z": "use_deterministic_algorithms Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms."
    },
    {
        "Y": "Returns True",
        "X": "What happens if the global deterministic flag is turned on?",
        "Z": "are_deterministic_algorithms_enabled Returns True if the global deterministic flag is turned on."
    },
    {
        "Y": "are_deterministic_algorithms_enabled",
        "X": "What returns true if the global deterministic flag is turned on?",
        "Z": "are_deterministic_algorithms_enabled Returns True if the global deterministic flag is turned on."
    },
    {
        "Y": "once",
        "X": "How many times per process may PyTorch warnings appear?",
        "Z": "set_warn_always When this flag is False (default) then some PyTorch warnings may only appear once per process."
    },
    {
        "Y": "False",
        "X": "If set_warn_always is what, then some PyTorch warnings may only appear once per process?",
        "Z": "set_warn_always When this flag is False (default) then some PyTorch warnings may only appear once per process."
    },
    {
        "Y": "once",
        "X": "When set_warn_always is False, some PyTorch warnings may only appear how many times per process?",
        "Z": "set_warn_always When this flag is False (default) then some PyTorch warnings may only appear once per process."
    },
    {
        "Y": "False",
        "X": "If set_warn_always is set to what flag (default) then some PyTorch warnings may only appear once per process?",
        "Z": "set_warn_always When this flag is False (default) then some PyTorch warnings may only appear once per process."
    },
    {
        "Y": "Returns True",
        "X": "What happens if the global warn_always flag is turned on?",
        "Z": "is_warn_always_enabled Returns True if the global warn_always flag is turned on."
    },
    {
        "Y": "is_warn_always_enabled",
        "X": "What returns true if the global warn_always flag is turned on?",
        "Z": "is_warn_always_enabled Returns True if the global warn_always flag is turned on."
    },
    {
        "Y": "symbolically traceable",
        "X": "_assert A wrapper around Python's assert which is what?",
        "Z": "_assert A wrapper around Python\u2019s assert which is symbolically traceable."
    },
    {
        "Y": "_assert",
        "X": "What is a wrapper around Python's assert?",
        "Z": "_assert A wrapper around Python\u2019s assert which is symbolically traceable."
    },
    {
        "Y": "1D",
        "X": "conv1d Applies what type of convolution over an input signal composed of several input planes?",
        "Z": "conv1d Applies a 1D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn.Conv1d",
        "X": "What Applies a 1D convolution over an input signal composed of several input planes?",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "What type of convolution does conv2d apply?",
        "Z": "conv2d Applies a 2D convolution over an input image composed of several input planes."
    },
    {
        "Y": "conv2d",
        "X": "What Applies a 2D convolution over an input image composed of several input planes?",
        "Z": "conv2d Applies a 2D convolution over an input image composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What type of convolution does conv3d apply?",
        "Z": "conv3d Applies a 3D convolution over an input image composed of several input planes."
    },
    {
        "Y": "conv3d",
        "X": "What applies a 3D convolution over an input image composed of several input planes?",
        "Z": "conv3d Applies a 3D convolution over an input image composed of several input planes."
    },
    {
        "Y": "deconvolution",
        "X": "What is a 1D transposed convolution operator?",
        "Z": "conv_transpose1d Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "1D",
        "X": "Conv_transpose1d Applies a transposed convolution operator over an input signal composed of several input planes?",
        "Z": "conv_transpose1d Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "deconvolution",
        "X": "What is conv_transpose1d?",
        "Z": "conv_transpose1d Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "conv_transpose1d",
        "X": "What applies a 1D transposed convolution operator over an input signal composed of several input planes?",
        "Z": "conv_transpose1d Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "deconvolution",
        "X": "What is conv_transpospos2d?",
        "Z": "conv_transpose2d Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "conv_transpose2d",
        "X": "What applies a 2D transposed convolution operator over an input image composed of several input planes?",
        "Z": "conv_transpose2d Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "deconvolution",
        "X": "What is conv_transpose2d?",
        "Z": "conv_transpose2d Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d."
    },
    {
        "Y": "deconvolution",
        "X": "What is conv_transpospos3d?",
        "Z": "conv_transpose3d Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d"
    },
    {
        "Y": "3D",
        "X": "What type of transposed convolution operator does conv_transpose3d use?",
        "Z": "conv_transpose3d Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d"
    },
    {
        "Y": "deconvolution",
        "X": "What is conv_transpose3d?",
        "Z": "conv_transpose3d Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d"
    },
    {
        "Y": "conv_transpose3d",
        "X": "What applies a 3D transposed convolution operator over an input image composed of several input planes?",
        "Z": "conv_transpose3d Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d"
    },
    {
        "Y": "batched input tensor",
        "X": "What type of input tensor does unfold extract local blocks from?",
        "Z": "unfold Extracts sliding local blocks from a batched input tensor."
    },
    {
        "Y": "sliding local blocks",
        "X": "What does unfold extract from a batched input tensor?",
        "Z": "unfold Extracts sliding local blocks from a batched input tensor."
    },
    {
        "Y": "local blocks",
        "X": "What do unfold Extracts sliding from a batched input tensor?",
        "Z": "unfold Extracts sliding local blocks from a batched input tensor."
    },
    {
        "Y": "tensor",
        "X": "fold Combines an array of sliding local blocks into a large containing what?",
        "Z": "fold Combines an array of sliding local blocks into a large containing tensor."
    },
    {
        "Y": "1D",
        "X": "avg_pool1d Applies what kind of pooling over an input signal composed of several input planes?",
        "Z": "avg_pool1d Applies a 1D average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.AvgPool1d",
        "X": "What Applies a 1D average pooling over an input signal composed of several input planes?",
        "Z": "nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "avg_pool2d",
        "X": "What Applies 2D average-pooling operation inkHkWkH times kWkHkWregions by step",
        "Z": "avg_pool2d Applies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps."
    },
    {
        "Y": "avg_pool3d",
        "X": "What Applies 3D average-pooling operation inkTkHkWkT times kH times",
        "Z": "avg_pool3d Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps."
    },
    {
        "Y": "3D",
        "X": "How does avg_pool3d apply?",
        "Z": "avg_pool3d Applies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps."
    },
    {
        "Y": "1D",
        "X": "max_pool1d Applies a max pooling over an input signal composed of several input planes?",
        "Z": "max_pool1d Applies a 1D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "a 1D max pooling",
        "X": "max_pool1d Applies what over an input signal?",
        "Z": "max_pool1d Applies a 1D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.MaxPool2d",
        "X": "What applies a 2D max pooling over an input signal composed of several input planes?",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "max_pool3d Applies a what kind of max pooling over an input signal composed of several input planes?",
        "Z": "max_pool3d Applies a 3D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.MaxPool3d",
        "X": "What applies a 3D max pooling over an input signal composed of several input planes?",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "MaxUnpool1d",
        "X": "What computes a partial inverse of MaxPool1d?",
        "Z": "nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d."
    },
    {
        "Y": "inverse",
        "X": "max_unpool1d Computes a partial what of MaxPool1d?",
        "Z": "max_unpool1d Computes a partial inverse ofMaxPool1d."
    },
    {
        "Y": "MaxUnpool2d",
        "X": "What computes a partial inverse of MaxPool2d?",
        "Z": "nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d."
    },
    {
        "Y": "inverse",
        "X": "max_unpool2d Computes a partial what of MaxPool2d?",
        "Z": "max_unpool2d Computes a partial inverse ofMaxPool2d."
    },
    {
        "Y": "MaxUnpool3d",
        "X": "What computes a partial inverse of MaxPool3d?",
        "Z": "nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d."
    },
    {
        "Y": "inverse",
        "X": "max_unpool3d Computes a partial what of MaxPool3d?",
        "Z": "max_unpool3d Computes a partial inverse ofMaxPool3d."
    },
    {
        "Y": "1D",
        "X": "lp_pool1d Applies a power-average pooling over an input signal composed of several input planes?",
        "Z": "lp_pool1d Applies a 1D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.LPPool1d",
        "X": "What applies a 1D power-average pooling over an input signal composed of several input planes?",
        "Z": "nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.LPPool2d",
        "X": "What applies a 2D power-average pooling over an input signal composed of several input planes?",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "1D",
        "X": "How large is adaptive_max_pool1d?",
        "Z": "adaptive_max_pool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveMaxPool1d",
        "X": "What applies a 1D adaptive max pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "What type of pooling does adaptive_max_pool2d apply?",
        "Z": "adaptive_max_pool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveMaxPool2d",
        "X": "What applies a 2D adaptive max pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What kind of pooling does adaptive_max_pool3d apply?",
        "Z": "adaptive_max_pool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveMaxPool3d",
        "X": "What applies a 3D adaptive max pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "1D",
        "X": "How large is adaptive_avg_pool1d?",
        "Z": "adaptive_avg_pool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveAvgPool1d",
        "X": "What applies a 1D adaptive average pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "What type of pooling does adaptive_avg_pool2d apply?",
        "Z": "adaptive_avg_pool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveAvgPool2d",
        "X": "What applies a 2D adaptive average pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What kind of pooling does adaptive_avg_pool3d apply?",
        "Z": "adaptive_avg_pool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "AdaptiveAvgPool3d",
        "X": "What applies a 3D adaptive average pooling over an input signal composed of several input planes?",
        "Z": "nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "What type of fractional max pooling does fractional_max_pool2d apply?",
        "Z": "fractional_max_pool2d Applies 2D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "fractional_max_pool2d",
        "X": "What applies 2D fractional max pooling over an input signal composed of several input planes?",
        "Z": "fractional_max_pool2d Applies 2D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What kind of fractional max pooling does fractional_max_pool3d apply?",
        "Z": "fractional_max_pool3d Applies 3D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "fractional_max_pool3d",
        "X": "What applies 3D fractional max pooling over an input signal composed of several input planes?",
        "Z": "fractional_max_pool3d Applies 3D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "Thresholds",
        "X": "What is the threshold for each element of the input Tensor?",
        "Z": "threshold Thresholds each element of the input Tensor."
    },
    {
        "Y": "nn.Threshold Thresholds",
        "X": "What is the term for each element of the input Tensor?",
        "Z": "nn.Threshold Thresholds each element of the input Tensor."
    },
    {
        "Y": "threshold_ In-place version ofthreshold()",
        "X": "What is function that determines the threshold?",
        "Z": "threshold_ In-place version ofthreshold()."
    },
    {
        "Y": "threshold_ In-place",
        "X": "What version of ofthreshold() is used?",
        "Z": "threshold_ In-place version ofthreshold()."
    },
    {
        "Y": "rectified linear unit function",
        "X": "What does relu apply element-wise?",
        "Z": "relu Applies the rectified linear unit function element-wise."
    },
    {
        "Y": "element-wise",
        "X": "Relu Applies the rectified linear unit function what?",
        "Z": "relu Applies the rectified linear unit function element-wise."
    },
    {
        "Y": "relu_ In-place version ofrelu()",
        "X": "What does relu_ In-place version ofrelu() do?",
        "Z": "relu_ In-place version ofrelu()."
    },
    {
        "Y": "hardtanh",
        "X": "What Applies the HardTanh function element-wise?",
        "Z": "hardtanh Applies the HardTanh function element-wise."
    },
    {
        "Y": "HardTanh",
        "X": "What function does hardtanh apply element-wise?",
        "Z": "hardtanh Applies the HardTanh function element-wise."
    },
    {
        "Y": "element-wise",
        "X": "How does hardtanh apply the HardTanh function?",
        "Z": "hardtanh Applies the HardTanh function element-wise."
    },
    {
        "Y": "hardtanh_ In-place version ofhardtanh()",
        "X": "What does hardtanh do?",
        "Z": "hardtanh_ In-place version ofhardtanh()."
    },
    {
        "Y": "hardtanh",
        "X": "What is In-place version of hardtanh()?",
        "Z": "hardtanh_ In-place version ofhardtanh()."
    },
    {
        "Y": "element-wise",
        "X": "How is the hardswish function described in the paper?",
        "Z": "nn.Hardswish Applies the hardswish function, element-wise, as described in the paper:"
    },
    {
        "Y": "hardswish",
        "X": "What applies the hardswish function, element-wise?",
        "Z": "hardswish Applies the hardswish function, element-wise, as described in the paper:"
    },
    {
        "Y": "element-wise function",
        "X": "What type of function does relu6 apply?",
        "Z": "relu6 Applies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6)."
    },
    {
        "Y": "elu",
        "X": "What Applies element-wise,ELU(x)=max(0,x)+min(0,(exp",
        "Z": "elu Applies element-wise,ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))."
    },
    {
        "Y": "In-place",
        "X": "What is version of elu?",
        "Z": "elu_ In-place version ofelu()."
    },
    {
        "Y": "elu_ In-place",
        "X": "What is version of ofelu()?",
        "Z": "elu_ In-place version ofelu()."
    },
    {
        "Y": "tanh",
        "X": "What Applies element-wise?",
        "Z": "tanh Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b"
    },
    {
        "Y": "selu",
        "X": "What Applies element-wise,SELU(x)=scale(max(0,x)+min(0,(",
        "Z": "selu Applies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946."
    },
    {
        "Y": "celu",
        "X": "What Applies element-wise,CELU(x)=max(0,x)+min(0,(exp",
        "Z": "celu Applies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121))."
    },
    {
        "Y": "leaky_relu",
        "X": "What applies element-wise?",
        "Z": "leaky_relu Applies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)"
    },
    {
        "Y": "leaky_relu",
        "X": "What is In-place version of leaky_relu()?",
        "Z": "leaky_relu_ In-place version ofleaky_relu()."
    },
    {
        "Y": "leaky_relu",
        "X": "What is in-place version of leaky_relu()?",
        "Z": "leaky_relu_ In-place version ofleaky_relu()."
    },
    {
        "Y": "leaky_relu()",
        "X": "What is the leaky_relu_ In-place version of?",
        "Z": "leaky_relu_ In-place version ofleaky_relu()."
    },
    {
        "Y": "prelu",
        "X": "What Applies element-wise the functionPReLU(x)=max(0,x)+weightmin(0,x",
        "Z": "prelu Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter."
    },
    {
        "Y": "weight",
        "X": "Prelu Applies element-wise the functionPReLU(x)=max(0,x)+ what?",
        "Z": "prelu Applies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter."
    },
    {
        "Y": "Randomized leaky ReLU",
        "X": "What is rrelu?",
        "Z": "rrelu Randomized leaky ReLU."
    },
    {
        "Y": "rrelu_ In-place version ofrrelu()",
        "X": "What is rrelu_ In-place version ofrrelu()?",
        "Z": "rrelu_ In-place version ofrrelu()."
    },
    {
        "Y": "rrelu_ In-place",
        "X": "What is version of ofrrelu()?",
        "Z": "rrelu_ In-place version ofrrelu()."
    },
    {
        "Y": "gated linear unit",
        "X": "What is glu?",
        "Z": "glu The gated linear unit."
    },
    {
        "Y": "gated linear unit",
        "X": "What type of unit is glu?",
        "Z": "glu The gated linear unit."
    },
    {
        "Y": "x",
        "X": "What is the value of the functionGELU?",
        "Z": "gelu Applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)"
    },
    {
        "Y": "logsigmoid",
        "X": "What Applies element-wiseLogSigmoid(xi)=log(11+exp(xi))text",
        "Z": "logsigmoid Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)"
    },
    {
        "Y": "element-wise",
        "X": "How does logsigmoid Applies?",
        "Z": "logsigmoid Applies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)"
    },
    {
        "Y": "hard shrinkage",
        "X": "What function does hardshrink apply?",
        "Z": "hardshrink Applies the hard shrinkage function element-wise"
    },
    {
        "Y": "hardshrink",
        "X": "What applies the hard shrinkage function element-wise?",
        "Z": "hardshrink Applies the hard shrinkage function element-wise"
    },
    {
        "Y": "hard shrinkage",
        "X": "hardshrink Applies what function element-wise?",
        "Z": "hardshrink Applies the hard shrinkage function element-wise"
    },
    {
        "Y": "x",
        "X": "What is tanhshrink(x)=xTanh(x)textTanhshrink",
        "Z": "tanhshrink Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)"
    },
    {
        "Y": "x",
        "X": "What is the element-wise value of tanhshrink?",
        "Z": "tanhshrink Applies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)"
    },
    {
        "Y": "softsign",
        "X": "What does the functionSoftSign(x)=x1+xtextSoftSign(x) = frac",
        "Z": "softsign Applies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b"
    },
    {
        "Y": "softplus",
        "X": "What Applies element-wise, the functionSoftplus(x)=1log(1+exp(x))",
        "Z": "softplus Applies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x))."
    },
    {
        "Y": "softmin function",
        "X": "What does softmin Apply?",
        "Z": "softmin Applies a softmin function."
    },
    {
        "Y": "softmin",
        "X": "What Applies a softmin function?",
        "Z": "softmin Applies a softmin function."
    },
    {
        "Y": "softmax function",
        "X": "What does softmax Apply?",
        "Z": "softmax Applies a softmax function."
    },
    {
        "Y": "softmax",
        "X": "What Applies a softmax function?",
        "Z": "sparse.softmax Applies a softmax function."
    },
    {
        "Y": "soft shrinkage",
        "X": "What function does softshrink apply?",
        "Z": "softshrink Applies the soft shrinkage function element wise"
    },
    {
        "Y": "softshrink",
        "X": "What applies the soft shrinkage function element wise?",
        "Z": "softshrink Applies the soft shrinkage function element wise"
    },
    {
        "Y": "gumbel_softmax Samples",
        "X": "What is a sample of the Gumbel-Softmax distribution?",
        "Z": "gumbel_softmax Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes."
    },
    {
        "Y": "gumbel_softmax",
        "X": "What is distribution that contains samples from the Gumbel-Softmax distribution?",
        "Z": "gumbel_softmax Samples from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretizes."
    },
    {
        "Y": "log_softmax",
        "X": "What Applies a softmax followed by a logarithm?",
        "Z": "log_softmax Applies a softmax followed by a logarithm."
    },
    {
        "Y": "logarithm",
        "X": "What is followed by a log_softmax?",
        "Z": "log_softmax Applies a softmax followed by a logarithm."
    },
    {
        "Y": "logarithm",
        "X": "What is followed by a softmax?",
        "Z": "log_softmax Applies a softmax followed by a logarithm."
    },
    {
        "Y": "x",
        "X": "tanh(x)=tanh(x)=exp(x)exp(x)+ex",
        "Z": "tanh Applies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b"
    },
    {
        "Y": "element-wise function",
        "X": "What type of function does sigmoid apply?",
        "Z": "sigmoid Applies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b"
    },
    {
        "Y": "nn.Tanhshrink",
        "X": "What Applies the element-wise function?",
        "Z": "nn.Tanhshrink Applies the element-wise function:"
    },
    {
        "Y": "nn.Tanh",
        "X": "What applies the element-wise function?",
        "Z": "nn.Tanh Applies the element-wise function:"
    },
    {
        "Y": "element-wise",
        "X": "How does silu apply the Sigmoid Linear Unit function?",
        "Z": "silu Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
    },
    {
        "Y": "Sigmoid Linear Unit",
        "X": "What does SiLU stand for?",
        "Z": "nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
    },
    {
        "Y": "silu",
        "X": "What Applies the Sigmoid Linear Unit (SiLU) function, element-wise?",
        "Z": "silu Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
    },
    {
        "Y": "element-wise",
        "X": "How does silu apply the SiLU function?",
        "Z": "silu Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
    },
    {
        "Y": "Mish function",
        "X": "What does mish apply element-wise?",
        "Z": "mish Applies the Mish function, element-wise."
    },
    {
        "Y": "element-wise",
        "X": "What aspect of the Mish function is mish applied?",
        "Z": "mish Applies the Mish function, element-wise."
    },
    {
        "Y": "element-wise",
        "X": "How does mish apply the Mish function?",
        "Z": "mish Applies the Mish function, element-wise."
    },
    {
        "Y": "Mish",
        "X": "What function does mish apply element-wise?",
        "Z": "mish Applies the Mish function, element-wise."
    },
    {
        "Y": "Batch Normalization",
        "X": "What does batch_norm apply for each channel across a batch of data?",
        "Z": "batch_norm Applies Batch Normalization for each channel across a batch of data."
    },
    {
        "Y": "batch_norm",
        "X": "What Applies Batch Normalization for each channel across a batch of data?",
        "Z": "batch_norm Applies Batch Normalization for each channel across a batch of data."
    },
    {
        "Y": "group_norm",
        "X": "What Applies Group Normalization for last certain number of dimensions?",
        "Z": "group_norm Applies Group Normalization for last certain number of dimensions."
    },
    {
        "Y": "Group Normalization",
        "X": "group_norm Applies what for last certain number of dimensions?",
        "Z": "group_norm Applies Group Normalization for last certain number of dimensions."
    },
    {
        "Y": "instance_norm",
        "X": "What Applies Instance Normalization for each channel in each data sample in a batch?",
        "Z": "instance_norm Applies Instance Normalization for each channel in each data sample in a batch."
    },
    {
        "Y": "instance_norm",
        "X": "What applies Instance Normalization for each channel in each data sample in a batch?",
        "Z": "instance_norm Applies Instance Normalization for each channel in each data sample in a batch."
    },
    {
        "Y": "layer_norm",
        "X": "What Applies Layer Normalization for last certain number of dimensions?",
        "Z": "layer_norm Applies Layer Normalization for last certain number of dimensions."
    },
    {
        "Y": "Layer Normalization",
        "X": "What does layer_norm apply for last certain number of dimensions?",
        "Z": "layer_norm Applies Layer Normalization for last certain number of dimensions."
    },
    {
        "Y": "layer_norm",
        "X": "What applies Layer Normalization for last certain number of dimensions?",
        "Z": "layer_norm Applies Layer Normalization for last certain number of dimensions."
    },
    {
        "Y": "Layer Normalization",
        "X": "Layer_norm Applies what for last certain number of dimensions?",
        "Z": "layer_norm Applies Layer Normalization for last certain number of dimensions."
    },
    {
        "Y": "local response normalization",
        "X": "What does local_response_norm apply over an input signal composed of several input planes?",
        "Z": "local_response_norm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "channels occupy the second dimension",
        "X": "What does local_response_norm apply to an input signal composed of several input planes?",
        "Z": "local_response_norm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "channels occupy the second dimension",
        "X": "What is the second dimension of the input planes?",
        "Z": "local_response_norm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "PerformsLpL_pLp",
        "X": "What normalizes the normalization of inputs over a specified dimension?",
        "Z": "normalize PerformsLpL_pLp\u200bnormalization of inputs over specified dimension."
    },
    {
        "Y": "normalize",
        "X": "What does PerformsLpL_pLp normalization of inputs over specified dimension?",
        "Z": "normalize PerformsLpL_pLp\u200bnormalization of inputs over specified dimension."
    },
    {
        "Y": "linear",
        "X": "What type of transformation applies a linear transformation to the incoming data?",
        "Z": "linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b."
    },
    {
        "Y": "xAT+b",
        "X": "y = xAT + by= xAT+b. linear Applies a linear transformation to the incoming data:y",
        "Z": "linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b."
    },
    {
        "Y": "bilinear",
        "X": "What type of transformation applies a bilinear transformation to the incoming data?",
        "Z": "bilinear Applies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b"
    },
    {
        "Y": "probabilitypusing samples",
        "X": "What is used to randomly zero some of the elements of the input tensor?",
        "Z": "dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution."
    },
    {
        "Y": "alpha dropout",
        "X": "What does alpha_dropout apply to the input?",
        "Z": "alpha_dropout Applies alpha dropout to the input."
    },
    {
        "Y": "alpha_dropout",
        "X": "What applies alpha dropout to the input?",
        "Z": "alpha_dropout Applies alpha dropout to the input."
    },
    {
        "Y": "feature map",
        "X": "What is a channel a part of?",
        "Z": "feature_alpha_dropout Randomly masks out entire channels (a channel is a feature map, e.g."
    },
    {
        "Y": "a 3D feature map",
        "X": "What is a channel?",
        "Z": "nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j])."
    },
    {
        "Y": "nn.Dropout3d",
        "X": "What is program that randomly zeros out entire channels?",
        "Z": "nn.Dropout3d Randomly zero out entire channels (a channel is a 3D feature map, e.g., thejjj-th channel of theiii-th sample in the batched input is a 3D tensorinput[i,j]\\text{input}[i, j]input[i,j])."
    },
    {
        "Y": "a fixed dictionary and size",
        "X": "embedding A simple lookup table that looks up embeddings in what?",
        "Z": "embedding A simple lookup table that looks up embeddings in a fixed dictionary and size."
    },
    {
        "Y": "embeddings",
        "X": "A simple lookup table that looks up what in a fixed dictionary and size?",
        "Z": "embedding A simple lookup table that looks up embeddings in a fixed dictionary and size."
    },
    {
        "Y": "instantiating the intermediate embeddings",
        "X": "The embedding_bag computes sums, means and maxes of embeddings without doing what?",
        "Z": "embedding_bag Computes sums, means or maxes ofbagsof embeddings, without instantiating the intermediate embeddings."
    },
    {
        "Y": "embedding_bag",
        "X": "What Computes sums, means or maxes ofbagsof embeddings, without instantiating the intermediate embeddings",
        "Z": "embedding_bag Computes sums, means or maxes ofbagsof embeddings, without instantiating the intermediate embeddings."
    },
    {
        "Y": "one_hot",
        "X": "What takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_class",
        "Z": "one_hot Takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."
    },
    {
        "Y": "zeros",
        "X": "What does the tensor of shape(*,num_classes) have everywhere except where the index of last dimension matches the corresponding",
        "Z": "one_hot Takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."
    },
    {
        "Y": "one",
        "X": "How many _hot takes LongTensor with index values of shape(*)and returns a tensor of shape(*",
        "Z": "one_hot Takes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."
    },
    {
        "Y": "pairwise_distance",
        "X": "What does Seetorch.nn.PairwiseDistancefor details?",
        "Z": "pairwise_distance Seetorch.nn.PairwiseDistancefor details"
    },
    {
        "Y": "cosine similarity",
        "X": "What does cosine_similarity return?",
        "Z": "cosine_similarity Returns cosine similarity between x1 and x2, computed along dim."
    },
    {
        "Y": "dim",
        "X": "What is the cosine similarity computed along?",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim."
    },
    {
        "Y": "cosine_similarity",
        "X": "What returns cosine similarity between x1 and x2?",
        "Z": "cosine_similarity Returns cosine similarity between x1 and x2, computed along dim."
    },
    {
        "Y": "computed along dim",
        "X": "How is the cosine similarity between x1 and x2 computed?",
        "Z": "cosine_similarity Returns cosine similarity between x1 and x2, computed along dim."
    },
    {
        "Y": "p-norm distance",
        "X": "What does pdist compute between every pair of row vectors in the input?",
        "Z": "pdist Computes the p-norm distance between every pair of row vectors in the input."
    },
    {
        "Y": "p-norm",
        "X": "pdist Computes the distance between every pair of row vectors in the input?",
        "Z": "pdist Computes the p-norm distance between every pair of row vectors in the input."
    },
    {
        "Y": "binary_cross_entropy Function",
        "X": "What is function that measures the Binary Cross Entropy between the target and the output?",
        "Z": "binary_cross_entropy Function that measures the Binary Cross Entropy between the target and the output."
    },
    {
        "Y": "binary_cross_entropy",
        "X": "What function measures the Binary Cross Entropy between the target and the output?",
        "Z": "binary_cross_entropy Function that measures the Binary Cross Entropy between the target and the output."
    },
    {
        "Y": "binary_cross_entropy",
        "X": "What is the function that measures Binary Cross Entropy between target and output logits?",
        "Z": "binary_cross_entropy_with_logits Function that measures Binary Cross Entropy between target and output logits."
    },
    {
        "Y": "binary_cross_entropy_with_logits",
        "X": "What function measures Binary Cross Entropy between target and output logits?",
        "Z": "binary_cross_entropy_with_logits Function that measures Binary Cross Entropy between target and output logits."
    },
    {
        "Y": "nll_loss",
        "X": "What is the negative log likelihood loss?",
        "Z": "nll_loss The negative log likelihood loss."
    },
    {
        "Y": "Poisson negative log likelihood loss",
        "X": "What is poisson_nll_loss?",
        "Z": "poisson_nll_loss Poisson negative log likelihood loss."
    },
    {
        "Y": "cosine_embedding_loss",
        "X": "What is a cosine embedding loss?",
        "Z": "cosine_embedding_loss SeeCosineEmbeddingLossfor details."
    },
    {
        "Y": "cosine_embedding_loss",
        "X": "What is loss caused by cosineEmbeddingLoss?",
        "Z": "cosine_embedding_loss SeeCosineEmbeddingLossfor details."
    },
    {
        "Y": "cross_entropy",
        "X": "What criterion combineslog_softmaxandnll_lossin a single function?",
        "Z": "cross_entropy This criterion combineslog_softmaxandnll_lossin a single function."
    },
    {
        "Y": "Connectionist Temporal Classification",
        "X": "What type of loss is ctc_loss?",
        "Z": "ctc_loss The Connectionist Temporal Classification loss."
    },
    {
        "Y": "The Connectionist Temporal Classification",
        "X": "What is ctc_loss?",
        "Z": "ctc_loss The Connectionist Temporal Classification loss."
    },
    {
        "Y": "negative log",
        "X": "What type of loss is gaussian_nll_loss?",
        "Z": "gaussian_nll_loss Gaussian negative log likelihood loss."
    },
    {
        "Y": "negative log likelihood loss",
        "X": "What is gaussian_nll_loss?",
        "Z": "gaussian_nll_loss Gaussian negative log likelihood loss."
    },
    {
        "Y": "SeeHingeEmbeddingLossfor",
        "X": "What are the details of hinge_embedding_loss?",
        "Z": "hinge_embedding_loss SeeHingeEmbeddingLossfor details."
    },
    {
        "Y": "hinge_embedding_loss",
        "X": "What does SeeHingeEmbeddingLossfor details?",
        "Z": "hinge_embedding_loss SeeHingeEmbeddingLossfor details."
    },
    {
        "Y": "Loss",
        "X": "What is the divergence of kl_div TheKullback-Leibler?",
        "Z": "kl_div TheKullback-Leibler divergence Loss"
    },
    {
        "Y": "kl_div",
        "X": "What is TheKullback-Leibler divergence Loss?",
        "Z": "kl_div TheKullback-Leibler divergence Loss"
    },
    {
        "Y": "l1_loss Function",
        "X": "What takes the mean element-wise absolute value difference?",
        "Z": "l1_loss Function that takes the mean element-wise absolute value difference."
    },
    {
        "Y": "mse_loss",
        "X": "What measures the element-wise mean squared error?",
        "Z": "mse_loss Measures the element-wise mean squared error."
    },
    {
        "Y": "triplet_margin_with_distance_loss",
        "X": "What do you see for details?",
        "Z": "triplet_margin_with_distance_loss SeeTripletMarginWithDistanceLossfor details."
    },
    {
        "Y": "margin_ranking_loss",
        "X": "What does marginRankingLoss stand for?",
        "Z": "margin_ranking_loss SeeMarginRankingLossfor details."
    },
    {
        "Y": "multilabel_margin_loss",
        "X": "What is multilabel_margin_loss?",
        "Z": "multilabel_margin_loss SeeMultiLabelMarginLossfor details."
    },
    {
        "Y": "multilabel_soft_margin_loss",
        "X": "What is file that is used for labels?",
        "Z": "multilabel_soft_margin_loss SeeMultiLabelSoftMarginLossfor details."
    },
    {
        "Y": "multilabel_soft_margin_loss",
        "X": "What does MultiLabelSoftMarginLoss stand for?",
        "Z": "multilabel_soft_margin_loss SeeMultiLabelSoftMarginLossfor details."
    },
    {
        "Y": "weight=None",
        "X": "What is the weight of the multi_margin_loss?",
        "Z": "multi_margin_loss multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,"
    },
    {
        "Y": "multi_margin_loss",
        "X": "What is the term for multi_margin_loss(input, target, p=1, margin=1, weight=None,",
        "Z": "multi_margin_loss multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,"
    },
    {
        "Y": "delta",
        "X": "What is the absolute element-wise error below?",
        "Z": "huber_loss Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "huber_loss Function",
        "X": "What uses a squared term if the absolute element-wise error falls below delta?",
        "Z": "huber_loss Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "if the absolute element-wise error falls below delta",
        "X": "What does the huber_loss function use a squared term for?",
        "Z": "huber_loss Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "huber_loss Function",
        "X": "What function uses a squared term if the absolute element-wise error falls below delta?",
        "Z": "huber_loss Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "smooth_l1_loss Function",
        "X": "What uses a squared term if the absolute element-wise error falls below beta?",
        "Z": "smooth_l1_loss Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "squared term",
        "X": "What term does the smooth_l1_loss function use if the absolute element-wise error falls below beta?",
        "Z": "smooth_l1_loss Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "soft_margin_loss",
        "X": "What is loss that occurs when you lose a lot of money?",
        "Z": "soft_margin_loss SeeSoftMarginLossfor details."
    },
    {
        "Y": "triplet_margin_loss",
        "X": "What is the name of a triplet?",
        "Z": "triplet_margin_loss SeeTripletMarginLossfor details"
    },
    {
        "Y": "triplet_margin_loss",
        "X": "What is triplet_margin_loss?",
        "Z": "triplet_margin_loss SeeTripletMarginLossfor details"
    },
    {
        "Y": "triplet_margin_with_distance_loss",
        "X": "What does triplet_margin_with_distance_loss refer to?",
        "Z": "triplet_margin_with_distance_loss SeeTripletMarginWithDistanceLossfor details."
    },
    {
        "Y": "pixel_shuffle",
        "X": "What rearranges elements in a tensor of shape?",
        "Z": "pixel_shuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor."
    },
    {
        "Y": "r",
        "X": "What ranges elements in a tensor of shape(,Cr2,H,W)(*, C time",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor."
    },
    {
        "Y": "pixel_shuffle",
        "X": "What Rearranges elements in a tensor of shape(,Cr2,H,W)(*, C ",
        "Z": "pixel_shuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor."
    },
    {
        "Y": "pixel_unshuffle",
        "X": "What reverses thePixelShuffleoperation by rearranging elements in a tensor of shape?",
        "Z": "pixel_unshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor."
    },
    {
        "Y": "pixel_unshuffle",
        "X": "What Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(,C",
        "Z": "pixel_unshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor."
    },
    {
        "Y": "Pads tensor",
        "X": "What is pad?",
        "Z": "pad Pads tensor."
    },
    {
        "Y": "pad Pads tensor",
        "X": "What is pad pad?",
        "Z": "pad Pads tensor."
    },
    {
        "Y": "pad Pads tensor",
        "X": "What is the term for pad Pads?",
        "Z": "pad Pads tensor."
    },
    {
        "Y": "the givensizeor the givenscale_factor",
        "X": "What does the interpolate Down/up samples the input to?",
        "Z": "interpolate Down/up samples the input to either the givensizeor the givenscale_factor"
    },
    {
        "Y": "the givenscale_factor",
        "X": "What does the interpolate Down/up sample the input to?",
        "Z": "interpolate Down/up samples the input to either the givensizeor the givenscale_factor"
    },
    {
        "Y": "the input to either the givensizeor the givenscale_factor",
        "X": "What is an upsample?",
        "Z": "upsample Upsamples the input to either the givensizeor the givenscale_factor"
    },
    {
        "Y": "upsample",
        "X": "What upsamples the input to either the givensize or the givenscale_factor?",
        "Z": "upsample Upsamples the input to either the givensizeor the givenscale_factor"
    },
    {
        "Y": "nearest neighbours\u2019 pixel values",
        "X": "What does upsample_nearest use?",
        "Z": "upsample_nearest Upsamples the input, using nearest neighbours\u2019 pixel values."
    },
    {
        "Y": "upsample_nearest",
        "X": "What upsamples the input, using nearest neighbours' pixel values?",
        "Z": "upsample_nearest Upsamples the input, using nearest neighbours\u2019 pixel values."
    },
    {
        "Y": "nearest neighbours",
        "X": "Upsample_nearest Upsamples the input using what pixel values?",
        "Z": "upsample_nearest Upsamples the input, using nearest neighbours\u2019 pixel values."
    },
    {
        "Y": "bilinear",
        "X": "What type of upsampling does upsample?",
        "Z": "upsample_bilinear Upsamples the input, using bilinear upsampling."
    },
    {
        "Y": "upsample_bilinear",
        "X": "What upsamples the input using bilinear upsampling?",
        "Z": "upsample_bilinear Upsamples the input, using bilinear upsampling."
    },
    {
        "Y": "grid_sample",
        "X": "What computes theoutputusinginputvalues and pixel locations from a flow-fieldgrid?",
        "Z": "grid_sample Given aninput and a flow-fieldgrid, computes theoutputusinginputvalues and pixel locations fromgrid."
    },
    {
        "Y": "affine_grid",
        "X": "What generates a 2D or 3D flow field?",
        "Z": "affine_grid Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta."
    },
    {
        "Y": "affine matricestheta",
        "X": "What is the batch of affine_grid given?",
        "Z": "affine_grid Generates a 2D or 3D flow field (sampling grid), given a batch of affine matricestheta."
    },
    {
        "Y": "device_ids",
        "X": "What are the GPUs given in?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids."
    },
    {
        "Y": "module",
        "X": "What does torch.nn.parallel.data_parallel evaluate in parallel across the GPUs given in device_ids?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids."
    },
    {
        "Y": "device_ids",
        "X": "What does torch.nn.parallel.data_parallel use to evaluate module(input) in parallel across GPUs?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids."
    },
    {
        "Y": "32",
        "X": "How many bits does float32ortorch.float torch have?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "floating point torch",
        "X": "What is a 32-bit torch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "*.FloatTensor",
        "X": "What is 32-bit floating point torch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "floating point torch",
        "X": "What is the 32-bit version of the float32ortorch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "floating point",
        "X": "What type of torch is float32ortorch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "*",
        "X": "What is floating point torch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.*.FloatTensor"
    },
    {
        "Y": "64-bit",
        "X": "What is the version of the floating point torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor"
    },
    {
        "Y": "*.DoubleTensor",
        "X": "What is 64-bit floating point torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor"
    },
    {
        "Y": "double torch",
        "X": "What is a floating point torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor"
    },
    {
        "Y": "64-bit",
        "X": "What is the floating point torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor"
    },
    {
        "Y": "DoubleTensor",
        "X": "What is float64ortorch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.*.DoubleTensor"
    },
    {
        "Y": "64-bit",
        "X": "What is the version of the complex torch?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat "
    },
    {
        "Y": "64-bit",
        "X": "What is the version of complex torch.complex64ortorch.cfloat?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat "
    },
    {
        "Y": "cfloat",
        "X": "What is a 64-bit complex torch?",
        "Z": "64-bit complex torch.complex64ortorch.cfloat "
    },
    {
        "Y": "128",
        "X": "How many bits is the complex torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble  "
    },
    {
        "Y": "complex",
        "X": "What is torch.complex128ortorch.cdouble?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble  "
    },
    {
        "Y": "cdouble",
        "X": "What is 128-bit complex torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble  "
    },
    {
        "Y": "128",
        "X": "How many bits is a complex torch?",
        "Z": "128-bit complex torch.complex128ortorch.cdouble  "
    },
    {
        "Y": "16",
        "X": "How many bits does a floating point1 torch have?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor"
    },
    {
        "Y": "*.HalfTensor",
        "X": "What is 16-bit floating point1 torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor"
    },
    {
        "Y": "half torch",
        "X": "What is a 16-bit floating point1 torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor"
    },
    {
        "Y": "floating point",
        "X": "What is the 16-bit version of a torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor"
    },
    {
        "Y": "HalfTensor",
        "X": "What is a 16-bit floating point torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.*.HalfTensor"
    },
    {
        "Y": "16",
        "X": "How many bits does a bfloat16 torch have?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor"
    },
    {
        "Y": "*.BFloat16Tensor",
        "X": "What is 16-bit floating point2 torch?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor"
    },
    {
        "Y": "floating point2",
        "X": "What is the 16-bit torch?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.*.BFloat16Tensor"
    },
    {
        "Y": "8",
        "X": "How many bits is a torch?",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor"
    },
    {
        "Y": "unsigned",
        "X": "What is an 8-bit integer?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor"
    },
    {
        "Y": "*.ByteTensor",
        "X": "What is 8-bit integer (unsigned) torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor"
    },
    {
        "Y": "unsigned",
        "X": "What is 8-bit integer?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor"
    },
    {
        "Y": "8-bit integer",
        "X": "What is the unsigned torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor"
    },
    {
        "Y": "*.CharTensor",
        "X": "What is 8-bit integer (signed) torch?",
        "Z": "8-bit integer (signed) torch.int8 torch.*.CharTensor"
    },
    {
        "Y": "8-bit integer",
        "X": "What is a torch?",
        "Z": "8-bit integer (signed) torch.int8 torch.*.CharTensor"
    },
    {
        "Y": "8",
        "X": "How many bits is the torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor"
    },
    {
        "Y": "ShortTensor",
        "X": "What is short torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor"
    },
    {
        "Y": "16",
        "X": "How many bit integers does torch.int16ortorch.short torch have?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.*.ShortTensor"
    },
    {
        "Y": "*.IntTensor",
        "X": "What is 32-bit integer (signed) torch?",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.*.IntTensor"
    },
    {
        "Y": "64",
        "X": "How many bits are in a long torch?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor"
    },
    {
        "Y": "*.LongTensor",
        "X": "What is 64-bit integer (signed) torch?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor"
    },
    {
        "Y": "LongTensor",
        "X": "What is a long torch?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor"
    },
    {
        "Y": "64-bit",
        "X": "Int64ortorch.long torch is what?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.*.LongTensor"
    },
    {
        "Y": "Boolean torch",
        "X": "What is a bool torch?",
        "Z": "Boolean torch.bool torch.*.BoolTensor"
    },
    {
        "Y": "*.BoolTensor",
        "X": "What is a Boolean torch?",
        "Z": "Boolean torch.bool torch.*.BoolTensor"
    },
    {
        "Y": "Optimizer",
        "X": "What is group that adds a param group to theOptimizersparam_groups?",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups."
    },
    {
        "Y": "Optimizer.add_param_group",
        "X": "What add a param group to theOptimizersparam_groups?",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups."
    },
    {
        "Y": "load_state_dict",
        "X": "What is dict that loads the optimizer state?",
        "Z": "Optimizer.load_state_dict Loads the optimizer state."
    },
    {
        "Y": "Optimizer.load_state_dict",
        "X": "What is function that loads the optimizer state?",
        "Z": "Optimizer.load_state_dict Loads the optimizer state."
    },
    {
        "Y": "adict",
        "X": "What is the state of the optimizer?",
        "Z": "Optimizer.state_dict Returns the state of the optimizer as adict."
    },
    {
        "Y": "Optimizer.state_dict",
        "X": "What returns the state of the optimizer as adict?",
        "Z": "Optimizer.state_dict Returns the state of the optimizer as adict."
    },
    {
        "Y": "Optimizer.step",
        "X": "What performs a single optimization step?",
        "Z": "Optimizer.step Performs a single optimization step (parameter update)."
    },
    {
        "Y": "zero",
        "X": "What grad sets the gradients of all optimizedtorch.Tensors to zero?",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero."
    },
    {
        "Y": "Optimizer.zero_grad",
        "X": "What sets the gradients of all optimizedtorch.Tensors to zero?",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero."
    },
    {
        "Y": "Adadelta algorithm",
        "X": "What does Adadelta implement?",
        "Z": "Adadelta Implements Adadelta algorithm."
    },
    {
        "Y": "Adadelta",
        "X": "What implements the Adadelta algorithm?",
        "Z": "Adadelta Implements Adadelta algorithm."
    },
    {
        "Y": "Adadelta algorithm",
        "X": "Adadelta Implements what?",
        "Z": "Adadelta Implements Adadelta algorithm."
    },
    {
        "Y": "Adagrad algorithm",
        "X": "What does Adagrad implement?",
        "Z": "Adagrad Implements Adagrad algorithm."
    },
    {
        "Y": "Adam",
        "X": "What implements the Adam algorithm?",
        "Z": "Adam Implements Adam algorithm."
    },
    {
        "Y": "Adam algorithm",
        "X": "What does Adam implement?",
        "Z": "Adam Implements Adam algorithm."
    },
    {
        "Y": "AdamW",
        "X": "What implements the AdamW algorithm?",
        "Z": "AdamW Implements AdamW algorithm."
    },
    {
        "Y": "AdamW algorithm",
        "X": "What does AdamW implement?",
        "Z": "AdamW Implements AdamW algorithm."
    },
    {
        "Y": "lazy",
        "X": "What type of Adam algorithm does SparseAdam implement?",
        "Z": "SparseAdam Implements lazy version of Adam algorithm suitable for sparse tensors."
    },
    {
        "Y": "SparseAdam",
        "X": "What implements a lazy version of Adam algorithm?",
        "Z": "SparseAdam Implements lazy version of Adam algorithm suitable for sparse tensors."
    },
    {
        "Y": "SparseAdam",
        "X": "What implements lazy version of Adam algorithm?",
        "Z": "SparseAdam Implements lazy version of Adam algorithm suitable for sparse tensors."
    },
    {
        "Y": "sparse tensors",
        "X": "SparseAdam implements lazy version of Adam algorithm suitable for what?",
        "Z": "SparseAdam Implements lazy version of Adam algorithm suitable for sparse tensors."
    },
    {
        "Y": "Adamax",
        "X": "What implements the Adamax algorithm?",
        "Z": "Adamax Implements Adamax algorithm (a variant of Adam based on infinity norm)."
    },
    {
        "Y": "infinity norm",
        "X": "What is the Adamax algorithm based on?",
        "Z": "Adamax Implements Adamax algorithm (a variant of Adam based on infinity norm)."
    },
    {
        "Y": "infinity norm",
        "X": "Adamax Implements is a variant of Adam based on what?",
        "Z": "Adamax Implements Adamax algorithm (a variant of Adam based on infinity norm)."
    },
    {
        "Y": "Averaged Stochastic Gradient Descent",
        "X": "What does ASGD implement?",
        "Z": "ASGD Implements Averaged Stochastic Gradient Descent."
    },
    {
        "Y": "L-BFGS algorithm",
        "X": "What does LBFGS implement?",
        "Z": "LBFGS Implements L-BFGS algorithm, heavily inspired byminFunc."
    },
    {
        "Y": "minFunc",
        "X": "LBFGS is heavily inspired by what algorithm?",
        "Z": "LBFGS Implements L-BFGS algorithm, heavily inspired byminFunc."
    },
    {
        "Y": "L-BFGS",
        "X": "What algorithm does LBFGS implement?",
        "Z": "LBFGS Implements L-BFGS algorithm, heavily inspired byminFunc."
    },
    {
        "Y": "RMSprop algorithm",
        "X": "What does RMSprop implement?",
        "Z": "RMSprop Implements RMSprop algorithm."
    },
    {
        "Y": "RMSprop",
        "X": "What implements RMSprop algorithm?",
        "Z": "RMSprop Implements RMSprop algorithm."
    },
    {
        "Y": "Rprop",
        "X": "What implements the resilient backpropagation algorithm?",
        "Z": "Rprop Implements the resilient backpropagation algorithm."
    },
    {
        "Y": "resilient backpropagation algorithm",
        "X": "What does Rprop implement?",
        "Z": "Rprop Implements the resilient backpropagation algorithm."
    },
    {
        "Y": "stochastic gradient descent",
        "X": "What does SGD implement?",
        "Z": "SGD Implements stochastic gradient descent (optionally with momentum)."
    },
    {
        "Y": "momentum",
        "X": "SGD Implements stochastic gradient descent (optionally with what?",
        "Z": "SGD Implements stochastic gradient descent (optionally with momentum)."
    },
    {
        "Y": "momentum",
        "X": "SGD implements stochastic gradient descent (optionally with what?",
        "Z": "SGD Implements stochastic gradient descent (optionally with momentum)."
    },
    {
        "Y": "lr_scheduler.LambdaLR",
        "X": "What sets the learning rate of each parameter group to the initial lr times a given function?",
        "Z": "lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function."
    },
    {
        "Y": "the learning rate",
        "X": "What does lr_scheduler.LambdaLR set?",
        "Z": "lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function."
    },
    {
        "Y": "lr_scheduler",
        "X": "What does MultiplicativeLR use to multiply the learning rate of each parameter group by the factor given in the specified function?",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function."
    },
    {
        "Y": "Multiply the learning rate of each parameter group by the factor given in the specified function",
        "X": "What is lr_scheduler.MultiplicativeLR?",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function."
    },
    {
        "Y": "gamma",
        "X": "How does lr_scheduler.StepLR Decay the learning rate of each parameter group?",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs."
    },
    {
        "Y": "lr_scheduler",
        "X": "What is program that Decays the learning rate of each parameter group by gamma every step_size ep",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs."
    },
    {
        "Y": "gamma",
        "X": "StepLR Decays the learning rate of each parameter group by what?",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs."
    },
    {
        "Y": "gamma",
        "X": "What does lr_scheduler.MultiStepLR Decay the learning rate of each parameter group by?",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."
    },
    {
        "Y": "lr_scheduler",
        "X": "MultiStepLR Decays the learning rate by gamma once the number of epoch reaches one of the milestones",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones."
    },
    {
        "Y": "gamma",
        "X": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by what?",
        "Z": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch."
    },
    {
        "Y": "gamma",
        "X": "What does lr_scheduler.ExponentialLR Decay the learning rate of each parameter group by every epoch",
        "Z": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch."
    },
    {
        "Y": "lr_scheduler",
        "X": "What set the learning rate of each parameter group using a cosine annealing schedule?",
        "Z": "lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR:"
    },
    {
        "Y": "the learning rate",
        "X": "What does lr_scheduler.CosineAnnealingLR Set?",
        "Z": "lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR:"
    },
    {
        "Y": "lr_scheduler",
        "X": "What is program that reduces learning rate when a metric has stopped improving?",
        "Z": "lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving."
    },
    {
        "Y": "learning rate",
        "X": "What does lr_scheduler.ReduceLROnPlateau Reduce when a metric has stopped improving?",
        "Z": "lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving."
    },
    {
        "Y": "cyclical learning rate policy",
        "X": "What does CLR stand for?",
        "Z": "lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR)."
    },
    {
        "Y": "lr_scheduler.CyclicLR",
        "X": "What sets the learning rate of each parameter group according to cyclical learning rate policy?",
        "Z": "lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR)."
    },
    {
        "Y": "1cycle learning rate policy",
        "X": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what policy?",
        "Z": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy."
    },
    {
        "Y": "1cycle",
        "X": "What is the learning rate policy for each parameter group?",
        "Z": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy."
    },
    {
        "Y": "SGDR",
        "X": "Where is the number of epochs between two warm restarts?",
        "Z": "lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:"
    },
    {
        "Y": "lr_scheduler",
        "X": "What is used to set the learning rate of each parameter group using a cosine annealing schedule?",
        "Z": "lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:"
    },
    {
        "Y": "the learning rate",
        "X": "What does lr_scheduler.CosineAnnealingWarmRestarts Set?",
        "Z": "lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:"
    },
    {
        "Y": "module",
        "X": "Parameter A kind of Tensor that is to be considered what type of parameter?",
        "Z": "Parameter A kind of Tensor that is to be considered a module parameter."
    },
    {
        "Y": "Parameter",
        "X": "What is a kind of Tensor that is to be considered a module parameter?",
        "Z": "Parameter A kind of Tensor that is to be considered a module parameter."
    },
    {
        "Y": "Tensor",
        "X": "Parameter A kind of what is to be considered a module parameter?",
        "Z": "Parameter A kind of Tensor that is to be considered a module parameter."
    },
    {
        "Y": "UninitializedParameter",
        "X": "What is a parameter that is not initialized?",
        "Z": "UninitializedParameter A parameter that is not initialized."
    },
    {
        "Y": "UninitializedBuffer",
        "X": "What is a buffer that is not initialized?",
        "Z": "UninitializedBuffer A buffer that is not initialized."
    },
    {
        "Y": "Module Base",
        "X": "What class is used for all neural network modules?",
        "Z": "Module Base class for all neural network modules."
    },
    {
        "Y": "sequential container",
        "X": "What is a Sequential A?",
        "Z": "Sequential A sequential container."
    },
    {
        "Y": "ModuleList",
        "X": "What holds submodules in a list?",
        "Z": "ModuleList Holds submodules in a list."
    },
    {
        "Y": "submodules",
        "X": "ModuleList holds what in a list?",
        "Z": "ModuleList Holds submodules in a list."
    },
    {
        "Y": "ModuleDict",
        "X": "What holds submodules in a dictionary?",
        "Z": "ModuleDict Holds submodules in a dictionary."
    },
    {
        "Y": "ParameterList",
        "X": "What holds parameters in a list?",
        "Z": "ParameterList Holds parameters in a list."
    },
    {
        "Y": "ParameterDict",
        "X": "What holds parameters in a dictionary?",
        "Z": "ParameterDict Holds parameters in a dictionary."
    },
    {
        "Y": "register_module_forward_pre_hook",
        "X": "What registers a forward pre-hook common to all modules?",
        "Z": "register_module_forward_pre_hook Registers a forward pre-hook common to all modules."
    },
    {
        "Y": "all modules",
        "X": "register_module_forward_pre_hook Registers a forward pre-hook common to what?",
        "Z": "register_module_forward_pre_hook Registers a forward pre-hook common to all modules."
    },
    {
        "Y": "register_module_forward_hook",
        "X": "What registers a global forward hook for all the modules?",
        "Z": "register_module_forward_hook Registers a global forward hook for all the modules"
    },
    {
        "Y": "all the modules",
        "X": "What does register_module_forward_hook register a global forward hook for?",
        "Z": "register_module_forward_hook Registers a global forward hook for all the modules"
    },
    {
        "Y": "register_module_backward_hook",
        "X": "What registers a backward hook common to all the modules?",
        "Z": "register_module_backward_hook Registers a backward hook common to all the modules."
    },
    {
        "Y": "backward hook",
        "X": "What does register_module_backward_hook register?",
        "Z": "register_module_backward_hook Registers a backward hook common to all the modules."
    },
    {
        "Y": "all the modules",
        "X": "register_module_backward_hook Registers a backward hook common to what?",
        "Z": "register_module_backward_hook Registers a backward hook common to all the modules."
    },
    {
        "Y": "1D",
        "X": "nn.Conv1d Applies what type of convolution over an input signal?",
        "Z": "nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "Conv2d Applies a 2D convolution over an input signal composed of several input planes?",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn.Conv2d",
        "X": "What Applies a 2D convolution over an input signal composed of several input planes?",
        "Z": "nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "Conv3d Applies a 3D convolution over an input signal composed of several input planes?",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn.Conv3d",
        "X": "What Applies a 3D convolution over an input signal composed of several input planes?",
        "Z": "nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is 1D transposed convolution operator?",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "nn.ConvTranspose1d",
        "X": "What Applies a 1D transposed convolution operator over an input image composed of several input planes?",
        "Z": "nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is 2D transposed convolution operator?",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "nn.ConvTranspose2d",
        "X": "What Applies a 2D transposed convolution operator over an input image composed of several input planes?",
        "Z": "nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is 3D transposed convolution operator?",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "nn.ConvTranspose3d",
        "X": "What Applies a 3D transposed convolution operator over an input image composed of several input planes?",
        "Z": "nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes."
    },
    {
        "Y": "thein_channelsargument",
        "X": "What is the lazy initialization of of theConv1d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size",
        "X": "What is the lazy initialization of thein_channelsargument of theConv1d that is inferred from?",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1)."
    },
    {
        "Y": "lazy initialization",
        "X": "What is thein_channelsargument of theConv1d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from the input.size(1)."
    },
    {
        "Y": "thein_channelsargument",
        "X": "What is the lazy initialization of of theConv2d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size",
        "X": "What is the lazy initialization of thein_channelsargument of theConv2d that is inferred from?",
        "Z": "nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from the input.size(1)."
    },
    {
        "Y": "thenum_featuresargument of theBatchNorm3d",
        "X": "What is inferred from the input.size(1)?",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "thein_channelsargument",
        "X": "What is the lazy initialization of of theConv3d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "lazy initialization",
        "X": "What is thein_channelsargument of theConv3d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size",
        "X": "The lazy initialization of thein_channelsargument of theConvTranspose1d is inferred from what?",
        "Z": "nn.LazyConvTranspose1d a torch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument of theConvTranspose1dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size",
        "X": "The lazy initialization of thein_channelsargument of theConvTranspose2d is inferred from what?",
        "Z": "nn.LazyConvTranspose2d a torch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument of theConvTranspose2dthat is inferred from the input.size(1)."
    },
    {
        "Y": "thein_channelsargument",
        "X": "What is the lazy initialization of of theConvTranspose3d that is inferred from the input.size(1)?",
        "Z": "nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size",
        "X": "The lazy initialization of thein_channelsargument of theConvTranspose3d is inferred from what?",
        "Z": "nn.LazyConvTranspose3d a torch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument of theConvTranspose3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "local blocks",
        "X": "Unfold Extracts sliding what from a batched input tensor?",
        "Z": "nn.Unfold Extracts sliding local blocks from a batched input tensor."
    },
    {
        "Y": "tensor",
        "X": "nn.Fold Combines an array of sliding local blocks into a large containing what?",
        "Z": "nn.Fold Combines an array of sliding local blocks into a large containing tensor."
    },
    {
        "Y": "nn.Fold",
        "X": "What combine an array of sliding local blocks into a large containing tensor?",
        "Z": "nn.Fold Combines an array of sliding local blocks into a large containing tensor."
    },
    {
        "Y": "1D",
        "X": "nn.MaxPool1d Applies a max pooling over an input signal composed of several input planes?",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.MaxPool1d",
        "X": "What applies a 1D max pooling over an input signal composed of several input planes?",
        "Z": "nn.MaxPool1d Applies a 1D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "nn.MaxPool2d Applies a what kind of max pooling over an input signal?",
        "Z": "nn.MaxPool2d Applies a 2D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes?",
        "Z": "nn.MaxPool3d Applies a 3D max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is the value of the inverse of MaxUnpool1d?",
        "Z": "nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d."
    },
    {
        "Y": "inverse",
        "X": "nn.MaxUnpool1d Computes a partial what of MaxPool1d?",
        "Z": "nn.MaxUnpool1d Computes a partial inverse ofMaxPool1d."
    },
    {
        "Y": "nn",
        "X": "What is the value of the inverse of MaxUnpool2d?",
        "Z": "nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d."
    },
    {
        "Y": "inverse",
        "X": "nn.MaxUnpool2d Computes a partial what of MaxPool2d?",
        "Z": "nn.MaxUnpool2d Computes a partial inverse ofMaxPool2d."
    },
    {
        "Y": "nn",
        "X": "What is the value of the inverse of MaxUnpool3d?",
        "Z": "nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d."
    },
    {
        "Y": "inverse",
        "X": "nn.MaxUnpool3d Computes a partial what of MaxPool3d?",
        "Z": "nn.MaxUnpool3d Computes a partial inverse ofMaxPool3d."
    },
    {
        "Y": "1D",
        "X": "nn.AvgPool1d Applies what type of pooling over an input signal?",
        "Z": "nn.AvgPool1d Applies a 1D average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.AvgPool2d",
        "X": "What Applies a 2D average pooling over an input signal composed of several input planes?",
        "Z": "nn.AvgPool2d Applies a 2D average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.AvgPool3d",
        "X": "What Applies a 3D average pooling over an input signal composed of several input planes?",
        "Z": "nn.AvgPool3d Applies a 3D average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "What kind of fractional max pooling does nn.FractionalMaxPool2d apply?",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.FractionalMaxPool2d",
        "X": "What applies a 2D fractional max pooling over an input signal composed of several input planes?",
        "Z": "nn.FractionalMaxPool2d Applies a 2D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What kind of pooling does nn.FractionalMaxPool3d apply?",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn.FractionalMaxPool3d",
        "X": "What applies a 3D fractional max pooling over an input signal composed of several input planes?",
        "Z": "nn.FractionalMaxPool3d Applies a 3D fractional max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "1D",
        "X": "nn.LPPool1d Applies a power-average pooling over an input signal composed of several input planes?",
        "Z": "nn.LPPool1d Applies a 1D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "2D",
        "X": "nn.LPPool2d Applies a power-average pooling over an input signal composed of several input planes in",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "several input planes",
        "X": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of what?",
        "Z": "nn.LPPool2d Applies a 2D power-average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is the value of AdaptiveMaxPool1d?",
        "Z": "nn.AdaptiveMaxPool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is the value of AdaptiveMaxPool2d?",
        "Z": "nn.AdaptiveMaxPool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is the value of AdaptiveMaxPool3d?",
        "Z": "nn.AdaptiveMaxPool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes."
    },
    {
        "Y": "1D",
        "X": "AdaptiveAvgPool1d Applies what kind of pooling over an input signal?",
        "Z": "nn.AdaptiveAvgPool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "nn",
        "X": "What is the value of AdaptiveAvgPool2d?",
        "Z": "nn.AdaptiveAvgPool2d Applies a 2D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "3D",
        "X": "What type of pooling does AdaptiveAvgPool3d apply?",
        "Z": "nn.AdaptiveAvgPool3d Applies a 3D adaptive average pooling over an input signal composed of several input planes."
    },
    {
        "Y": "reflection of the input boundary",
        "X": "What does nn.ReflectionPad1d use to pad the input tensor?",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary."
    },
    {
        "Y": "reflection of the input boundary",
        "X": "What does nn.ReflectionPad1d Pad the input tensor using?",
        "Z": "nn.ReflectionPad1d Pads the input tensor using the reflection of the input boundary."
    },
    {
        "Y": "reflection of the input boundary",
        "X": "What does nn.ReflectionPad2d use to pad the input tensor?",
        "Z": "nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary."
    },
    {
        "Y": "reflection of the input boundary",
        "X": "What does nn.ReflectionPad2d Pads the input tensor using?",
        "Z": "nn.ReflectionPad2d Pads the input tensor using the reflection of the input boundary."
    },
    {
        "Y": "replication",
        "X": "What does nn.ReplicationPad1d use to pad the input tensor?",
        "Z": "nn.ReplicationPad1d Pads the input tensor using replication of the input boundary."
    },
    {
        "Y": "replication of the input boundary",
        "X": "What does nn.ReplicationPad2d use to pad the input tensor?",
        "Z": "nn.ReplicationPad2d Pads the input tensor using replication of the input boundary."
    },
    {
        "Y": "replication",
        "X": "What does nn.ReplicationPad2d Pads the input tensor using?",
        "Z": "nn.ReplicationPad2d Pads the input tensor using replication of the input boundary."
    },
    {
        "Y": "replication",
        "X": "What does nn.ReplicationPad3d use to pad the input tensor?",
        "Z": "nn.ReplicationPad3d Pads the input tensor using replication of the input boundary."
    },
    {
        "Y": "zero",
        "X": "nn.ZeroPad2d Pads the input tensor boundaries with what?",
        "Z": "nn.ZeroPad2d Pads the input tensor boundaries with zero."
    },
    {
        "Y": "constant value",
        "X": "What does nn.ConstantPad1d pad the input tensor boundaries with?",
        "Z": "nn.ConstantPad1d Pads the input tensor boundaries with a constant value."
    },
    {
        "Y": "nn.ConstantPad1d",
        "X": "What Pads the input tensor boundaries with a constant value?",
        "Z": "nn.ConstantPad1d Pads the input tensor boundaries with a constant value."
    },
    {
        "Y": "constant value",
        "X": "What does nn.ConstantPad2d pad the input tensor boundaries with?",
        "Z": "nn.ConstantPad2d Pads the input tensor boundaries with a constant value."
    },
    {
        "Y": "nn.ConstantPad3d Pads",
        "X": "What are the input tensor boundaries with a constant value?",
        "Z": "nn.ConstantPad3d Pads the input tensor boundaries with a constant value."
    },
    {
        "Y": "constant value",
        "X": "What does nn.ConstantPad3d pad the input tensor boundaries with?",
        "Z": "nn.ConstantPad3d Pads the input tensor boundaries with a constant value."
    },
    {
        "Y": "element-wise",
        "X": "What type of function does nn.ELU apply?",
        "Z": "nn.ELU Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "nn.ELU Applies what?",
        "Z": "nn.ELU Applies the element-wise function:"
    },
    {
        "Y": "hard shrinkage",
        "X": "What function does Hardshrink apply?",
        "Z": "nn.Hardshrink Applies the hard shrinkage function element-wise:"
    },
    {
        "Y": "element-wise",
        "X": "nn.Hardshrink Applies the hard shrinkage function what?",
        "Z": "nn.Hardshrink Applies the hard shrinkage function element-wise:"
    },
    {
        "Y": "element-wise function",
        "X": "What does nn.Hardsigmoid apply?",
        "Z": "nn.Hardsigmoid Applies the element-wise function:"
    },
    {
        "Y": "HardTanh",
        "X": "Which function does nn.Hardtanh apply element-wise?",
        "Z": "nn.Hardtanh Applies the HardTanh function element-wise"
    },
    {
        "Y": "element-wise",
        "X": "How does HardTanh apply the HardTanh function?",
        "Z": "nn.Hardtanh Applies the HardTanh function element-wise"
    },
    {
        "Y": "HardTanh",
        "X": "What function does nn.Hardtanh apply element-wise?",
        "Z": "nn.Hardtanh Applies the HardTanh function element-wise"
    },
    {
        "Y": "hardswish function",
        "X": "What does nn.Hardswish Apply?",
        "Z": "nn.Hardswish Applies the hardswish function, element-wise, as described in the paper:"
    },
    {
        "Y": "hardswish",
        "X": "nn.Hardswish Applies what function, element-wise?",
        "Z": "nn.Hardswish Applies the hardswish function, element-wise, as described in the paper:"
    },
    {
        "Y": "element-wise",
        "X": "What kind of function does nn.LeakyReLU apply?",
        "Z": "nn.LeakyReLU Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "What does nn.LeakyReLU apply?",
        "Z": "nn.LeakyReLU Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "What does nn.LogSigmoid apply?",
        "Z": "nn.LogSigmoid Applies the element-wise function:"
    },
    {
        "Y": "MultiheadAttention",
        "X": "What allows the model to jointly attend to information from different representation subspaces?",
        "Z": "nn.MultiheadAttention Allows the model to jointly attend to information from different representation subspaces."
    },
    {
        "Y": "element-wise function",
        "X": "nn.PReLU Applies what?",
        "Z": "nn.PReLU Applies the element-wise function:"
    },
    {
        "Y": "rectified linear unit function",
        "X": "What does nn.ReLU apply element-wise?",
        "Z": "nn.ReLU Applies the rectified linear unit function element-wise:"
    },
    {
        "Y": "element-wise",
        "X": "nn.ReLU Applies the rectified linear unit function what?",
        "Z": "nn.ReLU Applies the rectified linear unit function element-wise:"
    },
    {
        "Y": "nn.ReLU",
        "X": "What applies the rectified linear unit function element-wise?",
        "Z": "nn.ReLU Applies the rectified linear unit function element-wise:"
    },
    {
        "Y": "element-wise",
        "X": "What kind of function does nn.ReLU6 apply?",
        "Z": "nn.ReLU6 Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "nn.ReLU6 Applies what?",
        "Z": "nn.ReLU6 Applies the element-wise function:"
    },
    {
        "Y": "element-wise",
        "X": "How is the randomized leaky rectified liner unit function described in the paper?",
        "Z": "nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:"
    },
    {
        "Y": "nn.RReLU",
        "X": "What applies the randomized leaky rectified liner unit function, element-wise?",
        "Z": "nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:"
    },
    {
        "Y": "element-wise",
        "X": "How does nn.RReLU apply the randomized leaky rectified liner unit function?",
        "Z": "nn.RReLU Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:"
    },
    {
        "Y": "Applied element-wise",
        "X": "What is nn.SELU?",
        "Z": "nn.SELU Applied element-wise, as:"
    },
    {
        "Y": "element-wise",
        "X": "How is nn.SELU applied?",
        "Z": "nn.SELU Applied element-wise, as:"
    },
    {
        "Y": "element-wise",
        "X": "What kind of function does nn.CELU apply?",
        "Z": "nn.CELU Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "What does nn.CELU apply?",
        "Z": "nn.CELU Applies the element-wise function:"
    },
    {
        "Y": "Gaussian Error Linear Units",
        "X": "What function does nn.GELU apply?",
        "Z": "nn.GELU Applies the Gaussian Error Linear Units function:"
    },
    {
        "Y": "element-wise function",
        "X": "What does nn.Sigmoid apply?",
        "Z": "nn.Sigmoid Applies the element-wise function:"
    },
    {
        "Y": "element-wise",
        "X": "How does nn.SiLU apply the Sigmoid Linear Unit function?",
        "Z": "nn.SiLU Applies the Sigmoid Linear Unit (SiLU) function, element-wise."
    },
    {
        "Y": "element-wise",
        "X": "How does nn.Mish apply the Mish function?",
        "Z": "nn.Mish Applies the Mish function, element-wise."
    },
    {
        "Y": "nn.Mish",
        "X": "What applies the Mish function, element-wise?",
        "Z": "nn.Mish Applies the Mish function, element-wise."
    },
    {
        "Y": "element-wise function",
        "X": "What kind of function does nn.Softplus apply?",
        "Z": "nn.Softplus Applies the element-wise function:"
    },
    {
        "Y": "soft shrinkage",
        "X": "What function does Softshrink apply?",
        "Z": "nn.Softshrink Applies the soft shrinkage function element wise:"
    },
    {
        "Y": "soft shrinkage",
        "X": "What function does nn.Softshrink apply element wise?",
        "Z": "nn.Softshrink Applies the soft shrinkage function element wise:"
    },
    {
        "Y": "element wise",
        "X": "How does nn.Softshrink apply the soft shrinkage function?",
        "Z": "nn.Softshrink Applies the soft shrinkage function element wise:"
    },
    {
        "Y": "element-wise",
        "X": "What kind of function does nn.Softsign apply?",
        "Z": "nn.Softsign Applies the element-wise function:"
    },
    {
        "Y": "element-wise function",
        "X": "What type of function does nn.Tanh apply?",
        "Z": "nn.Tanh Applies the element-wise function:"
    },
    {
        "Y": "the element-wise function",
        "X": "nn.Tanhshrink Applies what?",
        "Z": "nn.Tanhshrink Applies the element-wise function:"
    },
    {
        "Y": "Tensor",
        "X": "nn.Threshold Thresholds each element of what input?",
        "Z": "nn.Threshold Thresholds each element of the input Tensor."
    },
    {
        "Y": "Threshold",
        "X": "What Thresholds each element of the input Tensor?",
        "Z": "nn.Threshold Thresholds each element of the input Tensor."
    },
    {
        "Y": "nn.Softmin",
        "X": "What Applies the Softmin function to an n-dimensional input Tensor?",
        "Z": "nn.Softmin Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range[0, 1]and sum to 1."
    },
    {
        "Y": "0,1",
        "X": "What is the range of the elements of the n-dimensional output Tensor?",
        "Z": "nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1."
    },
    {
        "Y": "nn.Softmax",
        "X": "What Applies the Softmax function to an n-dimensional input Tensor?",
        "Z": "nn.Softmax Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1."
    },
    {
        "Y": "over features",
        "X": "How does nn.Softmax2d apply SoftMax to each spatial location?",
        "Z": "nn.Softmax2d Applies SoftMax over features to each spatial location."
    },
    {
        "Y": "SoftMax",
        "X": "nn.Softmax2d Applies what over features to each spatial location?",
        "Z": "nn.Softmax2d Applies SoftMax over features to each spatial location."
    },
    {
        "Y": "n-dimensional",
        "X": "What type of input Tensor does nn.LogSoftmax apply the function to?",
        "Z": "nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor."
    },
    {
        "Y": "nn.LogSoftmax",
        "X": "What Applies thelog(Softmax(x))log(textSoftmax(x))log(Softmax(",
        "Z": "nn.LogSoftmax Applies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor."
    },
    {
        "Y": "Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou",
        "X": "Who wrote the Efficient softmax approximation for GPUs?",
        "Z": "nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou."
    },
    {
        "Y": "Herv\u00e9 J\u00e9gou",
        "X": "Along with Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and what other person did they",
        "Z": "nn.AdaptiveLogSoftmaxWithLoss Efficient softmax approximation as described inEfficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou."
    },
    {
        "Y": "nn",
        "X": "What is the value of the number that applies Batch Normalization over a 2D or 3D input?",
        "Z": "nn.BatchNorm1d Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "nn.BatchNorm1d",
        "X": "What Applies Batch Normalization over a 2D or 3D input?",
        "Z": "nn.BatchNorm1d Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "4D",
        "X": "What input does nn.BatchNorm2d apply Batch Normalization over?",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "nn.BatchNorm2d",
        "X": "What Applies Batch Normalization over a 4D input?",
        "Z": "nn.BatchNorm2d Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "Accelerating Deep Network Training",
        "X": "What is the purpose of Batch Normalization?",
        "Z": "nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "nn.BatchNorm3d",
        "X": "What Applies Batch Normalization over a 5D input?",
        "Z": "nn.BatchNorm3d Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "the input.size(1)",
        "X": "The lazy initialization of thenum_featuresargument of theBatchNorm1d is inferred from what?",
        "Z": "nn.LazyBatchNorm1d a torch.nn.BatchNorm1dmodule with lazy initialization of thenum_featuresargument of theBatchNorm1dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size(1)",
        "X": "The lazy initialization of thenum_featuresargument of theBatchNorm2d is inferred from what?",
        "Z": "nn.LazyBatchNorm2d a torch.nn.BatchNorm2dmodule with lazy initialization of thenum_featuresargument of theBatchNorm2dthat is inferred from the input.size(1)."
    },
    {
        "Y": "the input.size(1)",
        "X": "The lazy initialization of thenum_featuresargument of theBatchNorm3d is inferred from what?",
        "Z": "nn.LazyBatchNorm3d a torch.nn.BatchNorm3dmodule with lazy initialization of thenum_featuresargument of theBatchNorm3dthat is inferred from the input.size(1)."
    },
    {
        "Y": "nn.GroupNorm",
        "X": "What Applies Group Normalization over a mini-batch of inputs?",
        "Z": "nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization"
    },
    {
        "Y": "nn.GroupNorm Applies Group Normalization over a mini-batch of inputs",
        "X": "What does nn.GroupNorm Applies Group Normalization over a mini-batch of inputs?",
        "Z": "nn.GroupNorm Applies Group Normalization over a mini-batch of inputs as described in the paperGroup Normalization"
    },
    {
        "Y": "nn.SyncBatchNorm",
        "X": "What Applies Batch Normalization over a N-Dimensional input?",
        "Z": "nn.SyncBatchNorm Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paperBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift."
    },
    {
        "Y": "nn",
        "X": "InstanceNorm1d Applies Instance Normalization over a 3D input?",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "a mini-batch of 1D inputs with optional additional channel dimension",
        "X": "What is a 3D input?",
        "Z": "nn.InstanceNorm1d Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "4D",
        "X": "What input does nn.InstanceNorm2d apply Instance Normalization over?",
        "Z": "nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "Instance Normalization: The Missing Ingredient for Fast Stylization",
        "X": "What is paper that describes Instance Normalization over a 4D input?",
        "Z": "nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "4D",
        "X": "nn.InstanceNorm2d Applies Instance Normalization over what input?",
        "Z": "nn.InstanceNorm2d Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "5D",
        "X": "What input does nn.InstanceNorm3d apply Instance Normalization over?",
        "Z": "nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "Instance Normalization: The Missing Ingredient for Fast Stylization",
        "X": "What is paper that describes Instance Normalization over a 5D input?",
        "Z": "nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "5D",
        "X": "nn.InstanceNorm3d Applies Instance Normalization over what input?",
        "Z": "nn.InstanceNorm3d Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paperInstance Normalization: The Missing Ingredient for Fast Stylization."
    },
    {
        "Y": "paperLayer Normalization",
        "X": "What document describes Layer Normalization over a mini-batch of inputs?",
        "Z": "nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization"
    },
    {
        "Y": "Layer Normalization",
        "X": "What does nn.LayerNorm apply over a mini-batch of inputs?",
        "Z": "nn.LayerNorm Applies Layer Normalization over a mini-batch of inputs as described in the paperLayer Normalization"
    },
    {
        "Y": "local response normalization",
        "X": "What does nn.LocalResponseNorm apply over an input signal composed of several input planes?",
        "Z": "nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "channels occupy the second dimension",
        "X": "What does local response normalization apply to an input signal composed of several input planes?",
        "Z": "nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "channels",
        "X": "What occupy the second dimension of the input plane?",
        "Z": "nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "second dimension",
        "X": "What dimension do channels occupy?",
        "Z": "nn.LocalResponseNorm Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension."
    },
    {
        "Y": "nn.RNNBase",
        "X": "What is enzyme that is responsible for the RNNBase?",
        "Z": "nn.RNNBase "
    },
    {
        "Y": "nn.RNNBase",
        "X": "What is the name for nn.RNNBase?",
        "Z": "nn.RNNBase "
    },
    {
        "Y": "input sequence",
        "X": "nn.RNN Applies a multi-layer Elman RNN withtanhtanhtanhorReLU",
        "Z": "nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence."
    },
    {
        "Y": "multi-layer",
        "X": "What type of Elman RNN does nn.RNN apply?",
        "Z": "nn.RNN Applies a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence."
    },
    {
        "Y": "nn",
        "X": "What is multi-layer long short-term memory (LSTM) RNN that is applied to an input sequence?",
        "Z": "nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence."
    },
    {
        "Y": "multi-layer",
        "X": "nn.LSTM Applies a what type of long short-term memory RNN to an input sequence?",
        "Z": "nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence."
    },
    {
        "Y": "nn.LSTM",
        "X": "What applies a multi-layer long short-term memory (LSTM) RNN to an input sequence?",
        "Z": "nn.LSTM Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence."
    },
    {
        "Y": "nn",
        "X": "What is GRU that applies a multi-layer gated recurrent unit to an input sequence?",
        "Z": "nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence."
    },
    {
        "Y": "multi-layer gated recurrent unit",
        "X": "What is nn.GRU?",
        "Z": "nn.GRU Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence."
    },
    {
        "Y": "tanh or ReLU non-linearity",
        "X": "What type of non-linearity does an Elman RNN cell have?",
        "Z": "nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity."
    },
    {
        "Y": "nn",
        "X": "What is an Elman RNN cell with tanh or ReLU non-linearity?",
        "Z": "nn.RNNCell An Elman RNN cell with tanh or ReLU non-linearity."
    },
    {
        "Y": "long short-term memory",
        "X": "What does LSTM stand for?",
        "Z": "nn.LSTMCell A long short-term memory (LSTM) cell."
    },
    {
        "Y": "long short-term memory",
        "X": "What is LSTMCell?",
        "Z": "nn.LSTMCell A long short-term memory (LSTM) cell."
    },
    {
        "Y": "gated recurrent unit",
        "X": "What is GRUCell?",
        "Z": "nn.GRUCell A gated recurrent unit (GRU) cell"
    },
    {
        "Y": "GRU",
        "X": "What is gated recurrent unit cell?",
        "Z": "nn.GRUCell A gated recurrent unit (GRU) cell"
    },
    {
        "Y": "transformer model",
        "X": "What is a transformer?",
        "Z": "nn.Transformer A transformer model."
    },
    {
        "Y": "transformer",
        "X": "What type of transformer is a transformer?",
        "Z": "nn.Transformer A transformer model."
    },
    {
        "Y": "nn",
        "X": "What is TransformerEncoder?",
        "Z": "nn.TransformerEncoder TransformerEncoder is a stack of N encoder layers"
    },
    {
        "Y": "N encoder layers",
        "X": "What is nn.TransformerEncoder a stack of?",
        "Z": "nn.TransformerEncoder TransformerEncoder is a stack of N encoder layers"
    },
    {
        "Y": "nn",
        "X": "What is the stack of N decoder layers?",
        "Z": "nn.TransformerDecoder TransformerDecoder is a stack of N decoder layers"
    },
    {
        "Y": "N",
        "X": "How many decoder layers does TransformerDecoder have?",
        "Z": "nn.TransformerDecoder TransformerDecoder is a stack of N decoder layers"
    },
    {
        "Y": "TransformerDecoder",
        "X": "What is a stack of N decoder layers called?",
        "Z": "nn.TransformerDecoder TransformerDecoder is a stack of N decoder layers"
    },
    {
        "Y": "self-attn and feedforward network",
        "X": "What is the TransformerEncoderLayer made up of?",
        "Z": "nn.TransformerEncoderLayer TransformerEncoderLayer is made up of self-attn and feedforward network."
    },
    {
        "Y": "self-attn, multi-head-attn and feedforward network",
        "X": "What is the TransformerDecoderLayer made up of?",
        "Z": "nn.TransformerDecoderLayer TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network."
    },
    {
        "Y": "argument-insensitive",
        "X": "Is nn.Identity a placeholder identity operator?",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive."
    },
    {
        "Y": "argument-insensitive",
        "X": "nn.Identity A placeholder identity operator that is what?",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive."
    },
    {
        "Y": "placeholder",
        "X": "What type of identity operator is argument-insensitive?",
        "Z": "nn.Identity A placeholder identity operator that is argument-insensitive."
    },
    {
        "Y": "xAT+b",
        "X": "What is y=?",
        "Z": "nn.Linear Applies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b"
    },
    {
        "Y": "wherein_featuresis inferred",
        "X": "What does nn.LazyLinear a torch.nn.Linearmodule do?",
        "Z": "nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred."
    },
    {
        "Y": "Linearmodule",
        "X": "What type of Atorch is nn.LazyLinear Atorch?",
        "Z": "nn.LazyLinear a torch.nn.Linearmodule wherein_featuresis inferred."
    },
    {
        "Y": "nn.Dropout",
        "X": "What is program that randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution."
    },
    {
        "Y": "nn.Dropout",
        "X": "What randomly zeroes some elements of the input tensor with probabilitypusing samples from a Bernoulli distribution?",
        "Z": "nn.Dropout During training, randomly zeroes some of the elements of the input tensor with probabilitypusing samples from a Bernoulli distribution."
    },
    {
        "Y": "Alpha Dropout",
        "X": "What does nn.AlphaDropout apply over the input?",
        "Z": "nn.AlphaDropout Applies Alpha Dropout over the input."
    },
    {
        "Y": "embeddings of a fixed dictionary and size",
        "X": "What does Embedding store?",
        "Z": "nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size."
    },
    {
        "Y": "embeddings",
        "X": "A simple lookup table that stores what of a fixed dictionary and size?",
        "Z": "nn.Embedding A simple lookup table that stores embeddings of a fixed dictionary and size."
    },
    {
        "Y": "nn",
        "X": "What is the value of the EmbeddingBag?",
        "Z": "nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings."
    },
    {
        "Y": "nn.EmbeddingBag",
        "X": "What Computes sums or means of \u2018bags\u2019 of embeddings without instantiating the intermediate embeddings?",
        "Z": "nn.EmbeddingBag Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings."
    },
    {
        "Y": "cosine similarity",
        "X": "What does nn.CosineSimilarity return?",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim."
    },
    {
        "Y": "nn.CosineSimilarity",
        "X": "What returns cosine similarity betweenx1x_1x1 andx2x_2x2?",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim."
    },
    {
        "Y": "computed along dim",
        "X": "CosineSimilarity Returns cosine similarity betweenx1x_1x1 andx2x_2x2",
        "Z": "nn.CosineSimilarity Returns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed along dim."
    },
    {
        "Y": "p-norm",
        "X": "What does nn.PairwiseDistance use to compute the batchwise pairwise distance between vectorsv1v_1v1",
        "Z": "nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:"
    },
    {
        "Y": "nn.PairwiseDistance Computes",
        "X": "What is the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2 using the p",
        "Z": "nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:"
    },
    {
        "Y": "p-norm",
        "X": "What is used to calculate the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2?",
        "Z": "nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:"
    },
    {
        "Y": "nn.PairwiseDistance",
        "X": "What computes the batchwise pairwise distance between vectorsv1v_1v1,v2v_2v2 using the ",
        "Z": "nn.PairwiseDistance Computes the batchwise pairwise distance between vectorsv1v_1v1\u200b,v2v_2v2\u200busing the p-norm:"
    },
    {
        "Y": "nn.L1Loss",
        "X": "What creates a criterion that measures the mean absolute error?",
        "Z": "nn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy."
    },
    {
        "Y": "mean absolute error",
        "X": "What does MAE stand for?",
        "Z": "nn.L1Loss Creates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy."
    },
    {
        "Y": "nn.MSELoss",
        "X": "What creates a criterion that measures the mean squared error?",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy."
    },
    {
        "Y": "criterion",
        "X": "What does nn.MSELoss create?",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy."
    },
    {
        "Y": "squared L2 norm",
        "X": "What is the criterion that measures the mean squared error?",
        "Z": "nn.MSELoss Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy."
    },
    {
        "Y": "nn.CrossEntropyLoss",
        "X": "What criterion combines LogSoftmaxandNLLLossin one single class?",
        "Z": "nn.CrossEntropyLoss This criterion combinesLogSoftmaxandNLLLossin one single class."
    },
    {
        "Y": "Connectionist Temporal Classification loss",
        "X": "What is CTCLoss?",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss."
    },
    {
        "Y": "The Connectionist Temporal Classification",
        "X": "What is nn.CTCLoss?",
        "Z": "nn.CTCLoss The Connectionist Temporal Classification loss."
    },
    {
        "Y": "negative log likelihood loss",
        "X": "What is nn.NLLLoss?",
        "Z": "nn.NLLLoss The negative log likelihood loss."
    },
    {
        "Y": "Poisson distribution",
        "X": "What type of distribution is the target?",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target."
    },
    {
        "Y": "Negative log likelihood loss",
        "X": "What is nn.PoissonNLLLoss?",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target."
    },
    {
        "Y": "Poisson distribution of target",
        "X": "Negative log likelihood loss with what?",
        "Z": "nn.PoissonNLLLoss Negative log likelihood loss with Poisson distribution of target."
    },
    {
        "Y": "negative log likelihood loss",
        "X": "What is the result of nn.GaussianNLLLoss Gaussian?",
        "Z": "nn.GaussianNLLLoss Gaussian negative log likelihood loss."
    },
    {
        "Y": "negative log likelihood loss",
        "X": "What is nn.GaussianNLLLoss Gaussian?",
        "Z": "nn.GaussianNLLLoss Gaussian negative log likelihood loss."
    },
    {
        "Y": "nn.Gaussian",
        "X": "What is NLLLoss Gaussian negative log likelihood loss?",
        "Z": "nn.GaussianNLLLoss Gaussian negative log likelihood loss."
    },
    {
        "Y": "nn.KLDivLoss",
        "X": "What is Kullback-Leibler divergence loss measure?",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure"
    },
    {
        "Y": "Kullback-Leibler",
        "X": "What is the divergence loss measure?",
        "Z": "nn.KLDivLoss The Kullback-Leibler divergence loss measure"
    },
    {
        "Y": "nn.BCELoss",
        "X": "What creates a criterion that measures the Binary Cross Entropy between the target and the output?",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output:"
    },
    {
        "Y": "Binary Cross Entropy",
        "X": "What does nn.BCELoss measure between the target and the output?",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output:"
    },
    {
        "Y": "criterion",
        "X": "What does nn.BCELoss create that measures the Binary Cross Entropy between the target and the output?",
        "Z": "nn.BCELoss Creates a criterion that measures the Binary Cross Entropy between the target and the output:"
    },
    {
        "Y": "nn.BCEWithLogitsLoss",
        "X": "What is loss that combines aSigmoidlayer and theBCELossin?",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class."
    },
    {
        "Y": "aSigmoidlayer and theBCELossin",
        "X": "What two classes does nn.BCEWithLogitsLoss combine?",
        "Z": "nn.BCEWithLogitsLoss This loss combines aSigmoidlayer and theBCELossin one single class."
    },
    {
        "Y": "1D mini-batch tensoryyy",
        "X": "What label contains 1 or -1?",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "1 or -1",
        "X": "What is the label of the 1D mini-batch tensoryyy?",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "1 or -1)",
        "X": "What does the label 1D mini-batch tensoryyyy contain?",
        "Z": "nn.MarginRankingLoss Creates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batchTensors, and a label 1D mini-batch tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "nn.HingeEmbeddingLoss",
        "X": "What measures the loss given an input tensorxxxand a labels tensoryyy?",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "1 or -1",
        "X": "What are the labels tensoryyy containing?",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "nn.HingeEmbeddingLoss",
        "X": "What measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1)",
        "Z": "nn.HingeEmbeddingLoss Measures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "outputyyy",
        "X": "What is a 2DTensorof target class indices?",
        "Z": "nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices)."
    },
    {
        "Y": "nn",
        "X": "What is the value of MultiLabelMarginLoss?",
        "Z": "nn.MultiLabelMarginLoss Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices)."
    },
    {
        "Y": "a squared term",
        "X": "What does nn.HuberLoss use if the absolute element-wise error falls below delta?",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "delta",
        "X": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "nn.HuberLoss",
        "X": "What creates a criterion that uses a squared term if the absolute element-wise error falls below delta?",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "criterion",
        "X": "What does nn.HuberLoss create?",
        "Z": "nn.HuberLoss Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise."
    },
    {
        "Y": "a squared term",
        "X": "What does nn.SmoothL1Loss use if the absolute element-wise error falls below beta?",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "nn.SmoothL1Loss",
        "X": "What creates a criterion that uses a squared term if the absolute element-wise error falls below beta?",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "squared term",
        "X": "What term does nn.SmoothL1Loss use if the absolute element-wise error falls below beta?",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "L1",
        "X": "What Loss creates a criterion that uses a squared term if the absolute element-wise error falls below beta?",
        "Z": "nn.SmoothL1Loss Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise."
    },
    {
        "Y": "nn.SoftMarginLoss",
        "X": "What creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target ",
        "Z": "nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "criterion",
        "X": "What does nn.SoftMarginLoss create?",
        "Z": "nn.SoftMarginLoss Creates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1)."
    },
    {
        "Y": "multi-label one-versus-all loss based on max-entropy",
        "X": "What does nn.MultiLabelSoftMarginLoss create a criterion that optimizes?",
        "Z": "nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C)."
    },
    {
        "Y": "max-entropy",
        "X": "What is the criterion that optimizes a multi-label one-versus-all loss based on?",
        "Z": "nn.MultiLabelSoftMarginLoss Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C)."
    },
    {
        "Y": "nn.CosineEmbeddingLoss",
        "X": "What creates a criterion that measures the loss given input tensors?",
        "Z": "nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band a Tensorlabelyyywith values 1 or -1."
    },
    {
        "Y": "criterion",
        "X": "What does nn.CosineEmbeddingLoss create?",
        "Z": "nn.CosineEmbeddingLoss Creates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band a Tensorlabelyyywith values 1 or -1."
    },
    {
        "Y": "outputyyy",
        "X": "What is a 1D tensor of target class indices?",
        "Z": "nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):"
    },
    {
        "Y": "nn",
        "X": "What is the value of MultiMarginLoss?",
        "Z": "nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):"
    },
    {
        "Y": "a 1D tensor of target class indices",
        "X": "What is outputyyy?",
        "Z": "nn.MultiMarginLoss Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):"
    },
    {
        "Y": "nn.TripletMarginLoss",
        "X": "What creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2",
        "Z": "nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000."
    },
    {
        "Y": "triplet loss",
        "X": "What does nn.TripletMarginLoss measure?",
        "Z": "nn.TripletMarginLoss Creates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000."
    },
    {
        "Y": "triplet loss",
        "X": "What does nn.TripletMarginWithDistanceLoss measure?",
        "Z": "nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d)."
    },
    {
        "Y": "criterion",
        "X": "What does nn.TripletMarginWithDistanceLoss create?",
        "Z": "nn.TripletMarginWithDistanceLoss Creates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute the relationship between the anchor and positive example (\u201cpositive distance\u201d) and the anchor and negative example (\u201cnegative distance\u201d)."
    },
    {
        "Y": "upscale factor",
        "X": "What is r in a tensor of shape?",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor."
    },
    {
        "Y": "upscale",
        "X": "What type of factor is r?",
        "Z": "nn.PixelShuffle Rearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is an upscale factor."
    },
    {
        "Y": "nn",
        "X": "What is the value of the PixelUnshuffle?",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor."
    },
    {
        "Y": "r",
        "X": "What is thePixelShuffleoperation by rearranging elements in a tensor of shape(,C,H",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor."
    },
    {
        "Y": "a downscale factor",
        "X": "What is r?",
        "Z": "nn.PixelUnshuffle Reverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is a downscale factor."
    },
    {
        "Y": "nn",
        "X": "What is Upsample?",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data."
    },
    {
        "Y": "multi-channel",
        "X": "What type of data does nn.Upsample Upsample?",
        "Z": "nn.Upsample Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data."
    },
    {
        "Y": "upsampling",
        "X": "What does UpsamplingNearest2d do?",
        "Z": "nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels."
    },
    {
        "Y": "nearest neighbor",
        "X": "nn.UpsamplingNearest2d Applies a 2D upsampling to an input signal composed of several input",
        "Z": "nn.UpsamplingNearest2d Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels."
    },
    {
        "Y": "bilinear",
        "X": "What type of upsampling does nn.UpsamplingBilinear2d apply?",
        "Z": "nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels."
    },
    {
        "Y": "nn.UpsamplingBilinear2d",
        "X": "What Applies a 2D bilinear upsampling to an input signal composed of several input channels?",
        "Z": "nn.UpsamplingBilinear2d Applies a 2D bilinear upsampling to an input signal composed of several input channels."
    },
    {
        "Y": "nn.ChannelShuffle",
        "X": "What divides the channels in a tensor of shape?",
        "Z": "nn.ChannelShuffle Divide the channels in a tensor of shape(\u2217,C,H,W)(*, C , H, W)(\u2217,C,H,W)into g groups and rearrange them as(\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W)(\u2217,C,g\u200bg,H,W), while keeping the original tensor shape."
    },
    {
        "Y": "g groups",
        "X": "What are the channels in a tensor of shape divided into?",
        "Z": "nn.ChannelShuffle Divide the channels in a tensor of shape(\u2217,C,H,W)(*, C , H, W)(\u2217,C,H,W)into g groups and rearrange them as(\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W)(\u2217,C,g\u200bg,H,W), while keeping the original tensor shape."
    },
    {
        "Y": "module level",
        "X": "At what level does nn.DataParallel implement data parallelism?",
        "Z": "nn.DataParallel Implements data parallelism at the module level."
    },
    {
        "Y": "data parallelism",
        "X": "What does nn.DataParallel implement at the module level?",
        "Z": "nn.DataParallel Implements data parallelism at the module level."
    },
    {
        "Y": "ontorch.distributedpackage",
        "X": "What is nn.parallel.DistributedDataParallel based on?",
        "Z": "nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level."
    },
    {
        "Y": "distributed data parallelism",
        "X": "What does nn.parallel.DistributedDataParallel implement?",
        "Z": "nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level."
    },
    {
        "Y": "module level",
        "X": "At what level is distributed data parallelism based?",
        "Z": "nn.parallel.DistributedDataParallel Implements distributed data parallelism that is based ontorch.distributedpackage at the module level."
    },
    {
        "Y": "clip_grad_norm",
        "X": "What is clip that shows the gradient norm of an iterable of parameters?",
        "Z": "clip_grad_norm_ Clips gradient norm of an iterable of parameters."
    },
    {
        "Y": "clip_grad_norm",
        "X": "What is Clips gradient norm of an iterable of parameters?",
        "Z": "clip_grad_norm_ Clips gradient norm of an iterable of parameters."
    },
    {
        "Y": "clip_grad_value",
        "X": "What is clip that shows the gradient of an iterable of parameters at specified value?",
        "Z": "clip_grad_value_ Clips gradient of an iterable of parameters at specified value."
    },
    {
        "Y": "clip_grad_value",
        "X": "What is clips gradient of an iterable of parameters at specified value?",
        "Z": "clip_grad_value_ Clips gradient of an iterable of parameters at specified value."
    },
    {
        "Y": "specified value",
        "X": "At what value does clip_grad_value represent a gradient of an iterable of parameters?",
        "Z": "clip_grad_value_ Clips gradient of an iterable of parameters at specified value."
    },
    {
        "Y": "one",
        "X": "How many vectors do parameters_to_vector convert to?",
        "Z": "parameters_to_vector Convert parameters to one vector"
    },
    {
        "Y": "parameters",
        "X": "What does _to_vector Convert parameters to one vector?",
        "Z": "parameters_to_vector Convert parameters to one vector"
    },
    {
        "Y": "one",
        "X": "parameters_to_vector Convert parameters to how many vectors?",
        "Z": "parameters_to_vector Convert parameters to one vector"
    },
    {
        "Y": "one",
        "X": "How many vectors does vector_to_parameters convert to the parameters?",
        "Z": "vector_to_parameters Convert one vector to the parameters"
    },
    {
        "Y": "vector_to_parameters",
        "X": "What is the conversion of one vector to the parameters?",
        "Z": "vector_to_parameters Convert one vector to the parameters"
    },
    {
        "Y": "new pruning techniques",
        "X": "What is prune.BasePruningMethod used for?",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques."
    },
    {
        "Y": "pruning techniques",
        "X": "What is prune.BasePruningMethod Abstract base class for creation of?",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques."
    },
    {
        "Y": "prune.BasePruningMethod",
        "X": "What is the abstract base class for creation of new pruning techniques?",
        "Z": "prune.BasePruningMethod Abstract base class for creation of new pruning techniques."
    },
    {
        "Y": "prune",
        "X": "What is container holding a sequence of methods for iterative pruning?",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning."
    },
    {
        "Y": "iterative pruning",
        "X": "For what type of pruning is prune.PruningContainer used?",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning."
    },
    {
        "Y": "prune.PruningContainer Container",
        "X": "What holds a sequence of pruning methods for iterative pruning?",
        "Z": "prune.PruningContainer Container holding a sequence of pruning methods for iterative pruning."
    },
    {
        "Y": "mask of ones",
        "X": "What does prune.Identity Utility generate the pruning parametrization with?",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones."
    },
    {
        "Y": "prune",
        "X": "What is pruning method that does not prune any units but generates the pruning parametrization with a mask of ones?",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones."
    },
    {
        "Y": "pruning parametrization",
        "X": "What does the prune.Identity pruning method generate?",
        "Z": "prune.Identity Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones."
    },
    {
        "Y": "RandomUnstructured Prune",
        "X": "What is currently unpruned?",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random."
    },
    {
        "Y": "random",
        "X": "At what frequency are prune.RandomUnstructured Prune units in a tensor?",
        "Z": "prune.RandomUnstructured Prune (currently unpruned) units in a tensor at random."
    },
    {
        "Y": "zeroing out the ones with the lowest L1-norm",
        "X": "How can prune.L1Unstructured Prune units in a tensor be pruned?",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm."
    },
    {
        "Y": "prune.L1Unstructured Prune",
        "X": "What are currently unpruned units in a tensor?",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm."
    },
    {
        "Y": "tensor",
        "X": "What is prune.L1Unstructured Prune units in?",
        "Z": "prune.L1Unstructured Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm."
    },
    {
        "Y": "prune",
        "X": "What does RandomStructured Prune?",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random."
    },
    {
        "Y": "random",
        "X": "At what frequency do prune.RandomStructured Prune entire channels in a tensor?",
        "Z": "prune.RandomStructured Prune entire (currently unpruned) channels in a tensor at random."
    },
    {
        "Y": "Ln-norm",
        "X": "What tensor is prune.LnStructured based on?",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm."
    },
    {
        "Y": "prune.LnStructured Prune",
        "X": "What is entire (currently unpruned) channels in a tensor based on their Ln-",
        "Z": "prune.LnStructured Prune entire (currently unpruned) channels in a tensor based on their Ln-norm."
    },
    {
        "Y": "prune",
        "X": "What type of tree is a CustomFromMask?",
        "Z": "prune.CustomFromMask "
    },
    {
        "Y": "CustomFromMask",
        "X": "What is prune?",
        "Z": "prune.CustomFromMask "
    },
    {
        "Y": "prune.CustomFromMask",
        "X": "What is the name of what?",
        "Z": "prune.CustomFromMask "
    },
    {
        "Y": "prune.identity",
        "X": "What applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodule?",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units."
    },
    {
        "Y": "tensor",
        "X": "What does prune.identity apply pruning reparametrization to?",
        "Z": "prune.identity Applies pruning reparametrization to the tensor corresponding to the parameter callednameinmodulewithout actually pruning any units."
    },
    {
        "Y": "random",
        "X": "What is prune tensor?",
        "Z": "prune.random_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units selected at random."
    },
    {
        "Y": "prune.custom_from_mask",
        "X": "What Prunes tensor corresponding to parameter callednameinmodule?",
        "Z": "prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask."
    },
    {
        "Y": "removing the specifiedamountof",
        "X": "What does prune.l1_unstructured Prunes tensor do by removing the specifiedamountof?",
        "Z": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm."
    },
    {
        "Y": "prune.l1_unstructured",
        "X": "What Prunes tensor corresponding to parameter callednameinmodule by removing the specifiedamountof (currently unprune",
        "Z": "prune.l1_unstructured Prunes tensor corresponding to parameter callednameinmoduleby removing the specifiedamountof (currently unpruned) units with the lowest L1-norm."
    },
    {
        "Y": "global_unstructured",
        "X": "What type of prunes tensors?",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method."
    },
    {
        "Y": "tensors",
        "X": "What does prune.global_unstructured globally prune?",
        "Z": "prune.global_unstructured Globally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method."
    },
    {
        "Y": "pre-computed mask inmask",
        "X": "Prunes tensor corresponding to parameter callednameinmodule by applying what?",
        "Z": "prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask."
    },
    {
        "Y": "pre-computed",
        "X": "What type of mask inmask does prune.custom_from_mask use?",
        "Z": "prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask."
    },
    {
        "Y": "prune.custom_from_mask",
        "X": "What Prunes tensor corresponding to parameter callednameinmodule by applying the pre-computed mask inmask?",
        "Z": "prune.custom_from_mask Prunes tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask."
    },
    {
        "Y": "forward hook",
        "X": "Where does prune.remove remove the pruning method from?",
        "Z": "prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook."
    },
    {
        "Y": "prune.remove",
        "X": "What removes the pruning reparameterization from a module and the pruning method from the forward hook?",
        "Z": "prune.remove Removes the pruning reparameterization from a module and the pruning method from the forward hook."
    },
    {
        "Y": "theBasePruningMethod",
        "X": "What do modules inherit from?",
        "Z": "prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod."
    },
    {
        "Y": "theBasePruningMethod",
        "X": "What do modules that inherit from prune.is_pruned inherit from?",
        "Z": "prune.is_pruned Check whethermoduleis pruned by looking forforward_pre_hooksin its modules that inherit from theBasePruningMethod."
    },
    {
        "Y": "weight normalization",
        "X": "What does weight_norm apply to a parameter in a given module?",
        "Z": "weight_norm Applies weight normalization to a parameter in the given module."
    },
    {
        "Y": "weight_norm",
        "X": "What applies weight normalization to a parameter in the given module?",
        "Z": "weight_norm Applies weight normalization to a parameter in the given module."
    },
    {
        "Y": "weight normalization",
        "X": "weight_norm Applies what to a parameter in a given module?",
        "Z": "weight_norm Applies weight normalization to a parameter in the given module."
    },
    {
        "Y": "remove_weight_norm",
        "X": "What removes the weight normalization reparameterization from a module?",
        "Z": "remove_weight_norm Removes the weight normalization reparameterization from a module."
    },
    {
        "Y": "spectral normalization",
        "X": "What does spectral_norm apply to a parameter in a given module?",
        "Z": "spectral_norm Applies spectral normalization to a parameter in the given module."
    },
    {
        "Y": "spectral_norm",
        "X": "What Applies spectral normalization to a parameter in the given module?",
        "Z": "spectral_norm Applies spectral normalization to a parameter in the given module."
    },
    {
        "Y": "remove_spectral_norm",
        "X": "What removes the spectral normalization reparameterization from a module?",
        "Z": "remove_spectral_norm Removes the spectral normalization reparameterization from a module."
    },
    {
        "Y": "spectral normalization reparameterization",
        "X": "What does remove_spectral_norm remove from a module?",
        "Z": "remove_spectral_norm Removes the spectral normalization reparameterization from a module."
    },
    {
        "Y": "parametrizations",
        "X": "What applies spectral normalization to a parameter in a given module?",
        "Z": "parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module."
    },
    {
        "Y": "parametrizations",
        "X": "What does spectral_norm do?",
        "Z": "parametrizations.spectral_norm Applies spectral normalization to a parameter in the given module."
    },
    {
        "Y": "parametrization",
        "X": "What adds a parametrization to a tensor in a module?",
        "Z": "parametrize.register_parametrization Adds a parametrization to a tensor in a module."
    },
    {
        "Y": "module",
        "X": "In what type of module does parametrize.register_parametrization add a parametrization to a tensor?",
        "Z": "parametrize.register_parametrization Adds a parametrization to a tensor in a module."
    },
    {
        "Y": "parametrize",
        "X": "What does remove the parametrizations on a tensor in a module?",
        "Z": "parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module."
    },
    {
        "Y": "module",
        "X": "What type of module does parametrize.remove_parametrizations remove the parametrizations on a tensor in?",
        "Z": "parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module."
    },
    {
        "Y": "parametrizations",
        "X": "What removes the parametrizations on a tensor in a module?",
        "Z": "parametrize.remove_parametrizations Removes the parametrizations on a tensor in a module."
    },
    {
        "Y": "parametrize.cached Context manager",
        "X": "What enables the caching system within parametrizations registered withregister_parametrization()?",
        "Z": "parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization()."
    },
    {
        "Y": "parametrizations",
        "X": "The parametrize.cached Context manager enables the caching system within what?",
        "Z": "parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization()."
    },
    {
        "Y": "parametrizations registered withregister_parametrization()",
        "X": "What does the parametrize.cached Context manager enable the caching system within?",
        "Z": "parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization()."
    },
    {
        "Y": "parametrize.cached",
        "X": "What is Context manager that enables the caching system within parametrizations registered withregister_parametrization()?",
        "Z": "parametrize.cached Context manager that enables the caching system within parametrizations registered withregister_parametrization()."
    },
    {
        "Y": "parametrize.is_parametrized",
        "X": "What ReturnsTrue if module has an active parametrization?",
        "Z": "parametrize.is_parametrized ReturnsTrue if module has an active parametrization."
    },
    {
        "Y": "parametrize.ParametrizationList",
        "X": "What is container that holds and manages the originalparameter or buffer of a parametrizedtorch.nn.",
        "Z": "parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module."
    },
    {
        "Y": "parametrize.ParametrizationList",
        "X": "What is a sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Mod",
        "Z": "parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module."
    },
    {
        "Y": "parametrizedtorch",
        "X": "What is a parametrize.ParametrizationList a sequential container that holds and manages theoriginalparameter or buffer of?",
        "Z": "parametrize.ParametrizationList A sequential container that holds and manages theoriginalparameter or buffer of a parametrizedtorch.nn.Module."
    },
    {
        "Y": "nn.utils.rnn.PackedSequence",
        "X": "What holds the data and list ofbatch_sizesof a packed sequence?",
        "Z": "nn.utils.rnn.PackedSequence Holds the data and list ofbatch_sizesof a packed sequence."
    },
    {
        "Y": "Tensor",
        "X": "What type of sequence contains padded sequences of variable length?",
        "Z": "nn.utils.rnn.pack_padded_sequence Packs a Tensor containing padded sequences of variable length."
    },
    {
        "Y": "variable length",
        "X": "What type of sequences are padded sequences?",
        "Z": "nn.utils.rnn.pack_padded_sequence Packs a Tensor containing padded sequences of variable length."
    },
    {
        "Y": "padded sequences",
        "X": "What type of sequences does nn.utils.rnn.pack_padded_sequence Pack a Tensor",
        "Z": "nn.utils.rnn.pack_padded_sequence Packs a Tensor containing padded sequences of variable length."
    },
    {
        "Y": "variable length sequences",
        "X": "Pads a packed batch of what?",
        "Z": "nn.utils.rnn.pad_packed_sequence Pads a packed batch of variable length sequences."
    },
    {
        "Y": "variable length",
        "X": "What type of Tensors does nn.utils.rnn.pad_sequence Pad a list of?",
        "Z": "nn.utils.rnn.pad_sequence Pad a list of variable length Tensors withpadding_value"
    },
    {
        "Y": "Tensors",
        "X": "Pad a list of variable length what?",
        "Z": "nn.utils.rnn.pad_sequence Pad a list of variable length Tensors withpadding_value"
    },
    {
        "Y": "variable length",
        "X": "What type of Tensors does nn.utils.rnn.pack_sequence pack?",
        "Z": "nn.utils.rnn.pack_sequence Packs a list of variable length Tensors"
    },
    {
        "Y": "Tensors",
        "X": "What is the variable length of a pack?",
        "Z": "nn.utils.rnn.pack_sequence Packs a list of variable length Tensors"
    },
    {
        "Y": "dims",
        "X": "nn.Flatten Flattens a contiguous range of what into a tensor?",
        "Z": "nn.Flatten Flattens a contiguous range of dims into a tensor."
    },
    {
        "Y": "tensor",
        "X": "nn.Flatten Flattens a contiguous range of dims into what?",
        "Z": "nn.Flatten Flattens a contiguous range of dims into a tensor."
    },
    {
        "Y": "tensor dim",
        "X": "What does nn.Unflatten unflatten?",
        "Z": "nn.Unflatten Unflattens a tensor dim expanding it to a desired shape."
    },
    {
        "Y": "expanding it to a desired shape",
        "X": "What does Unflatten do to a tensor dim?",
        "Z": "nn.Unflatten Unflattens a tensor dim expanding it to a desired shape."
    },
    {
        "Y": "lazily initialize parameters",
        "X": "What is the purpose of nn.modules.lazy.LazyModuleMixin?",
        "Z": "nn.modules.lazy.LazyModuleMixin A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d"
    },
    {
        "Y": "lazy modules",
        "X": "What is modules that lazily initialize parameters?",
        "Z": "nn.modules.lazy.LazyModuleMixin A mixin for modules that lazily initialize parameters, also known as \u201clazy modules.\u201d"
    },
    {
        "Y": "1-D False",
        "X": "What is sorted_sequence[i-1]values[m][n]?",
        "Z": "1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i]"
    },
    {
        "Y": "1-D",
        "X": "What is False sorted_sequence[i-1]values[m][n]...[l][x",
        "Z": "1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i]"
    },
    {
        "Y": "False",
        "X": "What is sorted_sequence[i-1]values[m][n]...[l][x]=",
        "Z": "1-D False sorted_sequence[i-1]<values[m][n]...[l][x]<=sorted_sequence[i]"
    },
    {
        "Y": "True",
        "X": "sorted_sequence[i-1]=values[m][n]?",
        "Z": "1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]"
    },
    {
        "Y": "1-D",
        "X": "What is true sorted_sequence[i-1]=values[m][n]?",
        "Z": "1-D True sorted_sequence[i-1]<=values[m][n]...[l][x]<sorted_sequence[i]"
    },
    {
        "Y": "N-D False",
        "X": "What is sorted_sequence[m][n]?",
        "Z": "N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i]"
    },
    {
        "Y": "False",
        "X": "What is the default value for sorted_sequence[m][n]?",
        "Z": "N-D False sorted_sequence[m][n]...[l][i-1]<values[m][n]...[l][x]<=sorted_sequence[m][n]...[l][i]"
    },
    {
        "Y": "True",
        "X": "What is the value of sorted_sequence[m]?",
        "Z": "N-D True sorted_sequence[m][n]...[l][i-1]<=values[m][n]...[l][x]<sorted_sequence[m][n]...[l][i]"
    },
    {
        "Y": "Visual Studio 2019",
        "X": "What is the latest version of Microsoft Visual Studio?",
        "Z": "10.1 Visual Studio 2019 (16.X) (_MSC_VER< 1930) 1.3.0 ~ 1.7.0"
    },
    {
        "Y": "1.3.0",
        "X": "10.1 Visual Studio 2019 (16.X) (_MSC_VER 1930)  1.7.0?",
        "Z": "10.1 Visual Studio 2019 (16.X) (_MSC_VER< 1930) 1.3.0 ~ 1.7.0"
    },
    {
        "Y": "10.2",
        "X": "What is the version number of Visual Studio 2019?",
        "Z": "10.2 Visual Studio 2019 (16.X) (_MSC_VER< 1930) 1.5.0 ~ 1.7.0"
    },
    {
        "Y": "Visual Studio 2019",
        "X": "What is the latest version of Visual Studio?",
        "Z": "11.0 Visual Studio 2019 (16.X) (_MSC_VER< 1930) 1.7.0"
    },
    {
        "Y": "Fourier transform",
        "X": "fft Computes the one dimensional discrete what of input?",
        "Z": "fft Computes the one dimensional discrete Fourier transform of input."
    },
    {
        "Y": "Fourier transform",
        "X": "ifft Computes the one dimensional inverse discrete what of input?",
        "Z": "ifft Computes the one dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "fft2",
        "X": "What computes the 2 dimensional discrete Fourier transform of input?",
        "Z": "fft2 Computes the 2 dimensional discrete Fourier transform of input."
    },
    {
        "Y": "fft2",
        "X": "What computes the 2 dimensional discrete Fourier transform of input?",
        "Z": "fft2 Computes the 2 dimensional discrete Fourier transform of input."
    },
    {
        "Y": "ifft2",
        "X": "What computes the 2 dimensional inverse discrete Fourier transform of input?",
        "Z": "ifft2 Computes the 2 dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "ifft2",
        "X": "What Computes the 2 dimensional inverse discrete Fourier transform of input?",
        "Z": "ifft2 Computes the 2 dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "Fourier transform",
        "X": "fftn Computes the N dimensional discrete what?",
        "Z": "fftn Computes the N dimensional discrete Fourier transform of input."
    },
    {
        "Y": "N dimensional discrete Fourier transform",
        "X": "What does fftn compute?",
        "Z": "fftn Computes the N dimensional discrete Fourier transform of input."
    },
    {
        "Y": "Fourier transform",
        "X": "fftn Computes the N dimensional discrete what of input?",
        "Z": "fftn Computes the N dimensional discrete Fourier transform of input."
    },
    {
        "Y": "Fourier transform",
        "X": "ifftn Computes the N dimensional inverse discrete what?",
        "Z": "ifftn Computes the N dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "N dimensional inverse discrete Fourier transform",
        "X": "What does ifftn compute?",
        "Z": "ifftn Computes the N dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "ifftn",
        "X": "What computes the N dimensional inverse discrete Fourier transform of input?",
        "Z": "ifftn Computes the N dimensional inverse discrete Fourier transform of input."
    },
    {
        "Y": "one dimensional Fourier transform",
        "X": "rfft Computes what kind of transform of real-valuedinput?",
        "Z": "rfft Computes the one dimensional Fourier transform of real-valuedinput."
    },
    {
        "Y": "rfft",
        "X": "What Computes the one dimensional Fourier transform of real-valuedinput?",
        "Z": "rfft Computes the one dimensional Fourier transform of real-valuedinput."
    },
    {
        "Y": "Fourier transform",
        "X": "rfft Computes the one dimensional what of real-valuedinput?",
        "Z": "rfft Computes the one dimensional Fourier transform of real-valuedinput."
    },
    {
        "Y": "irfft",
        "X": "What Computes the inverse ofrfft()?",
        "Z": "irfft Computes the inverse ofrfft()."
    },
    {
        "Y": "rfft2",
        "X": "What computes the 2-dimensional discrete Fourier transform of realinput?",
        "Z": "rfft2 Computes the 2-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "rfft2",
        "X": "What Computes the 2-dimensional discrete Fourier transform of realinput?",
        "Z": "rfft2 Computes the 2-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "realinput",
        "X": "rfft2 Computes the 2-dimensional discrete Fourier transform of what?",
        "Z": "rfft2 Computes the 2-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "irfft2",
        "X": "What Computes the inverse ofrfft2()?",
        "Z": "irfft2 Computes the inverse ofrfft2()."
    },
    {
        "Y": "N-dimensional discrete Fourier transform",
        "X": "What does rfftn compute?",
        "Z": "rfftn Computes the N-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "realinput",
        "X": "rfftn Computes the N-dimensional discrete Fourier transform of what?",
        "Z": "rfftn Computes the N-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "rfftn",
        "X": "What computes the N-dimensional discrete Fourier transform of realinput?",
        "Z": "rfftn Computes the N-dimensional discrete Fourier transform of realinput."
    },
    {
        "Y": "irfftn",
        "X": "What Computes the inverse ofrfftn()?",
        "Z": "irfftn Computes the inverse ofrfftn()."
    },
    {
        "Y": "Fourier",
        "X": "hfft Computes the one dimensional discrete what transform of a Hermitian symmetricinputsignal?",
        "Z": "hfft Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal."
    },
    {
        "Y": "Fourier transform",
        "X": "hfft Computes the one dimensional discrete what of a Hermitian symmetricinputsignal?",
        "Z": "hfft Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal."
    },
    {
        "Y": "Hermitian symmetricinputsignal",
        "X": "hfft Computes the one dimensional discrete Fourier transform of what?",
        "Z": "hfft Computes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal."
    },
    {
        "Y": "ihfft",
        "X": "What Computes the inverse ofhfft()?",
        "Z": "ihfft Computes the inverse ofhfft()."
    },
    {
        "Y": "fftfreq",
        "X": "What computes the discrete Fourier Transform sample frequencies for a signal of sizen?",
        "Z": "fftfreq Computes the discrete Fourier Transform sample frequencies for a signal of sizen."
    },
    {
        "Y": "Fourier Transform",
        "X": "fftfreq Computes the discrete what?",
        "Z": "fftfreq Computes the discrete Fourier Transform sample frequencies for a signal of sizen."
    },
    {
        "Y": "sizen",
        "X": "rfftfreq Computes the sample frequencies forrfft()with a signal of what?",
        "Z": "rfftfreq Computes the sample frequencies forrfft()with a signal of sizen."
    },
    {
        "Y": "rfftfreq",
        "X": "What computes the sample frequencies forrfft?",
        "Z": "rfftfreq Computes the sample frequencies forrfft()with a signal of sizen."
    },
    {
        "Y": "rfftfreq",
        "X": "What Computes the sample frequencies forrfft() with a signal of sizen?",
        "Z": "rfftfreq Computes the sample frequencies forrfft()with a signal of sizen."
    },
    {
        "Y": "n-dimensional",
        "X": "fftshift reorders what dimension of FFT data?",
        "Z": "fftshift Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first."
    },
    {
        "Y": "negative frequency terms",
        "X": "What does fftshift order FFT data to have first?",
        "Z": "fftshift Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first."
    },
    {
        "Y": "byfftn()",
        "X": "fftshift Reorders n-dimensional FFT data as provided by what?",
        "Z": "fftshift Reorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first."
    },
    {
        "Y": "ifftshift Inverse offftshift()",
        "X": "What is function that does offftshift?",
        "Z": "ifftshift Inverse offftshift()."
    },
    {
        "Y": "offftshift",
        "X": "What is ifftshift Inverse?",
        "Z": "ifftshift Inverse offftshift()."
    },
    {
        "Y": "True boundaries",
        "X": "What does i-1 mean?",
        "Z": "True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i]"
    },
    {
        "Y": "False boundaries",
        "X": "What does [i-1]input[m][n]...[l][x]=boundaries[i]?",
        "Z": "False boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i]"
    },
    {
        "Y": "True boundaries",
        "X": "What does [i-1]=input[m][n]...[l][x]boundaries[i]?",
        "Z": "True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i]"
    },
    {
        "Y": "just-in-time compilation",
        "X": "What type of compilation is used to optimize a function?",
        "Z": "trace Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation."
    },
    {
        "Y": "trace Trace",
        "X": "What will return an executable orScriptFunction that will be optimized using just-in-time compilation?",
        "Z": "trace Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation."
    },
    {
        "Y": "just-in-time compilation",
        "X": "What is used to optimize a function?",
        "Z": "trace Trace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation."
    },
    {
        "Y": "Compilesfn",
        "X": "What does script_if_tracing call when it is first called during tracing?",
        "Z": "script_if_tracing Compilesfnwhen it is first called during tracing."
    },
    {
        "Y": "script_if_tracing",
        "X": "What compilesfn when it is first called during tracing?",
        "Z": "script_if_tracing Compilesfnwhen it is first called during tracing."
    },
    {
        "Y": "script_if_tracing",
        "X": "What does Compilesfn do when it is first called during tracing?",
        "Z": "script_if_tracing Compilesfnwhen it is first called during tracing."
    },
    {
        "Y": "just-in-time compilation",
        "X": "What is used to optimize a script module?",
        "Z": "trace_module Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation."
    },
    {
        "Y": "trace_module",
        "X": "What is module that returns an executableScriptModule that will be optimized using just-in-time compilation?",
        "Z": "trace_module Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation."
    },
    {
        "Y": "just-in-time compilation",
        "X": "What is used to optimize a scriptModule?",
        "Z": "trace_module Trace a module and return an executableScriptModulethat will be optimized using just-in-time compilation."
    },
    {
        "Y": "an asynchronous task executingfunc",
        "X": "what does fork create?",
        "Z": "fork Creates an asynchronous task executingfuncand a reference to the value of the result of this execution."
    },
    {
        "Y": "fork",
        "X": "What creates an asynchronous task executingfuncand a reference to the value of the result of this execution?",
        "Z": "fork Creates an asynchronous task executingfuncand a reference to the value of the result of this execution."
    },
    {
        "Y": "a torch.jit",
        "X": "What is the name of a.Future[T]asynchronous task?",
        "Z": "wait Forces completion of a torch.jit.Future[T]asynchronous task, returning the result of the task."
    },
    {
        "Y": "Forces",
        "X": "What does a torch.jit.Future[T]asynchronous wait for?",
        "Z": "wait Forces completion of a torch.jit.Future[T]asynchronous task, returning the result of the task."
    },
    {
        "Y": "C++torch::jit::Module",
        "X": "ScriptModule A wrapper around what?",
        "Z": "ScriptModule A wrapper around C++torch::jit::Module."
    },
    {
        "Y": "ScriptModule",
        "X": "What is a wrapper around C++torch?",
        "Z": "ScriptModule A wrapper around C++torch::jit::Module."
    },
    {
        "Y": "ScriptModule",
        "X": "What is a wrapper around C++torch::jit::Module?",
        "Z": "ScriptModule A wrapper around C++torch::jit::Module."
    },
    {
        "Y": "C++torch::jit::Module",
        "X": "What is ScriptModule a wrapper around?",
        "Z": "ScriptModule A wrapper around C++torch::jit::Module."
    },
    {
        "Y": "ScriptFunction",
        "X": "What is functionally equivalent to aScriptModule?",
        "Z": "ScriptFunction Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters."
    },
    {
        "Y": "aScriptModule",
        "X": "What is ScriptFunction functionally equivalent to?",
        "Z": "ScriptFunction Functionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters."
    },
    {
        "Y": "clone it",
        "X": "What does freeze Freezing aScriptModule do?",
        "Z": "freeze Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph."
    },
    {
        "Y": "freeze",
        "X": "What will clone aScriptModule and attempt to inline the cloned module\u2019s submodules,",
        "Z": "freeze Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph."
    },
    {
        "Y": "TorchScript IR Graph",
        "X": "What does freeze Freezing attempt to inline the cloned module's submodules, parameters, and attributes as constants in",
        "Z": "freeze Freezing aScriptModulewill clone it and attempt to inline the cloned module\u2019s submodules, parameters, and attributes as constants in the TorchScript IR Graph."
    },
    {
        "Y": "save",
        "X": "How do you save an offline version of this module for use in a separate process?",
        "Z": "save Save an offline version of this module for use in a separate process."
    },
    {
        "Y": "offline",
        "X": "What kind of version of this module can be saved for use in a separate process?",
        "Z": "save Save an offline version of this module for use in a separate process."
    },
    {
        "Y": "with torch.jit.save",
        "X": "What did load Load aScriptModuleorScriptFunctionpreviously save?",
        "Z": "load Load aScriptModuleorScriptFunctionpreviously saved with torch.jit.save"
    },
    {
        "Y": "Load aScriptModuleorScriptFunction",
        "X": "What was previously saved with torch.jit.saved?",
        "Z": "load Load aScriptModuleorScriptFunctionpreviously saved with torch.jit.save"
    },
    {
        "Y": "with torch.jit.save",
        "X": "What did load Load aScriptModuleorScriptFunctionpreviously saved?",
        "Z": "load Load aScriptModuleorScriptFunctionpreviously saved with torch.jit.save"
    },
    {
        "Y": "Python",
        "X": "What language does ignore indicate to the compiler that a function or method should be ignored and left as?",
        "Z": "ignore This decorator indicates to the compiler that a function or method should be ignored and left as a Python function."
    },
    {
        "Y": "a function or method",
        "X": "What does the decorator indicate to the compiler that should be ignored and left as a Python function?",
        "Z": "ignore This decorator indicates to the compiler that a function or method should be ignored and left as a Python function."
    },
    {
        "Y": "ignored",
        "X": "The decorator indicates to the compiler that a function or method should be what and left as a Python function?",
        "Z": "ignore This decorator indicates to the compiler that a function or method should be ignored and left as a Python function."
    },
    {
        "Y": "raising of an exception",
        "X": "What does this decorator indicate to the compiler that a function or method should be ignored and replaced with?",
        "Z": "unused This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception."
    },
    {
        "Y": "raising of an exception",
        "X": "What does the decorator indicate to the compiler that a function or method should be ignored and replaced with?",
        "Z": "unused This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception."
    },
    {
        "Y": "decorator",
        "X": "What indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception?",
        "Z": "unused This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception."
    },
    {
        "Y": "TorchScript",
        "X": "The isinstance function provides for conatiner type refinement in what?",
        "Z": "isinstance This function provides for conatiner type refinement in TorchScript."
    },
    {
        "Y": "conatiner type refinement",
        "X": "What does isinstance provide for in TorchScript?",
        "Z": "isinstance This function provides for conatiner type refinement in TorchScript."
    },
    {
        "Y": "isinstance",
        "X": "What function provides for conatiner type refinement in TorchScript?",
        "Z": "isinstance This function provides for conatiner type refinement in TorchScript."
    },
    {
        "Y": "TorchScript",
        "X": "isinstance provides for conatiner type refinement in what?",
        "Z": "isinstance This function provides for conatiner type refinement in TorchScript."
    },
    {
        "Y": "attribute",
        "X": "What does Attribute This method indicate to the TorchScript compiler that the left-hand side expression is a class instance of?",
        "Z": "Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype."
    },
    {
        "Y": "TorchScript",
        "X": "What compiler uses Attribute This method to indicate that the left-hand side expression is a class instance attribute with type oftype?",
        "Z": "Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype."
    },
    {
        "Y": "class instance attribute",
        "X": "Attribute This method is used to indicate to the TorchScript compiler that the left-hand side expression is a what?",
        "Z": "Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype."
    },
    {
        "Y": "the TorchScript compiler",
        "X": "Attribute This method is mostly used to indicate to whom that the left-hand side expression is a class instance attribute with type oftype?",
        "Z": "Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype."
    },
    {
        "Y": "a class instance attribute with type oftype",
        "X": "Attribute This method is mostly used to indicate to the TorchScript compiler that the left-hand side expression is what?",
        "Z": "Attribute This method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype."
    },
    {
        "Y": "TorchScript compiler",
        "X": "Who uses the annotate method to hint at the type of the value?",
        "Z": "annotate This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value."
    },
    {
        "Y": "annotate",
        "X": "What is a pass-through function that returns the_value?",
        "Z": "annotate This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value."
    },
    {
        "Y": "TorchScript",
        "X": "What compiler uses the annotate method to hint at the type of the_value?",
        "Z": "annotate This method is a pass-through function that returnsthe_value, used to hint TorchScript compiler the type ofthe_value."
    },
    {
        "Y": "Frobenius norm",
        "X": "What is 'fro'?",
        "Z": "\u2019fro\u2019 Frobenius norm \u2013"
    },
    {
        "Y": "Frobenius norm",
        "X": "What is the term for 'fro'?",
        "Z": "\u2019fro\u2019 Frobenius norm \u2013"
    },
    {
        "Y": "nuclear norm",
        "X": "What is the definition of a nuclear standard?",
        "Z": "\u2018nuc\u2019 nuclear norm \u2013"
    },
    {
        "Y": "nuclear norm",
        "X": "What is \u2018nuc\u2019?",
        "Z": "\u2018nuc\u2019 nuclear norm \u2013"
    },
    {
        "Y": "Number",
        "X": "What is the sum of abs(x)**ord)**(1./ord)?",
        "Z": "Number \u2013 sum(abs(x)**ord)**(1./ord)"
    },
    {
        "Y": "sum(abs(x)**ord)**(1./ord)",
        "X": "What is a number?",
        "Z": "Number \u2013 sum(abs(x)**ord)**(1./ord)"
    },
    {
        "Y": "Linear / Identity 111",
        "X": "What is Linear / Identity 111?",
        "Z": "Linear / Identity 111"
    },
    {
        "Y": "Linear / Identity 111",
        "X": "What is the definition of Linear / Identity 111?",
        "Z": "Linear / Identity 111"
    },
    {
        "Y": "111",
        "X": "How many Conv1,2,3D are there?",
        "Z": "Conv{1,2,3}D 111"
    },
    {
        "Y": "111",
        "X": "How many conv1,2,3D?",
        "Z": "Conv{1,2,3}D 111"
    },
    {
        "Y": "Sigmoid 111",
        "X": "What is Sigmoid 111?",
        "Z": "Sigmoid 111"
    },
    {
        "Y": "Sigmoid 111",
        "X": "What is Sigmoid?",
        "Z": "Sigmoid 111"
    },
    {
        "Y": "Tanh",
        "X": "What is the term for 53frac5335?",
        "Z": "Tanh 53\\frac{5}{3}35\u200b"
    },
    {
        "Y": "21",
        "X": "How many lines does Leaky Relu have?",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b"
    },
    {
        "Y": "Leaky Relu",
        "X": "Who is responsible for 21+negative_slope2sqrtfrac21 + textnegative_s",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b"
    },
    {
        "Y": "Leaky Relu 21",
        "X": "What is negative slope2sqrtfrac21 + textnegative_slope",
        "Z": "Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b"
    },
    {
        "Y": "32-bit floating point torch",
        "X": "What is a float32ortorch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor"
    },
    {
        "Y": "32",
        "X": "How many bits is float32ortorch?",
        "Z": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor"
    },
    {
        "Y": "64-bit floating point torch",
        "X": "What is a double torch?",
        "Z": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor"
    },
    {
        "Y": "16",
        "X": "How many bits are in a floating point1 torch?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor"
    },
    {
        "Y": "HalfTensor",
        "X": "What torch is a cuda?",
        "Z": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor"
    },
    {
        "Y": "16-bit floating point2 torch",
        "X": "What type of torch is the bfloat16 torch?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor"
    },
    {
        "Y": "16-bit floating point2 torch",
        "X": "What is bfloat16 torch?",
        "Z": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor"
    },
    {
        "Y": "32",
        "X": "How many bits does a complex torch have?",
        "Z": "32-bit complex torch.complex32  "
    },
    {
        "Y": "32-bit",
        "X": "What type of torch is complex32?",
        "Z": "32-bit complex torch.complex32  "
    },
    {
        "Y": "complex32",
        "X": "What is 32-bit complex torch?",
        "Z": "32-bit complex torch.complex32  "
    },
    {
        "Y": "64",
        "X": "What is the bit count for a complex torch?",
        "Z": "64-bit complex torch.complex64  "
    },
    {
        "Y": "complex torch.complex64",
        "X": "What is 64-bit version of the torch?",
        "Z": "64-bit complex torch.complex64  "
    },
    {
        "Y": "64-bit",
        "X": "What kind of complex torch is complex64?",
        "Z": "64-bit complex torch.complex64  "
    },
    {
        "Y": "complex64",
        "X": "What is 64-bit complex torch?",
        "Z": "64-bit complex torch.complex64  "
    },
    {
        "Y": "8-bit",
        "X": "Int8 torch.cuda.CharTensor torch.cuda.CharTensor torch.cuda.",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor"
    },
    {
        "Y": "16",
        "X": "How many bits is a short torch?",
        "Z": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor"
    },
    {
        "Y": "32",
        "X": "How many bit integers are in the torch?",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor"
    },
    {
        "Y": "32-bit",
        "X": "Int32ortorch.int torch.cuda.IntTensor torch.cuda.IntT",
        "Z": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor"
    },
    {
        "Y": "64-bit",
        "X": "What type of integer is the torch?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor"
    },
    {
        "Y": "LongTensor",
        "X": "What is the torch.cuda.LongTensor?",
        "Z": "64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor"
    },
    {
        "Y": "Boolean torch",
        "X": "What is the BoolTensor torch?",
        "Z": "Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor"
    },
    {
        "Y": "Boolean torch",
        "X": "What is torch?",
        "Z": "Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor"
    },
    {
        "Y": "4-bit integer",
        "X": "What is a quantized torch?",
        "Z": "quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor /"
    },
    {
        "Y": "quantized 8-bit integer",
        "X": "What is a torch that is unsigned?",
        "Z": "quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor /"
    },
    {
        "Y": "quantized 8-bit integer",
        "X": "What is torch.qint8?",
        "Z": "quantized 8-bit integer (signed) torch.qint8 torch.CharTensor /"
    },
    {
        "Y": "32-bit integer",
        "X": "What is torch.qfint32?",
        "Z": "quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor /"
    },
    {
        "Y": "quantized 32-bit integer",
        "X": "What is torch.IntTensor?",
        "Z": "quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor /"
    },
    {
        "Y": "quantized 4-bit integer",
        "X": "What type of torch is unsigned?",
        "Z": "quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor /"
    },
    {
        "Y": "ByteTensor",
        "X": "What is a quantized 4-bit integer (unsigned)3 torch?",
        "Z": "quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor /"
    },
    {
        "Y": "quantized 4-bit integer (unsigned)3 torch",
        "X": "What is quantized 4-bit integer (unsigned)3 torch?",
        "Z": "quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor /"
    },
    {
        "Y": "Tensor.new_tensor",
        "X": "What returns a new Tensor withdataas the tensor data?",
        "Z": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data."
    },
    {
        "Y": "new Tensor",
        "X": "What does Tensor.new_tensor return?",
        "Z": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data."
    },
    {
        "Y": " size filled",
        "X": "Tensor.new_full returns a Tensor of what size?",
        "Z": "Tensor.new_full Returns a Tensor of  size filled with fill_value."
    },
    {
        "Y": "Tensor.new_full",
        "X": "What returns a Tensor of  size filled with fill_value?",
        "Z": "Tensor.new_full Returns a Tensor of  size filled with fill_value."
    },
    {
        "Y": " size filled with uninitialized data",
        "X": "What type of Tensor does new_empty return?",
        "Z": "Tensor.new_empty Returns a Tensor of  size filled with uninitialized data."
    },
    {
        "Y": "Tensor.new_empty",
        "X": "What returns a Tensor of  size filled with uninitialized data?",
        "Z": "Tensor.new_empty Returns a Tensor of  size filled with uninitialized data."
    },
    {
        "Y": "uninitialized data",
        "X": "What type of data does Tensor.new_empty return?",
        "Z": "Tensor.new_empty Returns a Tensor of  size filled with uninitialized data."
    },
    {
        "Y": " size filled",
        "X": "What type of Tensor is returned by Tensor.new_ones?",
        "Z": "Tensor.new_ones Returns a Tensor of  size filled with1."
    },
    {
        "Y": " size filled with1",
        "X": "What is the Tensor returned by Tensor.new_ones?",
        "Z": "Tensor.new_ones Returns a Tensor of  size filled with1."
    },
    {
        "Y": " size filled with0",
        "X": "What is the size of the Tensor returned by Tensor.new_zeros?",
        "Z": "Tensor.new_zeros Returns a Tensor of  size filled with0."
    },
    {
        "Y": " size filled",
        "X": "What type of Tensor returns a Tensor?",
        "Z": "Tensor.new_zeros Returns a Tensor of  size filled with0."
    },
    {
        "Y": "Tensor",
        "X": "What is true if the Tensor is stored on the GPU?",
        "Z": "Tensor.is_cuda Is True ifthe Tensor is stored on the GPU,False otherwise."
    },
    {
        "Y": "GPU",
        "X": "Where is the Tensor stored?",
        "Z": "Tensor.is_cuda Is True ifthe Tensor is stored on the GPU,False otherwise."
    },
    {
        "Y": "meta",
        "X": "What type of tensor is true if the Tensor is a meta tensor?",
        "Z": "Tensor.is_meta Is True ifthe Tensor is a meta tensor,False otherwise."
    },
    {
        "Y": "thetorch",
        "X": "What is device where the Tensor is located?",
        "Z": "Tensor.device Is The torch.devicewhere this Tensor is."
    },
    {
        "Y": "tobackward()",
        "X": "What is first call to compute gradients for itself?",
        "Z": "torch.Tensor.grad This attribute is None bydefault and becomes a Tensor the first time a call to backward() computes gradients forself."
    },
    {
        "Y": "Tensor.grad",
        "X": "What attribute is None byby default?",
        "Z": "Tensor.grad This attribute is None bydefault and becomes a Tensor the first time a call to backward() computes gradients forself."
    },
    {
        "Y": "Tensor.ndim Alias fordim",
        "X": "What is Tensor.ndim Alias fordim?",
        "Z": "Tensor.ndim Alias fordim()"
    },
    {
        "Y": "Tensor.ndim",
        "X": "What is Alias fordim?",
        "Z": "Tensor.ndim Alias fordim()"
    },
    {
        "Y": "imaginary values",
        "X": "What does the new tensor contain?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor."
    },
    {
        "Y": "imaginary values of the self tensor",
        "X": "What does Tensor.imag return?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor."
    },
    {
        "Y": "Tensor",
        "X": "What is.abs Seetorch.abs()?",
        "Z": "Tensor.abs Seetorch.abs()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.abs?",
        "Z": "Tensor.abs Seetorch.abs()"
    },
    {
        "Y": "Tensor.abs",
        "X": "What is Seetorch.abs() function?",
        "Z": "Tensor.abs Seetorch.abs()"
    },
    {
        "Y": "Tensor.abs_ In-place version ofabs()",
        "X": "What does Tensor.abs_ In-place version ofabs() do?",
        "Z": "Tensor.abs_ In-place version ofabs()"
    },
    {
        "Y": "Tensor.abs_ In-place",
        "X": "What is version ofabs()?",
        "Z": "Tensor.abs_ In-place version ofabs()"
    },
    {
        "Y": "Alias forabs",
        "X": "What does Tensor.absolute stand for?",
        "Z": "Tensor.absolute Alias forabs()"
    },
    {
        "Y": "Tensor",
        "X": "What is program that creates Alias forabs?",
        "Z": "Tensor.absolute Alias forabs()"
    },
    {
        "Y": "Tensor.absolute Alias",
        "X": "What type of Alias are forabs?",
        "Z": "Tensor.absolute Alias forabs()"
    },
    {
        "Y": "absolute()Alias forabs_()",
        "X": "What is the Tensor.absolute_ In-place version of?",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_()"
    },
    {
        "Y": "Tensor.absolute_ In-place version",
        "X": "What is in-place version of absolute()Alias forabs_()?",
        "Z": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_()"
    },
    {
        "Y": "Tensor.acos",
        "X": "What is Seetorch.acos?",
        "Z": "Tensor.acos Seetorch.acos()"
    },
    {
        "Y": "Seetorch.acos",
        "X": "What is Tensor.acos?",
        "Z": "Tensor.acos Seetorch.acos()"
    },
    {
        "Y": "Tensor.acos_ In-place version ofacos()",
        "X": "What does Tensor.acos_ In-place version ofacos() do?",
        "Z": "Tensor.acos_ In-place version ofacos()"
    },
    {
        "Y": "Tensor.acos",
        "X": "What is in-place version ofacos?",
        "Z": "Tensor.acos_ In-place version ofacos()"
    },
    {
        "Y": "Tensor.arccos",
        "X": "What is Seetorch.arccos?",
        "Z": "Tensor.arccos Seetorch.arccos()"
    },
    {
        "Y": "Seetorch.arccos",
        "X": "What is Tensor.arccos?",
        "Z": "Tensor.arccos Seetorch.arccos()"
    },
    {
        "Y": "Tensor.arccos",
        "X": "What is Seetorch.arccos?",
        "Z": "Tensor.arccos Seetorch.arccos()"
    },
    {
        "Y": "Tensor.arccos_ In-place version ofarccos()",
        "X": "What is arccos function?",
        "Z": "Tensor.arccos_ In-place version ofarccos()"
    },
    {
        "Y": "Tensor.arccos",
        "X": "What is In-place version ofarccos?",
        "Z": "Tensor.arccos_ In-place version ofarccos()"
    },
    {
        "Y": "Tensor",
        "X": "What is a scalar or tensor toselftensor?",
        "Z": "Tensor.add Add a scalar or tensor toselftensor."
    },
    {
        "Y": "scalar",
        "X": "What is a tensor toselftensor?",
        "Z": "Tensor.add Add a scalar or tensor toselftensor."
    },
    {
        "Y": "Tensor.add_ In-place version ofadd()",
        "X": "What is add function?",
        "Z": "Tensor.add_ In-place version ofadd()"
    },
    {
        "Y": "Tensor.add_ In-place",
        "X": "What is version of add()?",
        "Z": "Tensor.add_ In-place version ofadd()"
    },
    {
        "Y": "Tensor",
        "X": "What is the Seetorch?",
        "Z": "Tensor.addmm Seetorch.addmm()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.addbmm?",
        "Z": "Tensor.addbmm Seetorch.addbmm()"
    },
    {
        "Y": "Tensor",
        "X": "What is the term for a Seetorch?",
        "Z": "Tensor.addmm Seetorch.addmm()"
    },
    {
        "Y": "Tensor.addbmm_ In-place version ofaddbmm()",
        "X": "What is in-place version of addbmm()?",
        "Z": "Tensor.addbmm_ In-place version ofaddbmm()"
    },
    {
        "Y": "Tensor.addbmm_ In-place",
        "X": "What version ofaddbmm() is used?",
        "Z": "Tensor.addbmm_ In-place version ofaddbmm()"
    },
    {
        "Y": "Tensor.trunc Seetorch.trunc",
        "X": "What is file?",
        "Z": "Tensor.trunc Seetorch.trunc()"
    },
    {
        "Y": "Tensor.addcdiv",
        "X": "What does Seetorch.addcdiv() call?",
        "Z": "Tensor.addcdiv Seetorch.addcdiv()"
    },
    {
        "Y": "Tensor.addcdiv_ In-place version ofaddcdiv()",
        "X": "What is in-place version of addcdiv()?",
        "Z": "Tensor.addcdiv_ In-place version ofaddcdiv()"
    },
    {
        "Y": "Tensor.addcdiv_ In-place",
        "X": "What is version ofaddcdiv()?",
        "Z": "Tensor.addcdiv_ In-place version ofaddcdiv()"
    },
    {
        "Y": "Tensor.addcmul Seetorch.addcmul",
        "X": "What is program?",
        "Z": "Tensor.addcmul Seetorch.addcmul()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor?",
        "Z": "Tensor.xlogy Seetorch.xlogy()"
    },
    {
        "Y": "Tensor.addcmul",
        "X": "What does Seetorch.addcmul() do?",
        "Z": "Tensor.addcmul Seetorch.addcmul()"
    },
    {
        "Y": "Tensor.addcmul_ In-place version ofaddcmul()",
        "X": "What is in-place version of addcmul()?",
        "Z": "Tensor.addcmul_ In-place version ofaddcmul()"
    },
    {
        "Y": "Tensor.addcmul_ In-place",
        "X": "What version ofaddcmul() is used?",
        "Z": "Tensor.addcmul_ In-place version ofaddcmul()"
    },
    {
        "Y": "Tensor.addmm_ In-place version ofaddmm()",
        "X": "What is in-place version of addmm()?",
        "Z": "Tensor.addmm_ In-place version ofaddmm()"
    },
    {
        "Y": "Tensor.addmm_ In-place",
        "X": "What is version ofaddmm()?",
        "Z": "Tensor.addmm_ In-place version ofaddmm()"
    },
    {
        "Y": "Tensor.addmv Seetorch.addmv()",
        "X": "What does Tensor.addmv Seetorch.addmv do?",
        "Z": "Tensor.addmv Seetorch.addmv()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.addmv?",
        "Z": "Tensor.addmv Seetorch.addmv()"
    },
    {
        "Y": "Tensor.addmv",
        "X": "What is Seetorch.addmv() function?",
        "Z": "Tensor.addmv Seetorch.addmv()"
    },
    {
        "Y": "Tensor.addmv_ In-place version ofaddmv()",
        "X": "What is function used by addmv()?",
        "Z": "Tensor.addmv_ In-place version ofaddmv()"
    },
    {
        "Y": "Tensor.addmv_ In-place",
        "X": "What version ofaddmv() is used?",
        "Z": "Tensor.addmv_ In-place version ofaddmv()"
    },
    {
        "Y": "Tensor.addr Seetorch.addr()",
        "X": "What does Tensor.addr do?",
        "Z": "Tensor.addr Seetorch.addr()"
    },
    {
        "Y": "Seetorch.addr",
        "X": "What is Tensor.addr?",
        "Z": "Tensor.addr Seetorch.addr()"
    },
    {
        "Y": "Tensor.addr",
        "X": "What is Seetorch.addr?",
        "Z": "Tensor.addr Seetorch.addr()"
    },
    {
        "Y": "Tensor.addr_ In-place version ofaddr()",
        "X": "What is in-place version ofaddr()?",
        "Z": "Tensor.addr_ In-place version ofaddr()"
    },
    {
        "Y": "Tensor.addr_ In-place",
        "X": "What is version ofaddr()?",
        "Z": "Tensor.addr_ In-place version ofaddr()"
    },
    {
        "Y": "Tensor.allclose Seetorch.allclose",
        "X": "What is company that owns the Seetorch?",
        "Z": "Tensor.allclose Seetorch.allclose()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.allclose?",
        "Z": "Tensor.allclose Seetorch.allclose()"
    },
    {
        "Y": "Tensor.amax Seetorch.amax",
        "X": "What is Tensor.amax Seetorch.amax?",
        "Z": "Tensor.amax Seetorch.amax()"
    },
    {
        "Y": "Tensor.amin",
        "X": "What is Seetorch.amin?",
        "Z": "Tensor.amin Seetorch.amin()"
    },
    {
        "Y": "Tensor.amin Seetorch.amin",
        "X": "What is Tensor.amin Seetorch.amin?",
        "Z": "Tensor.amin Seetorch.amin()"
    },
    {
        "Y": "Tensor.angle Seetorch.angle",
        "X": "What is Tensor.angle Seetorch.angle?",
        "Z": "Tensor.angle Seetorch.angle()"
    },
    {
        "Y": "Tensor.angle Seetorch.angle",
        "X": "What is a Tensor.angle?",
        "Z": "Tensor.angle Seetorch.angle()"
    },
    {
        "Y": "functioncallable",
        "X": "What does Tensor.apply_ apply to each element in the tensor?",
        "Z": "Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable."
    },
    {
        "Y": "Tensor.apply_",
        "X": "What applies the functioncallableto each element in the tensor?",
        "Z": "Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable."
    },
    {
        "Y": "Tensor.argmax Seetorch.argmax",
        "X": "What is Tensor.argmax Seetorch.argmax?",
        "Z": "Tensor.argmax Seetorch.argmax()"
    },
    {
        "Y": "Tensor.argmax",
        "X": "What is Seetorch.argmax() function?",
        "Z": "Tensor.argmax Seetorch.argmax()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.argmin?",
        "Z": "Tensor.argmin Seetorch.argmin()"
    },
    {
        "Y": "Tensor.argmin",
        "X": "What is Seetorch.argmin?",
        "Z": "Tensor.argmin Seetorch.argmin()"
    },
    {
        "Y": "Tensor.argsort Seetorch.argsort()",
        "X": "What does Tensor.argsort Seetorch.argsort() do?",
        "Z": "Tensor.argsort Seetorch.argsort()"
    },
    {
        "Y": "Tensor.asin",
        "X": "What is Seetorch.asin?",
        "Z": "Tensor.asin Seetorch.asin()"
    },
    {
        "Y": "Tensor.asin",
        "X": "What is Seetorch.asin?",
        "Z": "Tensor.asin Seetorch.asin()"
    },
    {
        "Y": "Tensor.asin",
        "X": "What is In-place version ofasin()?",
        "Z": "Tensor.asin_ In-place version ofasin()"
    },
    {
        "Y": "Tensor.arcsin",
        "X": "What does Seetorch.arcsin call?",
        "Z": "Tensor.arcsin Seetorch.arcsin()"
    },
    {
        "Y": "Seetorch.arcsin",
        "X": "What is Tensor.arcsin?",
        "Z": "Tensor.arcsin Seetorch.arcsin()"
    },
    {
        "Y": "Tensor.arcsin",
        "X": "What is Seetorch.arcsin?",
        "Z": "Tensor.arcsin Seetorch.arcsin()"
    },
    {
        "Y": "Tensor.arcsin_ In-place version ofarcsin()",
        "X": "What is in-place version of arcsin()?",
        "Z": "Tensor.arcsin_ In-place version ofarcsin()"
    },
    {
        "Y": "Tensor.arcsin",
        "X": "What is In-place version ofarcsin()?",
        "Z": "Tensor.arcsin_ In-place version ofarcsin()"
    },
    {
        "Y": "Tensor.as_strided Seetorch.as_strided()",
        "X": "What does Tensor.as_strided Seetorch.as_strided() do?",
        "Z": "Tensor.as_strided Seetorch.as_strided()"
    },
    {
        "Y": "Seetorch",
        "X": "What is a Tensor.as_strided?",
        "Z": "Tensor.as_strided Seetorch.as_strided()"
    },
    {
        "Y": "Tensor",
        "X": "What is Seetorch?",
        "Z": "Tensor.take_along_dim Seetorch.take_along_dim()"
    },
    {
        "Y": "Tensor.atan Seetorch.atan",
        "X": "What is Tensor.atan Seetorch.atan?",
        "Z": "Tensor.atan Seetorch.atan()"
    },
    {
        "Y": "Tensor.atan Seetorch.atan",
        "X": "What is Tensor.atan?",
        "Z": "Tensor.atan Seetorch.atan()"
    },
    {
        "Y": "Tensor.atan_ In-place version ofatan()",
        "X": "What is in-place version of atan?",
        "Z": "Tensor.atan_ In-place version ofatan()"
    },
    {
        "Y": "Tensor.atan",
        "X": "What is In-place version of atan?",
        "Z": "Tensor.atan_ In-place version ofatan()"
    },
    {
        "Y": "Tensor.arctan",
        "X": "What is Seetorch.arctan?",
        "Z": "Tensor.arctan Seetorch.arctan()"
    },
    {
        "Y": "Tensor.arctan_ In-place version ofarctan()",
        "X": "What is in-place version of arctan?",
        "Z": "Tensor.arctan_ In-place version ofarctan()"
    },
    {
        "Y": "Tensor.arctan",
        "X": "What is In-place version ofarctan?",
        "Z": "Tensor.arctan_ In-place version ofarctan()"
    },
    {
        "Y": "Tensor.atan2 Seetorch.atan2()",
        "X": "What does Tensor.atan2 Seetorch.atan2() do?",
        "Z": "Tensor.atan2 Seetorch.atan2()"
    },
    {
        "Y": "Tensor.atan2 Seetorch.atan2()",
        "X": "What is Tensor.atan2 Seetorch.atan2()?",
        "Z": "Tensor.atan2 Seetorch.atan2()"
    },
    {
        "Y": "Tensor.atan2_ In-place version ofatan2()",
        "X": "What is in-place version of atan2()?",
        "Z": "Tensor.atan2_ In-place version ofatan2()"
    },
    {
        "Y": "Tensor.atan2_ In-place",
        "X": "What is version of atan2()?",
        "Z": "Tensor.atan2_ In-place version ofatan2()"
    },
    {
        "Y": "Tensor.all Seetorch.all",
        "X": "What is Tensor.all Seetorch?",
        "Z": "Tensor.all Seetorch.all()"
    },
    {
        "Y": "Tensor.any Seetorch.any",
        "X": "What is a Tensor?",
        "Z": "Tensor.any Seetorch.any()"
    },
    {
        "Y": "Tensor",
        "X": "What is a Seetorch?",
        "Z": "Tensor.any Seetorch.any()"
    },
    {
        "Y": "Tensor",
        "X": "What computes the gradient of current tensor w.r.t?",
        "Z": "Tensor.backward Computes the gradient of current tensor w.r.t."
    },
    {
        "Y": "Tensor.backward",
        "X": "What computes the gradient of current tensor w.r.t.?",
        "Z": "Tensor.backward Computes the gradient of current tensor w.r.t."
    },
    {
        "Y": "Tensor",
        "X": "What is a baddbmm Seetorch?",
        "Z": "Tensor.baddbmm Seetorch.baddbmm()"
    },
    {
        "Y": "Tensor.baddbmm_ In-place version ofbaddbmm()",
        "X": "What is In-place version ofbaddbmm()?",
        "Z": "Tensor.baddbmm_ In-place version ofbaddbmm()"
    },
    {
        "Y": "Tensor.baddbmm_ In-place",
        "X": "What is version ofbaddbmm()?",
        "Z": "Tensor.baddbmm_ In-place version ofbaddbmm()"
    },
    {
        "Y": "independently sampled",
        "X": "Eachresult[i]textttresult[i]result[i]is what?",
        "Z": "Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i])."
    },
    {
        "Y": "Tensor.bernoulli",
        "X": "What fills each location ofselfwith an independent sample fromBernoulli(p)textBernoulli(",
        "Z": "Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p)."
    },
    {
        "Y": "self.bfloat16",
        "X": "What is equivalent to self.to(torch.bfloat16)?",
        "Z": "Tensor.bfloat16 self.bfloat16()is equivalent toself.to(torch.bfloat16)."
    },
    {
        "Y": "self.to(torch.bfloat16)",
        "X": "What is self.bfloat16 equivalent to?",
        "Z": "Tensor.bfloat16 self.bfloat16()is equivalent toself.to(torch.bfloat16)."
    },
    {
        "Y": "Seetorch.bincount()",
        "X": "What is Tensor.bincount?",
        "Z": "Tensor.bincount Seetorch.bincount()"
    },
    {
        "Y": "Tensor.bincount",
        "X": "What is Seetorch.bincount function?",
        "Z": "Tensor.bincount Seetorch.bincount()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that makes a bitwise_not Seetorch.bitwise_not()?",
        "Z": "Tensor.bitwise_not Seetorch.bitwise_not()"
    },
    {
        "Y": "Tensor",
        "X": "What is in-place version ofbitwise_not?",
        "Z": "Tensor.bitwise_not_ In-place version ofbitwise_not()"
    },
    {
        "Y": "Tensor.bitwise_not",
        "X": "What is In-place version ofbitwise_not()?",
        "Z": "Tensor.bitwise_not_ In-place version ofbitwise_not()"
    },
    {
        "Y": "Tensor",
        "X": "What is a bitwise and Seetorch?",
        "Z": "Tensor.bitwise_and Seetorch.bitwise_and()"
    },
    {
        "Y": "Tensor",
        "X": "What is the.bitwise_and_ In-place version ofbitwise_and?",
        "Z": "Tensor.bitwise_and_ In-place version ofbitwise_and()"
    },
    {
        "Y": "Tensor.bitwise_and_ In-place",
        "X": "What version of bitwise_and() is used?",
        "Z": "Tensor.bitwise_and_ In-place version ofbitwise_and()"
    },
    {
        "Y": "Tensor.diagonal Seetorch.diagonal()",
        "X": "What is function?",
        "Z": "Tensor.diagonal Seetorch.diagonal()"
    },
    {
        "Y": "Tensor.bitwise_or",
        "X": "What is Seetorch.bitwise_or?",
        "Z": "Tensor.bitwise_or Seetorch.bitwise_or()"
    },
    {
        "Y": "Tensor.bitwise_or_ In-place version ofbitwise_or()",
        "X": "What is In-place version ofbitwise_or?",
        "Z": "Tensor.bitwise_or_ In-place version ofbitwise_or()"
    },
    {
        "Y": "Tensor.bitwise_or",
        "X": "What is the In-place version ofbitwise_or?",
        "Z": "Tensor.bitwise_or_ In-place version ofbitwise_or()"
    },
    {
        "Y": "Tensor.bitwise_xor",
        "X": "What is function that determines a bitwise xor?",
        "Z": "Tensor.bitwise_xor Seetorch.bitwise_xor()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.bitwise_xor?",
        "Z": "Tensor.bitwise_xor Seetorch.bitwise_xor()"
    },
    {
        "Y": "Tensor.bitwise_xor",
        "X": "What is In-place version ofbitwise_xor?",
        "Z": "Tensor.bitwise_xor_ In-place version ofbitwise_xor()"
    },
    {
        "Y": "Tensor.bitwise_xor",
        "X": "What is the In-place version ofbitwise_xor?",
        "Z": "Tensor.bitwise_xor_ In-place version ofbitwise_xor()"
    },
    {
        "Y": "self.bool()",
        "X": "What is equivalent toself.to(torch.bool)?",
        "Z": "Tensor.bool self.bool()is equivalent toself.to(torch.bool)."
    },
    {
        "Y": "self.to(torch.bool)",
        "X": "What is the equivalent of self.bool()?",
        "Z": "Tensor.bool self.bool()is equivalent toself.to(torch.bool)."
    },
    {
        "Y": "self.to(torch.uint8)",
        "X": "What is tensor.byte self.byte() equivalent to?",
        "Z": "Tensor.byte self.byte()is equivalent toself.to(torch.uint8)."
    },
    {
        "Y": "Tensor",
        "X": "What is program that broadcasts to Seetorch?",
        "Z": "Tensor.broadcast_to Seetorch.broadcast_to()."
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.broadcast_to?",
        "Z": "Tensor.broadcast_to Seetorch.broadcast_to()."
    },
    {
        "Y": "Tensor.broadcast_to",
        "X": "What does Seetorch.broadcast_to() do?",
        "Z": "Tensor.broadcast_to Seetorch.broadcast_to()."
    },
    {
        "Y": "Cauchy",
        "X": "What distribution does Tensor.cauchy come from?",
        "Z": "Tensor.cauchy_ Fills the tensor with numbers drawn from the Cauchy distribution:"
    },
    {
        "Y": "Cauchy",
        "X": "Tensor.cauchy_ Fills the tensor with numbers drawn from what distribution?",
        "Z": "Tensor.cauchy_ Fills the tensor with numbers drawn from the Cauchy distribution:"
    },
    {
        "Y": "Tensor.ceil Seetorch.ceil",
        "X": "What is a tent?",
        "Z": "Tensor.ceil Seetorch.ceil()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.ceil?",
        "Z": "Tensor.ceil Seetorch.ceil()"
    },
    {
        "Y": "Tensor.t_ In-place version oft()",
        "X": "What is in-place version of the in-place version of the in-place version of the in-place version of the in",
        "Z": "Tensor.t_ In-place version oft()"
    },
    {
        "Y": "Tensor.ceil_ In-place",
        "X": "What is version ofceil()?",
        "Z": "Tensor.ceil_ In-place version ofceil()"
    },
    {
        "Y": "self.char()",
        "X": "What is equivalent to self.to(torch.int8)?",
        "Z": "Tensor.char self.char()is equivalent toself.to(torch.int8)."
    },
    {
        "Y": "self.to(torch.int8)",
        "X": "What is tensor.char self.char() equivalent to?",
        "Z": "Tensor.char self.char()is equivalent toself.to(torch.int8)."
    },
    {
        "Y": "Tensor.cholesky Seetorch.cholesky",
        "X": "What is cholesky?",
        "Z": "Tensor.cholesky Seetorch.cholesky()"
    },
    {
        "Y": "Tensor.cholesky",
        "X": "What is the name of Seetorch.cholesky?",
        "Z": "Tensor.cholesky Seetorch.cholesky()"
    },
    {
        "Y": "Tensor.cholesky_inverse",
        "X": "What is inverse of the Seetorch.cholesky_inverse?",
        "Z": "Tensor.cholesky_inverse Seetorch.cholesky_inverse()"
    },
    {
        "Y": "Tensor.cholesky_inverse",
        "X": "What is Seetorch.cholesky_inverse?",
        "Z": "Tensor.cholesky_inverse Seetorch.cholesky_inverse()"
    },
    {
        "Y": "Seetorch.cholesky_solve",
        "X": "What does Tensor.cholesky_solve?",
        "Z": "Tensor.cholesky_solve Seetorch.cholesky_solve()"
    },
    {
        "Y": "Tensor.cholesky_solve",
        "X": "What does Seetorch.cholesky_solve do?",
        "Z": "Tensor.cholesky_solve Seetorch.cholesky_solve()"
    },
    {
        "Y": "Tensor.cholesky_solve",
        "X": "What does Seetorch.cholesky_solve() do?",
        "Z": "Tensor.cholesky_solve Seetorch.cholesky_solve()"
    },
    {
        "Y": "Tensor",
        "X": "What is.chunk Seetorch.chunk()?",
        "Z": "Tensor.chunk Seetorch.chunk()"
    },
    {
        "Y": "Tensor.clamp Seetorch.clamp",
        "X": "What is a clam?",
        "Z": "Tensor.clamp Seetorch.clamp()"
    },
    {
        "Y": "Seetorch.clamp",
        "X": "What is Tensor.clamp?",
        "Z": "Tensor.clamp Seetorch.clamp()"
    },
    {
        "Y": "Tensor.clamp",
        "X": "What is Seetorch.clamp() function?",
        "Z": "Tensor.clamp Seetorch.clamp()"
    },
    {
        "Y": "Tensor.clamp_ In-place version ofclamp()",
        "X": "What does Tensor.clamp_ In-place version ofclamp() do?",
        "Z": "Tensor.clamp_ In-place version ofclamp()"
    },
    {
        "Y": "Tensor.clamp_ In-place",
        "X": "What version ofclamp() does Tensor.clamp_ In-place?",
        "Z": "Tensor.clamp_ In-place version ofclamp()"
    },
    {
        "Y": "Tensor.clip",
        "X": "What is Alias forclamp()?",
        "Z": "Tensor.clip Alias forclamp()."
    },
    {
        "Y": "Tensor.clip_ Alias forclamp_()",
        "X": "What is Alias forclamp?",
        "Z": "Tensor.clip_ Alias forclamp_()."
    },
    {
        "Y": "Tensor.clip_ Alias",
        "X": "What is forclamp_()?",
        "Z": "Tensor.clip_ Alias forclamp_()."
    },
    {
        "Y": "Tensor.clone Seetorch.clone",
        "X": "What is clone?",
        "Z": "Tensor.clone Seetorch.clone()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.clone?",
        "Z": "Tensor.clone Seetorch.clone()"
    },
    {
        "Y": "Tensor",
        "X": "What is clone Seetorch.clone()?",
        "Z": "Tensor.clone Seetorch.clone()"
    },
    {
        "Y": "a contiguous in memory tensor",
        "X": "What does Tensor.contiguous return?",
        "Z": "Tensor.contiguous Returns a contiguous in memory tensor containing the same data asselftensor."
    },
    {
        "Y": "selftensor",
        "X": "Tensor.contiguous Returns a contiguous in memory tensor containing the same data as what?",
        "Z": "Tensor.contiguous Returns a contiguous in memory tensor containing the same data asselftensor."
    },
    {
        "Y": "Tensor.contiguous",
        "X": "What returns a contiguous in memory tensor containing the same data asselftensor?",
        "Z": "Tensor.contiguous Returns a contiguous in memory tensor containing the same data asselftensor."
    },
    {
        "Y": "Tensor",
        "X": "Which element copies the elements fromsrcintoselftensor and returnsself?",
        "Z": "Tensor.copy_ Copies the elements fromsrcintoselftensor and returnsself."
    },
    {
        "Y": "Tensor.copy",
        "X": "What _ Copies the elements fromsrcintoselftensor and returnsself?",
        "Z": "Tensor.copy_ Copies the elements fromsrcintoselftensor and returnsself."
    },
    {
        "Y": "Tensor.conj Seetorch.conj()",
        "X": "What does Tensor.conj Seetorch.conj do?",
        "Z": "Tensor.conj Seetorch.conj()"
    },
    {
        "Y": "Tensor.conj",
        "X": "What is Seetorch.conj?",
        "Z": "Tensor.conj Seetorch.conj()"
    },
    {
        "Y": "Seetorch.conj",
        "X": "What is Tensor.conj?",
        "Z": "Tensor.conj Seetorch.conj()"
    },
    {
        "Y": "Tensor",
        "X": "What is.copysign Seetorch.copysign() function?",
        "Z": "Tensor.copysign Seetorch.copysign()"
    },
    {
        "Y": "Tensor.copysign",
        "X": "What is Seetorch.copysign function?",
        "Z": "Tensor.copysign Seetorch.copysign()"
    },
    {
        "Y": "Tensor",
        "X": "What is program that does the copysign function?",
        "Z": "Tensor.copysign_ In-place version ofcopysign()"
    },
    {
        "Y": "Tensor.copysign_ In-place",
        "X": "What is version ofcopysign()?",
        "Z": "Tensor.copysign_ In-place version ofcopysign()"
    },
    {
        "Y": "Tensor.cos",
        "X": "What is Seetorch.cos?",
        "Z": "Tensor.cos Seetorch.cos()"
    },
    {
        "Y": "Seetorch.cos",
        "X": "What is Tensor.cos?",
        "Z": "Tensor.cos Seetorch.cos()"
    },
    {
        "Y": "Tensor.cos",
        "X": "What is the name of Seetorch.cos?",
        "Z": "Tensor.cos Seetorch.cos()"
    },
    {
        "Y": "Tensor.cos_ In-place version ofcos()",
        "X": "What does Tensor.cos do?",
        "Z": "Tensor.cos_ In-place version ofcos()"
    },
    {
        "Y": "Tensor.cos",
        "X": "What is in-place version of ofcos?",
        "Z": "Tensor.cos_ In-place version ofcos()"
    },
    {
        "Y": "Tensor.symeig Seetorch.symeig",
        "X": "What is website?",
        "Z": "Tensor.symeig Seetorch.symeig()"
    },
    {
        "Y": "Seetorch.cosh",
        "X": "What is Tensor.cosh?",
        "Z": "Tensor.cosh Seetorch.cosh()"
    },
    {
        "Y": "Tensor.cosh",
        "X": "What is Seetorch.cosh function?",
        "Z": "Tensor.cosh Seetorch.cosh()"
    },
    {
        "Y": "Tensor.cosh_ In-place version ofcosh()",
        "X": "What does Tensor.cosh do?",
        "Z": "Tensor.cosh_ In-place version ofcosh()"
    },
    {
        "Y": "Tensor.cosh",
        "X": "What is In-place version ofcosh()?",
        "Z": "Tensor.cosh_ In-place version ofcosh()"
    },
    {
        "Y": "Tensor.count_nonzero",
        "X": "What does Seetorch.count_nonzero do?",
        "Z": "Tensor.count_nonzero Seetorch.count_nonzero()"
    },
    {
        "Y": "Tensor.acosh Seetorch.acosh",
        "X": "What is Tensor.acosh Seetorch.acosh?",
        "Z": "Tensor.acosh Seetorch.acosh()"
    },
    {
        "Y": "Tensor.acosh",
        "X": "What is Seetorch.acosh function?",
        "Z": "Tensor.acosh Seetorch.acosh()"
    },
    {
        "Y": "Tensor.acosh_ In-place version ofacosh()",
        "X": "What is in-place version of acosh?",
        "Z": "Tensor.acosh_ In-place version ofacosh()"
    },
    {
        "Y": "Tensor.acosh",
        "X": "What is In-place version ofacosh()?",
        "Z": "Tensor.acosh_ In-place version ofacosh()"
    },
    {
        "Y": "Tensor",
        "X": "What does arccosh acosh() stand for?",
        "Z": "Tensor.arccosh acosh() -> Tensor"
    },
    {
        "Y": "Tensor",
        "X": "What does arccosh acosh() -> Tensor?",
        "Z": "Tensor.arccosh acosh() -> Tensor"
    },
    {
        "Y": "acosh",
        "X": "What is Tensor.arccosh?",
        "Z": "Tensor.arccosh acosh() -> Tensor"
    },
    {
        "Y": "Tensor",
        "X": "What does arccosh_ acosh_() stand for?",
        "Z": "Tensor.arccosh_ acosh_() -> Tensor"
    },
    {
        "Y": "acosh_()",
        "X": "What is the name of Tensor.arccosh?",
        "Z": "Tensor.arccosh_ acosh_() -> Tensor"
    },
    {
        "Y": "Tensor",
        "X": "What does arccosh_ acosh_() -> Tensor?",
        "Z": "Tensor.arccosh_ acosh_() -> Tensor"
    },
    {
        "Y": "acosh",
        "X": "What does Tensor.arccosh_?",
        "Z": "Tensor.arccosh_ acosh_() -> Tensor"
    },
    {
        "Y": "Tensor.cpu",
        "X": "What returns a copy of this object in CPU memory?",
        "Z": "Tensor.cpu Returns a copy of this object in CPU memory."
    },
    {
        "Y": "CPU",
        "X": "What type of memory does Tensor.cpu return a copy of?",
        "Z": "Tensor.cpu Returns a copy of this object in CPU memory."
    },
    {
        "Y": "Tensor.cross Seetorch.cross",
        "X": "What is Tensor.cross Seetorch.cross?",
        "Z": "Tensor.cross Seetorch.cross()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.cross?",
        "Z": "Tensor.cross Seetorch.cross()"
    },
    {
        "Y": "Tensor.cross",
        "X": "What is Seetorch.cross function?",
        "Z": "Tensor.cross Seetorch.cross()"
    },
    {
        "Y": "Tensor.cuda",
        "X": "What returns a copy of this object in CUDA memory?",
        "Z": "Tensor.cuda Returns a copy of this object in CUDA memory."
    },
    {
        "Y": "CUDA memory",
        "X": "Tensor.cuda returns a copy of this object in what?",
        "Z": "Tensor.cuda Returns a copy of this object in CUDA memory."
    },
    {
        "Y": "Seetorch.logcumsumexp",
        "X": "What is Tensor.logcumsumexp?",
        "Z": "Tensor.logcumsumexp Seetorch.logcumsumexp()"
    },
    {
        "Y": "Tensor.logcumsumexp",
        "X": "What does Seetorch.logcumsumexp use?",
        "Z": "Tensor.logcumsumexp Seetorch.logcumsumexp()"
    },
    {
        "Y": "Tensor.cummax Seetorch.cummax",
        "X": "What is Tensor.cummax Seetorch.cummax?",
        "Z": "Tensor.cummax Seetorch.cummax()"
    },
    {
        "Y": "Seetorch.cummax",
        "X": "What is Tensor.cummax?",
        "Z": "Tensor.cummax Seetorch.cummax()"
    },
    {
        "Y": "Tensor.cummax",
        "X": "What is Seetorch.cummax() function?",
        "Z": "Tensor.cummax Seetorch.cummax()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.cummin?",
        "Z": "Tensor.cummin Seetorch.cummin()"
    },
    {
        "Y": "Tensor.cummin",
        "X": "What is Seetorch.cummin?",
        "Z": "Tensor.cummin Seetorch.cummin()"
    },
    {
        "Y": "Tensor.cumprod",
        "X": "What is Seetorch.cumprod?",
        "Z": "Tensor.cumprod Seetorch.cumprod()"
    },
    {
        "Y": "Tensor.cumprod_ In-place version ofcumprod()",
        "X": "What is in-place version ofcumprod()?",
        "Z": "Tensor.cumprod_ In-place version ofcumprod()"
    },
    {
        "Y": "Tensor.cumprod_ In-place",
        "X": "What is version ofcumprod?",
        "Z": "Tensor.cumprod_ In-place version ofcumprod()"
    },
    {
        "Y": "Tensor.cumsum Seetorch.cumsum",
        "X": "What is Tensor.cumsum Seetorch.cumsum?",
        "Z": "Tensor.cumsum Seetorch.cumsum()"
    },
    {
        "Y": "Tensor.cumsum",
        "X": "What is Seetorch.cumsum?",
        "Z": "Tensor.cumsum Seetorch.cumsum()"
    },
    {
        "Y": "Tensor.cumsum_ In-place version ofcumsum()",
        "X": "What is In-place version ofcumsum()?",
        "Z": "Tensor.cumsum_ In-place version ofcumsum()"
    },
    {
        "Y": "Tensor.cumsum_ In-place",
        "X": "What is version ofcumsum()?",
        "Z": "Tensor.cumsum_ In-place version ofcumsum()"
    },
    {
        "Y": "Tensor.data_ptr",
        "X": "What returns the address of the first element of selftensor?",
        "Z": "Tensor.data_ptr Returns the address of the first element ofselftensor."
    },
    {
        "Y": "Tensor.deg2rad Seetorch.deg2rad()",
        "X": "What does Tensor.deg2rad do?",
        "Z": "Tensor.deg2rad Seetorch.deg2rad()"
    },
    {
        "Y": "Seetorch.deg2rad",
        "X": "What is Tensor.deg2rad?",
        "Z": "Tensor.deg2rad Seetorch.deg2rad()"
    },
    {
        "Y": "Tensor.deg2rad",
        "X": "What is Seetorch.deg2rad() function?",
        "Z": "Tensor.deg2rad Seetorch.deg2rad()"
    },
    {
        "Y": "dequantize it and return the dequantized float Tensor",
        "X": "What can you do with a quantized Tensor?",
        "Z": "Tensor.dequantize Given a quantized Tensor, dequantize it and return the dequantized float Tensor."
    },
    {
        "Y": "dequantize it",
        "X": "What do you do with a quantized Tensor?",
        "Z": "Tensor.dequantize Given a quantized Tensor, dequantize it and return the dequantized float Tensor."
    },
    {
        "Y": "Seetorch.det",
        "X": "What is Tensor.det?",
        "Z": "Tensor.det Seetorch.det()"
    },
    {
        "Y": "Tensor.det",
        "X": "What is Seetorch.det?",
        "Z": "Tensor.det Seetorch.det()"
    },
    {
        "Y": "Tensor.dense_dim",
        "X": "What returns the number of dense dimensions in asparse tensorself?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in asparse tensorself."
    },
    {
        "Y": "Tensor",
        "X": "What returns a new Tensor, detached from the current graph?",
        "Z": "torch.Tensor.detach Returns a new Tensor, detached from the current graph."
    },
    {
        "Y": "detached",
        "X": "Tensor.detach Returns a new Tensor, what from the current graph?",
        "Z": "Tensor.detach Returns a new Tensor, detached from the current graph."
    },
    {
        "Y": "leaf",
        "X": "Detaches the Tensor from the graph that created it, making it what?",
        "Z": "Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf."
    },
    {
        "Y": "torch",
        "X": "What detaches the Tensor from the graph that created it?",
        "Z": "torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf."
    },
    {
        "Y": "Tensor",
        "X": "What is component in the Seetorch?",
        "Z": "Tensor.greater_equal Seetorch.greater_equal()."
    },
    {
        "Y": "Tensor.diag Seetorch.diag",
        "X": "What is Tensor.diag Seetorch.diag?",
        "Z": "Tensor.diag Seetorch.diag()"
    },
    {
        "Y": "Tensor.diag_embed Seetorch.diag_embed()",
        "X": "What does Tensor.diag_embed Seetorch.diag_embed() do?",
        "Z": "Tensor.diag_embed Seetorch.diag_embed()"
    },
    {
        "Y": "Tensor.diagflat Seetorch.diagflat()",
        "X": "What does Tensor.diagflat Seetorch.diagflat() do?",
        "Z": "Tensor.diagflat Seetorch.diagflat()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.diagflat?",
        "Z": "Tensor.diagflat Seetorch.diagflat()"
    },
    {
        "Y": "Tensor.diagflat",
        "X": "What is Seetorch.diagflat() function?",
        "Z": "Tensor.diagflat Seetorch.diagflat()"
    },
    {
        "Y": "Seetorch",
        "X": "What is a Tensor.diagonal?",
        "Z": "Tensor.diagonal Seetorch.diagonal()"
    },
    {
        "Y": "at least 2-dimensions",
        "X": "How many dimensions does a tensor have?",
        "Z": "Tensor.fill_diagonal_ Fill the main diagonal of a tensor that has at least 2-dimensions."
    },
    {
        "Y": "2-dimensions",
        "X": "What is the minimum dimension of a tensor?",
        "Z": "Tensor.fill_diagonal_ Fill the main diagonal of a tensor that has at least 2-dimensions."
    },
    {
        "Y": "Tensor.fmax Seetorch.fmax()",
        "X": "What is Tensor.fmax Seetorch.fmax()?",
        "Z": "Tensor.fmax Seetorch.fmax()"
    },
    {
        "Y": "Tensor.fmin",
        "X": "What is Seetorch.fmin?",
        "Z": "Tensor.fmin Seetorch.fmin()"
    },
    {
        "Y": "Tensor.diff Seetorch.diff()",
        "X": "What does Tensor.diff Seetorch.diff do?",
        "Z": "Tensor.diff Seetorch.diff()"
    },
    {
        "Y": "Tensor.diff",
        "X": "What is Seetorch.diff() function?",
        "Z": "Tensor.diff Seetorch.diff()"
    },
    {
        "Y": "Seetorch.digamma",
        "X": "What is Tensor.digamma?",
        "Z": "Tensor.digamma Seetorch.digamma()"
    },
    {
        "Y": "Tensor",
        "X": "What is.digamma Seetorch?",
        "Z": "Tensor.digamma Seetorch.digamma()"
    },
    {
        "Y": "Tensor.digamma_ In-place version ofdigamma()",
        "X": "What is In-place version ofdigamma?",
        "Z": "Tensor.digamma_ In-place version ofdigamma()"
    },
    {
        "Y": "Tensor.digamma",
        "X": "What is In-place version ofdigamma()?",
        "Z": "Tensor.digamma_ In-place version ofdigamma()"
    },
    {
        "Y": "Tensor",
        "X": "What returns the number of dimensions of selftensor?",
        "Z": "Tensor.dim Returns the number of dimensions ofselftensor."
    },
    {
        "Y": "selftensor",
        "X": "Tensor.dim returns the number of dimensions of what?",
        "Z": "Tensor.dim Returns the number of dimensions ofselftensor."
    },
    {
        "Y": "Tensor.dist Seetorch.dist()",
        "X": "What does Tensor.dist Seetorch.dist do?",
        "Z": "Tensor.dist Seetorch.dist()"
    },
    {
        "Y": "Tensor.dist",
        "X": "What is Seetorch.dist?",
        "Z": "Tensor.dist Seetorch.dist()"
    },
    {
        "Y": "Tensor",
        "X": "What is.div Seetorch.div?",
        "Z": "Tensor.div Seetorch.div()"
    },
    {
        "Y": "Tensor.div Seetorch.div()",
        "X": "What does Tensor.div Seetorch.div() do?",
        "Z": "Tensor.div Seetorch.div()"
    },
    {
        "Y": "Tensor.div_ In-place version ofdiv()",
        "X": "What is div function?",
        "Z": "Tensor.div_ In-place version ofdiv()"
    },
    {
        "Y": "Tensor.div_ In-place",
        "X": "What is version ofdiv()?",
        "Z": "Tensor.div_ In-place version ofdiv()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that creates Seetorch.divide()?",
        "Z": "Tensor.divide Seetorch.divide()"
    },
    {
        "Y": "Tensor.divide",
        "X": "What does Seetorch.divide() do?",
        "Z": "Tensor.divide Seetorch.divide()"
    },
    {
        "Y": "Tensor.divide_ In-place version ofdivide()",
        "X": "What is in-place version ofdivide()?",
        "Z": "Tensor.divide_ In-place version ofdivide()"
    },
    {
        "Y": "Tensor.divide_ In-place",
        "X": "What is version ofdivide()?",
        "Z": "Tensor.divide_ In-place version ofdivide()"
    },
    {
        "Y": "Tensor.dot Seetorch.dot()",
        "X": "What does Tensor.dot Seetorch.dot() do?",
        "Z": "Tensor.dot Seetorch.dot()"
    },
    {
        "Y": "Seetorch.dot",
        "X": "What is Tensor.dot?",
        "Z": "Tensor.dot Seetorch.dot()"
    },
    {
        "Y": "Tensor.dot",
        "X": "What is Seetorch.dot() function?",
        "Z": "Tensor.dot Seetorch.dot()"
    },
    {
        "Y": "self.to(torch.float64)",
        "X": "What is tensor.double self.double() equivalent to?",
        "Z": "Tensor.double self.double()is equivalent toself.to(torch.float64)."
    },
    {
        "Y": "Seetorch.dsplit",
        "X": "What is Tensor.dsplit?",
        "Z": "Tensor.dsplit Seetorch.dsplit()"
    },
    {
        "Y": "Tensor.dsplit",
        "X": "What is Seetorch.dsplit() function?",
        "Z": "Tensor.dsplit Seetorch.dsplit()"
    },
    {
        "Y": "Tensor.eig Seetorch.eig()",
        "X": "What is file that is used to create a Seetorch?",
        "Z": "Tensor.eig Seetorch.eig()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.eig?",
        "Z": "Tensor.eig Seetorch.eig()"
    },
    {
        "Y": "Tensor.eig",
        "X": "What is Seetorch.eig?",
        "Z": "Tensor.eig Seetorch.eig()"
    },
    {
        "Y": "Tensor.element_size",
        "X": "What returns the size in bytes of an individual element?",
        "Z": "Tensor.element_size Returns the size in bytes of an individual element."
    },
    {
        "Y": "bytes",
        "X": "Tensor.element_size Returns the size in what?",
        "Z": "Tensor.element_size Returns the size in bytes of an individual element."
    },
    {
        "Y": "Tensor.eq Seetorch.eq()",
        "X": "What does Tensor.eq Seetorch.eq do?",
        "Z": "Tensor.eq Seetorch.eq()"
    },
    {
        "Y": "Tensor.eq",
        "X": "What is Seetorch.eq() function?",
        "Z": "Tensor.eq Seetorch.eq()"
    },
    {
        "Y": "Tensor.eq_ In-place version ofeq()",
        "X": "What does Tensor.eq_ In-place version ofeq() do?",
        "Z": "Tensor.eq_ In-place version ofeq()"
    },
    {
        "Y": "Tensor.eq_ In-place",
        "X": "What is version ofeq()?",
        "Z": "Tensor.eq_ In-place version ofeq()"
    },
    {
        "Y": "Tensor.equal Seetorch.equal",
        "X": "What is Tensor.equal Seetorch.equal?",
        "Z": "Tensor.equal Seetorch.equal()"
    },
    {
        "Y": "Tensor.erf",
        "X": "What is Seetorch.erf?",
        "Z": "Tensor.erf Seetorch.erf()"
    },
    {
        "Y": "Tensor.erf_ In-place",
        "X": "What is version oferf()?",
        "Z": "Tensor.erf_ In-place version oferf()"
    },
    {
        "Y": "Tensor.erfc Seetorch.erfc()",
        "X": "What does Tensor.erfc do?",
        "Z": "Tensor.erfc Seetorch.erfc()"
    },
    {
        "Y": "Seetorch.erfc",
        "X": "What is Tensor.erfc?",
        "Z": "Tensor.erfc Seetorch.erfc()"
    },
    {
        "Y": "Tensor.erfc",
        "X": "What is Seetorch.erfc?",
        "Z": "Tensor.erfc Seetorch.erfc()"
    },
    {
        "Y": "Tensor.erfc_ In-place version oferfc()",
        "X": "What is function that is used by the Tensor.erfc?",
        "Z": "Tensor.erfc_ In-place version oferfc()"
    },
    {
        "Y": "Tensor.erfc_ In-place",
        "X": "What is version of oferfc()?",
        "Z": "Tensor.erfc_ In-place version oferfc()"
    },
    {
        "Y": "Seetorch.erfinv",
        "X": "What is Tensor.erfinv?",
        "Z": "Tensor.erfinv Seetorch.erfinv()"
    },
    {
        "Y": "Tensor.erfinv",
        "X": "What is Seetorch.erfinv?",
        "Z": "Tensor.erfinv Seetorch.erfinv()"
    },
    {
        "Y": "Tensor.erfinv",
        "X": "What is In-place version of oferfinv()?",
        "Z": "Tensor.erfinv_ In-place version oferfinv()"
    },
    {
        "Y": "Tensor",
        "X": "What is.exp Seetorch.exp?",
        "Z": "Tensor.exp Seetorch.exp()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.exp?",
        "Z": "Tensor.exp Seetorch.exp()"
    },
    {
        "Y": "Tensor.exp",
        "X": "What is Seetorch.exp?",
        "Z": "Tensor.exp Seetorch.exp()"
    },
    {
        "Y": "Tensor.exp_ In-place version ofexp()",
        "X": "What does Tensor.exp_ In-place version ofexp() do?",
        "Z": "Tensor.exp_ In-place version ofexp()"
    },
    {
        "Y": "Tensor.exp_ In-place",
        "X": "What version ofexp() does Tensor.exp_ In-place?",
        "Z": "Tensor.exp_ In-place version ofexp()"
    },
    {
        "Y": "Tensor",
        "X": "What is component in the Seetorch.expm1 function?",
        "Z": "Tensor.expm1 Seetorch.expm1()"
    },
    {
        "Y": "Tensor.expm1",
        "X": "What is Seetorch.expm1?",
        "Z": "Tensor.expm1 Seetorch.expm1()"
    },
    {
        "Y": "Tensor.expm1_ In-place version ofexpm1()",
        "X": "What does Tensor.expm1_ In-place version ofexpm1() do?",
        "Z": "Tensor.expm1_ In-place version ofexpm1()"
    },
    {
        "Y": "Tensor.expm1_ In-place",
        "X": "What is version ofexpm1()?",
        "Z": "Tensor.expm1_ In-place version ofexpm1()"
    },
    {
        "Y": "singleton",
        "X": "What is the size of the selftensor?",
        "Z": "Tensor.expand Returns a new view of the self tensor with singleton dimensions expanded to a larger size."
    },
    {
        "Y": "Tensor.expand",
        "X": "What returns a new view of the self tensor with singleton dimensions expanded to a larger size?",
        "Z": "Tensor.expand Returns a new view of the self tensor with singleton dimensions expanded to a larger size."
    },
    {
        "Y": "Expand this tensor to the same size asother",
        "X": "What does Tensor.expand_as do?",
        "Z": "Tensor.expand_as Expand this tensor to the same size asother."
    },
    {
        "Y": "expand",
        "X": "What does the tensor do?",
        "Z": "Tensor.expand_as Expand this tensor to the same size asother."
    },
    {
        "Y": "Tensor.exponential",
        "X": "What fillsselftensor with elements drawn from the exponential distribution?",
        "Z": "Tensor.exponential_ Fillsselftensor with elements drawn from the exponential distribution:"
    },
    {
        "Y": "exponential",
        "X": "What type of tensor is filled with elements drawn from the exponential distribution?",
        "Z": "Tensor.exponential_ Fillsselftensor with elements drawn from the exponential distribution:"
    },
    {
        "Y": "exponential",
        "X": "What is the term for a tensor?",
        "Z": "Tensor.exponential_ Fillsselftensor with elements drawn from the exponential distribution:"
    },
    {
        "Y": "Tensor",
        "X": "What is program that fixes Seetorch.fix()?",
        "Z": "Tensor.fix Seetorch.fix()."
    },
    {
        "Y": "Tensor.fix Seetorch.fix()",
        "X": "What does Tensor.fix Seetorch.fix() do?",
        "Z": "Tensor.fix Seetorch.fix()."
    },
    {
        "Y": "Tensor.fix_ In-place version offix()",
        "X": "What does Tensor.fix_ In-place version offix() do?",
        "Z": "Tensor.fix_ In-place version offix()"
    },
    {
        "Y": "Tensor.fix_ In-place",
        "X": "What is version offix()?",
        "Z": "Tensor.fix_ In-place version offix()"
    },
    {
        "Y": "Tensor.fill_ Fillsselftensor",
        "X": "What does it do with the specified value?",
        "Z": "Tensor.fill_ Fillsselftensor with the specified value."
    },
    {
        "Y": "Tensor.fill",
        "X": "What does fillsselftensor with the specified value?",
        "Z": "Tensor.fill_ Fillsselftensor with the specified value."
    },
    {
        "Y": "seetorch.flatten",
        "X": "What does Tensor.flatten stand for?",
        "Z": "Tensor.flatten seetorch.flatten()"
    },
    {
        "Y": "Tensor",
        "X": "What is the term for a seetorch?",
        "Z": "Tensor.flatten seetorch.flatten()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.flip?",
        "Z": "Tensor.flip Seetorch.flip()"
    },
    {
        "Y": "Tensor.flip",
        "X": "What does Seetorch.flip() do?",
        "Z": "Tensor.flip Seetorch.flip()"
    },
    {
        "Y": "Seetorch.fliplr",
        "X": "What is Tensor.fliplr?",
        "Z": "Tensor.fliplr Seetorch.fliplr()"
    },
    {
        "Y": "Seetorch.flipud",
        "X": "What is Tensor.flipud?",
        "Z": "Tensor.flipud Seetorch.flipud()"
    },
    {
        "Y": "Tensor.flipud",
        "X": "What is Seetorch.flipud?",
        "Z": "Tensor.flipud Seetorch.flipud()"
    },
    {
        "Y": "self.float()",
        "X": "What is equivalent toself.to(torch.float32)?",
        "Z": "Tensor.float self.float()is equivalent toself.to(torch.float32)."
    },
    {
        "Y": "self.to(torch.float32)",
        "X": "What is the equivalent of Tensor.float self.float()?",
        "Z": "Tensor.float self.float()is equivalent toself.to(torch.float32)."
    },
    {
        "Y": "Tensor",
        "X": "What is float_power Seetorch.float_power()?",
        "Z": "Tensor.float_power Seetorch.float_power()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.float_power?",
        "Z": "Tensor.float_power Seetorch.float_power()"
    },
    {
        "Y": "Tensor.float_power Seetorch.float_power()",
        "X": "What does Tensor.float_power Seetorch.float_power() do?",
        "Z": "Tensor.float_power Seetorch.float_power()"
    },
    {
        "Y": "Tensor.float_power_ In-place version offloat_power()",
        "X": "What is Tensor.float_power_ In-place version offloat_power()?",
        "Z": "Tensor.float_power_ In-place version offloat_power()"
    },
    {
        "Y": "Tensor.float_power_ In-place",
        "X": "What version of float_power() is used?",
        "Z": "Tensor.float_power_ In-place version offloat_power()"
    },
    {
        "Y": "Tensor",
        "X": "What is.floor Seetorch.floor?",
        "Z": "Tensor.floor Seetorch.floor()"
    },
    {
        "Y": "Seetorch.floor",
        "X": "What is Tensor.floor?",
        "Z": "Tensor.floor Seetorch.floor()"
    },
    {
        "Y": "Tensor.floor",
        "X": "What is Seetorch.floor?",
        "Z": "Tensor.floor Seetorch.floor()"
    },
    {
        "Y": "Tensor.floor_ In-place version offloor()",
        "X": "What is floor()?",
        "Z": "Tensor.floor_ In-place version offloor()"
    },
    {
        "Y": "Tensor.floor_ In-place",
        "X": "What is version of floor?",
        "Z": "Tensor.floor_ In-place version offloor()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that creates the Seetorch.floor_divide?",
        "Z": "Tensor.floor_divide Seetorch.floor_divide()"
    },
    {
        "Y": "Tensor.floor_divide",
        "X": "What is Seetorch.floor_divide() function?",
        "Z": "Tensor.floor_divide Seetorch.floor_divide()"
    },
    {
        "Y": "Tensor",
        "X": "What is in-place version of floor_divide?",
        "Z": "Tensor.floor_divide_ In-place version offloor_divide()"
    },
    {
        "Y": "floor_divide_ In-place version",
        "X": "What is floor_divide?",
        "Z": "Tensor.floor_divide_ In-place version offloor_divide()"
    },
    {
        "Y": "floor_divide()",
        "X": "What is the in-place version of Tensor.floor_divide?",
        "Z": "Tensor.floor_divide_ In-place version offloor_divide()"
    },
    {
        "Y": "Tensor",
        "X": "Floor_divide_ In-place version offloor_divide()?",
        "Z": "Tensor.floor_divide_ In-place version offloor_divide()"
    },
    {
        "Y": "Tensor.fmod Seetorch.fmod()",
        "X": "What does Tensor.fmod Seetorch.fmod call?",
        "Z": "Tensor.fmod Seetorch.fmod()"
    },
    {
        "Y": "Seetorch.fmod",
        "X": "What is Tensor.fmod?",
        "Z": "Tensor.fmod Seetorch.fmod()"
    },
    {
        "Y": "Tensor.fmod",
        "X": "What is Seetorch.fmod?",
        "Z": "Tensor.fmod Seetorch.fmod()"
    },
    {
        "Y": "offmod()",
        "X": "What is Tensor.fmod_ In-place version?",
        "Z": "Tensor.fmod_ In-place version offmod()"
    },
    {
        "Y": "Tensor.fmod_ In-place version offmod()",
        "X": "What does Tensor.fmod_ In-place version offmod() do?",
        "Z": "Tensor.fmod_ In-place version offmod()"
    },
    {
        "Y": "offmod()",
        "X": "What is Tensor.fmod_ In-place version?",
        "Z": "Tensor.fmod_ In-place version offmod()"
    },
    {
        "Y": "Tensor.fmod_ In-place",
        "X": "What is version offmod()?",
        "Z": "Tensor.fmod_ In-place version offmod()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.frac?",
        "Z": "Tensor.frac Seetorch.frac()"
    },
    {
        "Y": "Tensor.frac",
        "X": "What is Seetorch.frac()?",
        "Z": "Tensor.frac Seetorch.frac()"
    },
    {
        "Y": "Tensor.frac_ In-place version offrac()",
        "X": "What is in-place version of frac()?",
        "Z": "Tensor.frac_ In-place version offrac()"
    },
    {
        "Y": "Tensor.frac_ In-place",
        "X": "What is version of frac()?",
        "Z": "Tensor.frac_ In-place version offrac()"
    },
    {
        "Y": "Tensor.frexp",
        "X": "What does Seetorch.frexp use?",
        "Z": "Tensor.frexp Seetorch.frexp()"
    },
    {
        "Y": "Tensor.frexp",
        "X": "What is Seetorch.frexp?",
        "Z": "Tensor.frexp Seetorch.frexp()"
    },
    {
        "Y": "Tensor.gather Seetorch.gather",
        "X": "What is Tensor.gather Seetorch.gather?",
        "Z": "Tensor.gather Seetorch.gather()"
    },
    {
        "Y": "Tensor.gather",
        "X": "What does Seetorch.gather do?",
        "Z": "Tensor.gather Seetorch.gather()"
    },
    {
        "Y": "Tensor",
        "X": "What is.gcd Seetorch.gcd?",
        "Z": "Tensor.gcd Seetorch.gcd()"
    },
    {
        "Y": "Seetorch.gcd",
        "X": "What does Tensor.gcd stand for?",
        "Z": "Tensor.gcd Seetorch.gcd()"
    },
    {
        "Y": "Tensor.gcd",
        "X": "What is Seetorch.gcd?",
        "Z": "Tensor.gcd Seetorch.gcd()"
    },
    {
        "Y": "Tensor.gcd_ In-place version ofgcd()",
        "X": "What is gcd function?",
        "Z": "Tensor.gcd_ In-place version ofgcd()"
    },
    {
        "Y": "Tensor.gcd_ In-place",
        "X": "What is version of gcd()?",
        "Z": "Tensor.gcd_ In-place version ofgcd()"
    },
    {
        "Y": "Tensor.ge Seetorch.ge()",
        "X": "What does Tensor.ge Seetorch.ge do?",
        "Z": "Tensor.ge Seetorch.ge()."
    },
    {
        "Y": "Tensor.ge Seetorch.ge",
        "X": "What is Tensor.ge Seetorch.ge?",
        "Z": "Tensor.ge Seetorch.ge()."
    },
    {
        "Y": "Tensor.ge_ In-place version ofge()",
        "X": "What does Tensor.ge do?",
        "Z": "Tensor.ge_ In-place version ofge()."
    },
    {
        "Y": "Tensor.ge",
        "X": "What is In-place version ofge()?",
        "Z": "Tensor.ge_ In-place version ofge()."
    },
    {
        "Y": "Seetorch",
        "X": "What is.greater_equal()?",
        "Z": "Tensor.greater_equal Seetorch.greater_equal()."
    },
    {
        "Y": "Tensor",
        "X": "What is In-place version ofgreater_equal()?",
        "Z": "Tensor.greater_equal_ In-place version ofgreater_equal()."
    },
    {
        "Y": "Tensor.greater_equal",
        "X": "What is the In-place version ofgreater_equal()?",
        "Z": "Tensor.greater_equal_ In-place version ofgreater_equal()."
    },
    {
        "Y": "geometric",
        "X": "What type of tensor is filled with elements drawn from the geometric distribution?",
        "Z": "Tensor.geometric_ Fillsselftensor with elements drawn from the geometric distribution:"
    },
    {
        "Y": "Tensor.geometric",
        "X": "What is fillsselftensor?",
        "Z": "Tensor.geometric_ Fillsselftensor with elements drawn from the geometric distribution:"
    },
    {
        "Y": "Tensor.geqrf",
        "X": "What is Seetorch.geqrf?",
        "Z": "Tensor.geqrf Seetorch.geqrf()"
    },
    {
        "Y": "Tensor.ger Seetorch.ger()",
        "X": "What does Tensor.ger Seetorch.ger do?",
        "Z": "Tensor.ger Seetorch.ger()"
    },
    {
        "Y": "Seetorch.ger",
        "X": "What is Tensor.ger?",
        "Z": "Tensor.ger Seetorch.ger()"
    },
    {
        "Y": "Tensor.ger",
        "X": "What is Seetorch.ger function?",
        "Z": "Tensor.ger Seetorch.ger()"
    },
    {
        "Y": "Tensor.get_device",
        "X": "What returns the device ordinal of the GPU on which the tensor resides?",
        "Z": "Tensor.get_device For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides."
    },
    {
        "Y": "Tensor.get_device",
        "X": "What function returns the device ordinal of the GPU on which the tensor resides?",
        "Z": "Tensor.get_device For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides."
    },
    {
        "Y": "the GPU",
        "X": "What device does Tensor.get_device return the device ordinal of?",
        "Z": "Tensor.get_device For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides."
    },
    {
        "Y": "Tensor.gt",
        "X": "What is Seetorch.gt()?",
        "Z": "Tensor.gt Seetorch.gt()."
    },
    {
        "Y": "Tensor.gt_ In-place version ofgt()",
        "X": "What is gt function?",
        "Z": "Tensor.gt_ In-place version ofgt()."
    },
    {
        "Y": "Tensor.gt_ In-place",
        "X": "What is version ofgt()?",
        "Z": "Tensor.gt_ In-place version ofgt()."
    },
    {
        "Y": "Tensor",
        "X": "What is component that makes a great Seetorch?",
        "Z": "Tensor.greater Seetorch.greater()."
    },
    {
        "Y": "Tensor.greater",
        "X": "What is In-place version ofgreater()?",
        "Z": "Tensor.greater_ In-place version ofgreater()."
    },
    {
        "Y": "self.half",
        "X": "What is the equivalent of self.to(torch.float16)?",
        "Z": "Tensor.half self.half()is equivalent toself.to(torch.float16)."
    },
    {
        "Y": "self.to(torch.float16)",
        "X": "What is tensor.half self.half() equivalent to?",
        "Z": "Tensor.half self.half()is equivalent toself.to(torch.float16)."
    },
    {
        "Y": "Tensor.hardshrink",
        "X": "What does Seetorch.nn.functional.hardshrink() do?",
        "Z": "Tensor.hardshrink Seetorch.nn.functional.hardshrink()"
    },
    {
        "Y": "Tensor",
        "X": "What type of hardshrink Seetorch.nn.functional.hardshrink()?",
        "Z": "Tensor.hardshrink Seetorch.nn.functional.hardshrink()"
    },
    {
        "Y": "Tensor.heaviside Seetorch.heaviside",
        "X": "What is Tensor.heaviside Seetorch.heaviside?",
        "Z": "Tensor.heaviside Seetorch.heaviside()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.heaviside?",
        "Z": "Tensor.heaviside Seetorch.heaviside()"
    },
    {
        "Y": "Tensor.histc Seetorch.histc()",
        "X": "What does Tensor.histc do?",
        "Z": "Tensor.histc Seetorch.histc()"
    },
    {
        "Y": "Seetorch.histc",
        "X": "What is Tensor.histc?",
        "Z": "Tensor.histc Seetorch.histc()"
    },
    {
        "Y": "Tensor.histc",
        "X": "What is Seetorch.histc?",
        "Z": "Tensor.histc Seetorch.histc()"
    },
    {
        "Y": "Seetorch.hsplit",
        "X": "What is Tensor.hsplit?",
        "Z": "Tensor.hsplit Seetorch.hsplit()"
    },
    {
        "Y": "Tensor.hsplit",
        "X": "What is Seetorch.hsplit() function?",
        "Z": "Tensor.hsplit Seetorch.hsplit()"
    },
    {
        "Y": "Tensor.hypot Seetorch.hypot",
        "X": "What is hypnotist?",
        "Z": "Tensor.hypot Seetorch.hypot()"
    },
    {
        "Y": "Seetorch.hypot",
        "X": "What is Tensor.hypot?",
        "Z": "Tensor.hypot Seetorch.hypot()"
    },
    {
        "Y": "Tensor.hypot",
        "X": "What is Seetorch.hypot() function?",
        "Z": "Tensor.hypot Seetorch.hypot()"
    },
    {
        "Y": "Tensor.hypot",
        "X": "What is In-place version ofhypot()?",
        "Z": "Tensor.hypot_ In-place version ofhypot()"
    },
    {
        "Y": "Tensor.i0 Seetorch.i0()",
        "X": "What does Tensor.i0 Seetorch.i0 do?",
        "Z": "Tensor.i0 Seetorch.i0()"
    },
    {
        "Y": "Tensor.i0_ In-place version ofi0()",
        "X": "What does Tensor.i0_ In-place version ofi0() do?",
        "Z": "Tensor.i0_ In-place version ofi0()"
    },
    {
        "Y": "Tensor.i0_ In-place",
        "X": "What is version ofi0()?",
        "Z": "Tensor.i0_ In-place version ofi0()"
    },
    {
        "Y": "Tensor.igamma Seetorch.igamma",
        "X": "What is Tensor.igamma Seetorch.igamma?",
        "Z": "Tensor.igamma Seetorch.igamma()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.igamma?",
        "Z": "Tensor.igamma Seetorch.igamma()"
    },
    {
        "Y": "Tensor.igamma",
        "X": "What is Seetorch.igamma?",
        "Z": "Tensor.igamma Seetorch.igamma()"
    },
    {
        "Y": "Tensor.igamma_ In-place version ofigamma()",
        "X": "What is In-place version ofigamma?",
        "Z": "Tensor.igamma_ In-place version ofigamma()"
    },
    {
        "Y": "Tensor.igamma_ In-place version ofigamma()",
        "X": "What is In-place version ofigamma()?",
        "Z": "Tensor.igamma_ In-place version ofigamma()"
    },
    {
        "Y": "Tensor.igammac Seetorch.igammac",
        "X": "What is Tensor.igammac Seetorch.igammac?",
        "Z": "Tensor.igammac Seetorch.igammac()"
    },
    {
        "Y": "Tensor.igammac_ In-place version ofigammac()",
        "X": "What does Tensor.igammac_ In-place version ofigammac() do?",
        "Z": "Tensor.igammac_ In-place version ofigammac()"
    },
    {
        "Y": "Tensor.igammac_ In-place",
        "X": "What is version ofigammac()?",
        "Z": "Tensor.igammac_ In-place version ofigammac()"
    },
    {
        "Y": "attr:alphatimestensorinto the self tensor",
        "X": "What does Tensor.index_add_ Accumulate the elements of?",
        "Z": "Tensor.index_add_ Accumulate the elements of attr:alphatimestensorinto the self tensor by adding to the indices in the order given inindex."
    },
    {
        "Y": "by adding to the indices",
        "X": "How do you add the elements of attr:alphatimestensorinto the self tensor?",
        "Z": "Tensor.index_add_ Accumulate the elements of attr:alphatimestensorinto the self tensor by adding to the indices in the order given inindex."
    },
    {
        "Y": "Tensor",
        "X": "What is the Out-of-place version of of torch.Tensor?",
        "Z": "Tensor.index_fill Out-of-place version of torch.Tensor.index_fill_()."
    },
    {
        "Y": "Out-of-place",
        "X": "What is version of Tensor.index_add?",
        "Z": "Tensor.index_add Out-of-place version of torch.Tensor.index_add_()."
    },
    {
        "Y": "Tensor.index_add Out-of-place",
        "X": "What version of of torch.Tensor.index_add_()?",
        "Z": "Tensor.index_add Out-of-place version of torch.Tensor.index_add_()."
    },
    {
        "Y": "the indices in the order given inindex",
        "X": "How are the elements of a selftensor copied?",
        "Z": "Tensor.index_copy_ Copies the elements oftensorinto the self tensor by selecting the indices in the order given inindex."
    },
    {
        "Y": "Tensor.index_copy",
        "X": "What _ Copies the elements oftensorinto the self tensor by selecting the indices in the order given inindex?",
        "Z": "Tensor.index_copy_ Copies the elements oftensorinto the self tensor by selecting the indices in the order given inindex."
    },
    {
        "Y": "Out-of-place",
        "X": "What is version of Tensor.index_copy?",
        "Z": "Tensor.index_copy Out-of-place version of torch.Tensor.index_copy_()."
    },
    {
        "Y": "Tensor.index_copy Out-of-place",
        "X": "What version of of torch.Tensor.index_copy_()?",
        "Z": "Tensor.index_copy Out-of-place version of torch.Tensor.index_copy_()."
    },
    {
        "Y": "the indices in the order given inindex",
        "X": "What does Tensor.index_fill_ select to fill the elements of the self tensor with valuevalue?",
        "Z": "Tensor.index_fill_ Fills the elements of the self tensor with valuevalueby selecting the indices in the order given inindex."
    },
    {
        "Y": "Tensor.index_fill",
        "X": "What fills the elements of the self tensor with valuevalue?",
        "Z": "Tensor.index_fill_ Fills the elements of the self tensor with valuevalueby selecting the indices in the order given inindex."
    },
    {
        "Y": "Tensor.index_fill Out-of-place",
        "X": "What version of of torch.Tensor.index_fill_()?",
        "Z": "Tensor.index_fill Out-of-place version of torch.Tensor.index_fill_()."
    },
    {
        "Y": "indices",
        "X": "What does Tensor.index_put_ put values from the tensorvaluesinto the tensorselfusing?",
        "Z": "Tensor.index_put_ Puts values from the tensorvaluesinto the tensorselfusing the indices specified inindices(which is a tuple of Tensors)."
    },
    {
        "Y": "Tensor.index_put",
        "X": "What puts values from the tensorvaluesinto the tensorselfusing the indices specified inindices?",
        "Z": "Tensor.index_put_ Puts values from the tensorvaluesinto the tensorselfusing the indices specified inindices(which is a tuple of Tensors)."
    },
    {
        "Y": "Tensor",
        "X": "What is out-place version of index_put_()?",
        "Z": "Tensor.index_put Out-place version ofindex_put_()."
    },
    {
        "Y": "index_put",
        "X": "What is the Out-place version of index_put_()?",
        "Z": "Tensor.index_put Out-place version ofindex_put_()."
    },
    {
        "Y": "Tensor.index_put Out-place",
        "X": "What version of index_put_() is used?",
        "Z": "Tensor.index_put Out-place version ofindex_put_()."
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.index_select?",
        "Z": "Tensor.index_select Seetorch.index_select()"
    },
    {
        "Y": "Tensor.index_select Seetorch.index_select()",
        "X": "What does Tensor.index_select Seetorch.index_select() do?",
        "Z": "Tensor.index_select Seetorch.index_select()"
    },
    {
        "Y": "Tensor",
        "X": "What indices return the indices tensor of asparse COO tensor?",
        "Z": "Tensor.indices Return the indices tensor of asparse COO tensor."
    },
    {
        "Y": "indices",
        "X": "What returns the indices tensor of asparse COO tensor?",
        "Z": "Tensor.indices Return the indices tensor of asparse COO tensor."
    },
    {
        "Y": "self.int",
        "X": "What is equivalent toself.to(torch.int32)?",
        "Z": "Tensor.int self.int()is equivalent toself.to(torch.int32)."
    },
    {
        "Y": "self.to(torch.int32)",
        "X": "What is self.int equivalent to?",
        "Z": "Tensor.int self.int()is equivalent toself.to(torch.int32)."
    },
    {
        "Y": "uint8_t",
        "X": "What is the underlying uint8_t value of the given Tensor?",
        "Z": "Tensor.int_repr Given a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."
    },
    {
        "Y": "self.int_repr()",
        "X": "What returns a CPU Tensor with uint8_t as data type?",
        "Z": "Tensor.int_repr Given a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."
    },
    {
        "Y": "uint8_t",
        "X": "What is the data type that stores the underlying uint8_t values of the given Tensor?",
        "Z": "Tensor.int_repr Given a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor."
    },
    {
        "Y": "Tensor",
        "X": "What is inverse Seetorch?",
        "Z": "Tensor.inverse Seetorch.inverse()"
    },
    {
        "Y": "Tensor.inverse Seetorch.inverse()",
        "X": "What is Tensor.inverse Seetorch.inverse()?",
        "Z": "Tensor.inverse Seetorch.inverse()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that closes Seetorch.isclose?",
        "Z": "Tensor.isclose Seetorch.isclose()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.isclose?",
        "Z": "Tensor.isclose Seetorch.isclose()"
    },
    {
        "Y": "Seetorch",
        "X": "What is.isfinite?",
        "Z": "Tensor.isfinite Seetorch.isfinite()"
    },
    {
        "Y": "Tensor.isfinite",
        "X": "What is Seetorch.isfinite?",
        "Z": "Tensor.isfinite Seetorch.isfinite()"
    },
    {
        "Y": "Tensor.isinf",
        "X": "What is Seetorch.isinf function?",
        "Z": "Tensor.isinf Seetorch.isinf()"
    },
    {
        "Y": "Seetorch.isposinf",
        "X": "What is Tensor.isposinf?",
        "Z": "Tensor.isposinf Seetorch.isposinf()"
    },
    {
        "Y": "Tensor.isposinf",
        "X": "What is Seetorch.isposinf function?",
        "Z": "Tensor.isposinf Seetorch.isposinf()"
    },
    {
        "Y": "Tensor.isneginf",
        "X": "What does Seetorch.isneginf do?",
        "Z": "Tensor.isneginf Seetorch.isneginf()"
    },
    {
        "Y": "Seetorch.isneginf",
        "X": "What is Tensor.isneginf?",
        "Z": "Tensor.isneginf Seetorch.isneginf()"
    },
    {
        "Y": "Tensor.isneginf",
        "X": "What is Seetorch.isneginf function?",
        "Z": "Tensor.isneginf Seetorch.isneginf()"
    },
    {
        "Y": "Tensor.isnan Seetorch.isnan",
        "X": "What is Tensor.isnan Seetorch.isnan?",
        "Z": "Tensor.isnan Seetorch.isnan()"
    },
    {
        "Y": "Tensor.is_contiguous Returns True",
        "X": "What happens if selftensor is contiguous in memory in the order specified by memory format?",
        "Z": "Tensor.is_contiguous Returns True ifselftensor is contiguous in memory in the order specified by memory format."
    },
    {
        "Y": "Tensor.is_contiguous",
        "X": "What Returns True if selftensor is contiguous in memory in the order specified by memory format?",
        "Z": "Tensor.is_contiguous Returns True ifselftensor is contiguous in memory in the order specified by memory format."
    },
    {
        "Y": "Tensor.is_complex",
        "X": "What returns true if the data type ofselfis a complex data type?",
        "Z": "Tensor.is_complex Returns True if the data type ofselfis a complex data type."
    },
    {
        "Y": "Tensor.is_complex",
        "X": "What Returns True if the data type ofselfis a complex data type?",
        "Z": "Tensor.is_complex Returns True if the data type ofselfis a complex data type."
    },
    {
        "Y": "Tensor.is_floating_point",
        "X": "What returns true if the data type ofselfis a floating point data type?",
        "Z": "Tensor.is_floating_point Returns True if the data type ofselfis a floating point data type."
    },
    {
        "Y": "True",
        "X": "Tensor.is_floating_point Returns what if the data type ofselfis a floating point data type?",
        "Z": "Tensor.is_floating_point Returns True if the data type ofselfis a floating point data type."
    },
    {
        "Y": "Tensor.is_floating_point",
        "X": "What Returns True if the data type ofselfis a floating point data type?",
        "Z": "Tensor.is_floating_point Returns True if the data type ofselfis a floating point data type."
    },
    {
        "Y": "leaf Tensors",
        "X": "What are all Tensors that have grad which is False?",
        "Z": "Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention."
    },
    {
        "Y": "Tensor.is_pinned",
        "X": "What returns true if the tensor resides in pinned memory?",
        "Z": "Tensor.is_pinned Returns true if this tensor resides in pinned memory."
    },
    {
        "Y": "pinned memory",
        "X": "Where does the tensor reside?",
        "Z": "Tensor.is_pinned Returns true if this tensor resides in pinned memory."
    },
    {
        "Y": "true",
        "X": "If this tensor resides in pinned memory, what does Tensor.is_pinned return?",
        "Z": "Tensor.is_pinned Returns true if this tensor resides in pinned memory."
    },
    {
        "Y": "exact same memory",
        "X": "What do both tensors point to?",
        "Z": "Tensor.is_set_to Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."
    },
    {
        "Y": "if both tensors are pointing to the exact same memory",
        "X": "If both tensors are pointing to the exact same memory, what does Tensor.is_set_to Returns True?",
        "Z": "Tensor.is_set_to Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride)."
    },
    {
        "Y": "shared memory",
        "X": "Tensor.is_shared Checks if tensor is in what type of memory?",
        "Z": "Tensor.is_shared Checks if tensor is in shared memory."
    },
    {
        "Y": "Tensor",
        "X": "What check if tensor is in shared memory?",
        "Z": "Tensor.is_shared Checks if tensor is in shared memory."
    },
    {
        "Y": "Tensor.is_shared",
        "X": "What checks if tensor is in shared memory?",
        "Z": "Tensor.is_shared Checks if tensor is in shared memory."
    },
    {
        "Y": "True",
        "X": "Tensor.is_signed Returns what if the data type ofselfis a signed data type?",
        "Z": "Tensor.is_signed Returns True if the data type ofselfis a signed data type."
    },
    {
        "Y": "Tensor.is_signed",
        "X": "What returns true if the data type ofselfis a signed data type?",
        "Z": "Tensor.is_signed Returns True if the data type ofselfis a signed data type."
    },
    {
        "Y": "Tensor.is_signed",
        "X": "What Returns True if the data type ofselfis a signed data type?",
        "Z": "Tensor.is_signed Returns True if the data type ofselfis a signed data type."
    },
    {
        "Y": "sparse storage layout",
        "X": "What type of storage layout does the Tensor use?",
        "Z": "Tensor.is_sparse Is True ifthe Tensor uses sparse storage layout,False otherwise."
    },
    {
        "Y": "False",
        "X": "What is true if the Tensor uses sparse storage layout?",
        "Z": "Tensor.is_sparse Is True ifthe Tensor uses sparse storage layout,False otherwise."
    },
    {
        "Y": "sparse",
        "X": "Is the Tensor true if the Tensor uses sparse storage layout?",
        "Z": "Tensor.is_sparse Is True ifthe Tensor uses sparse storage layout,False otherwise."
    },
    {
        "Y": "Tensor.istft Seetorch.istft()",
        "X": "What does Tensor.istft do?",
        "Z": "Tensor.istft Seetorch.istft()"
    },
    {
        "Y": "Seetorch.istft",
        "X": "What is Tensor.istft?",
        "Z": "Tensor.istft Seetorch.istft()"
    },
    {
        "Y": "Tensor.istft",
        "X": "What is Seetorch.istft?",
        "Z": "Tensor.istft Seetorch.istft()"
    },
    {
        "Y": "Tensor.isreal",
        "X": "What does Seetorch.isreal do?",
        "Z": "Tensor.isreal Seetorch.isreal()"
    },
    {
        "Y": "Seetorch",
        "X": "What is the real name for Tensor?",
        "Z": "Tensor.isreal Seetorch.isreal()"
    },
    {
        "Y": "Tensor.isreal",
        "X": "What is the name of Seetorch?",
        "Z": "Tensor.isreal Seetorch.isreal()"
    },
    {
        "Y": "Python",
        "X": "What language does Tensor.item come from?",
        "Z": "Tensor.item Returns the value of this tensor as a standard Python number."
    },
    {
        "Y": "Python",
        "X": "What language does Tensor.item return the value of this tensor as a standard number?",
        "Z": "Tensor.item Returns the value of this tensor as a standard Python number."
    },
    {
        "Y": "Tensor.kthvalue Seetorch.kthvalue",
        "X": "What is value?",
        "Z": "Tensor.kthvalue Seetorch.kthvalue()"
    },
    {
        "Y": "Seetorch",
        "X": "What is a Tensor.kthvalue?",
        "Z": "Tensor.kthvalue Seetorch.kthvalue()"
    },
    {
        "Y": "Tensor.kthvalue",
        "X": "What is Seetorch.kthvalue?",
        "Z": "Tensor.kthvalue Seetorch.kthvalue()"
    },
    {
        "Y": "Tensor.lcm",
        "X": "What is Seetorch.lcm()?",
        "Z": "Tensor.lcm Seetorch.lcm()"
    },
    {
        "Y": "Tensor.lcm_ In-place version oflcm()",
        "X": "What is in-place version of the in-place version oflcm()?",
        "Z": "Tensor.lcm_ In-place version oflcm()"
    },
    {
        "Y": "Tensor.lcm_ In-place",
        "X": "What is version oflcm()?",
        "Z": "Tensor.lcm_ In-place version oflcm()"
    },
    {
        "Y": "Tensor.ldexp",
        "X": "What does Seetorch.ldexp do?",
        "Z": "Tensor.ldexp Seetorch.ldexp()"
    },
    {
        "Y": "Seetorch.ldexp",
        "X": "What is Tensor.ldexp?",
        "Z": "Tensor.ldexp Seetorch.ldexp()"
    },
    {
        "Y": "Tensor.ldexp",
        "X": "What is Seetorch.ldexp?",
        "Z": "Tensor.ldexp Seetorch.ldexp()"
    },
    {
        "Y": "Tensor.ldexp_ In-place version ofldexp()",
        "X": "What does Tensor.ldexp_ In-place version ofldexp() do?",
        "Z": "Tensor.ldexp_ In-place version ofldexp()"
    },
    {
        "Y": "Tensor.ldexp_ In-place",
        "X": "What is version ofldexp()?",
        "Z": "Tensor.ldexp_ In-place version ofldexp()"
    },
    {
        "Y": "Tensor.le Seetorch.le",
        "X": "What is Tensor.le Seetorch.le?",
        "Z": "Tensor.le Seetorch.le()."
    },
    {
        "Y": "Tensor.le_ In-place version ofle()",
        "X": "What does Tensor.le_ In-place version ofle do?",
        "Z": "Tensor.le_ In-place version ofle()."
    },
    {
        "Y": "Tensor.le_ In-place",
        "X": "What is version ofle()?",
        "Z": "Tensor.le_ In-place version ofle()."
    },
    {
        "Y": "Tensor",
        "X": "What is component that makes Seetorch less_equal?",
        "Z": "Tensor.less_equal Seetorch.less_equal()."
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.less_equal?",
        "Z": "Tensor.less_equal Seetorch.less_equal()."
    },
    {
        "Y": "Tensor",
        "X": "What is In-place version ofless_equal()?",
        "Z": "Tensor.less_equal_ In-place version ofless_equal()."
    },
    {
        "Y": "Tensor.less_equal",
        "X": "What is the In-place version ofless_equal()?",
        "Z": "Tensor.less_equal_ In-place version ofless_equal()."
    },
    {
        "Y": "Tensor.lerp",
        "X": "What is Seetorch.lerp?",
        "Z": "Tensor.lerp Seetorch.lerp()"
    },
    {
        "Y": "Tensor.lerp_ In-place version oflerp()",
        "X": "What does Tensor.lerp_ In-place version oflerp do?",
        "Z": "Tensor.lerp_ In-place version oflerp()"
    },
    {
        "Y": "Tensor.lerp_ In-place",
        "X": "What is version oflerp()?",
        "Z": "Tensor.lerp_ In-place version oflerp()"
    },
    {
        "Y": "Seetorch.lgamma",
        "X": "What is Tensor.lgamma?",
        "Z": "Tensor.lgamma Seetorch.lgamma()"
    },
    {
        "Y": "Tensor.lgamma",
        "X": "What is Seetorch.lgamma?",
        "Z": "Tensor.lgamma Seetorch.lgamma()"
    },
    {
        "Y": "Tensor.lgamma_ In-place version oflgamma()",
        "X": "What is in-place version oflgamma?",
        "Z": "Tensor.lgamma_ In-place version oflgamma()"
    },
    {
        "Y": "Tensor.lgamma",
        "X": "What is In-place version oflgamma()?",
        "Z": "Tensor.lgamma_ In-place version oflgamma()"
    },
    {
        "Y": "Tensor.log Seetorch.log()",
        "X": "What does Tensor.log Seetorch.log do?",
        "Z": "Tensor.log Seetorch.log()"
    },
    {
        "Y": "Tensor.log Seetorch.log()",
        "X": "What is Tensor.log Seetorch.log()?",
        "Z": "Tensor.log Seetorch.log()"
    },
    {
        "Y": "Tensor.log_ In-place version oflog()",
        "X": "What does Tensor.log_ In-place version oflog() do?",
        "Z": "Tensor.log_ In-place version oflog()"
    },
    {
        "Y": "Tensor.log_ In-place",
        "X": "What is version oflog?",
        "Z": "Tensor.log_ In-place version oflog()"
    },
    {
        "Y": "Seetorch.logdet",
        "X": "What is Tensor.logdet?",
        "Z": "Tensor.logdet Seetorch.logdet()"
    },
    {
        "Y": "Tensor.logdet",
        "X": "What is Seetorch.logdet?",
        "Z": "Tensor.logdet Seetorch.logdet()"
    },
    {
        "Y": "Tensor.log10 Seetorch.log10()",
        "X": "What does Tensor.log10 Seetorch.log10 call?",
        "Z": "Tensor.log10 Seetorch.log10()"
    },
    {
        "Y": "Seetorch.log10",
        "X": "What is Tensor.log10?",
        "Z": "Tensor.log10 Seetorch.log10()"
    },
    {
        "Y": "Tensor.log10",
        "X": "What is Seetorch.log10?",
        "Z": "Tensor.log10 Seetorch.log10()"
    },
    {
        "Y": "Tensor.log10_ In-place version oflog10()",
        "X": "What does Tensor.log10_ In-place version oflog10() do?",
        "Z": "Tensor.log10_ In-place version oflog10()"
    },
    {
        "Y": "Tensor.log10_ In-place",
        "X": "What is version oflog10()?",
        "Z": "Tensor.log10_ In-place version oflog10()"
    },
    {
        "Y": "Tensor.log1p",
        "X": "What does Seetorch.log1p do?",
        "Z": "Tensor.log1p Seetorch.log1p()"
    },
    {
        "Y": "Seetorch.log1p",
        "X": "What is Tensor.log1p?",
        "Z": "Tensor.log1p Seetorch.log1p()"
    },
    {
        "Y": "Tensor.log1p",
        "X": "What is Seetorch.log1p() function?",
        "Z": "Tensor.log1p Seetorch.log1p()"
    },
    {
        "Y": "Tensor.log1p_ In-place version oflog1p()",
        "X": "What does Tensor.log1p_ In-place version oflog1p() do?",
        "Z": "Tensor.log1p_ In-place version oflog1p()"
    },
    {
        "Y": "Tensor.log1p_ In-place",
        "X": "What is version oflog1p()?",
        "Z": "Tensor.log1p_ In-place version oflog1p()"
    },
    {
        "Y": "Tensor.log2 Seetorch.log2()",
        "X": "What does Tensor.log2 Seetorch.log2() do?",
        "Z": "Tensor.log2 Seetorch.log2()"
    },
    {
        "Y": "Seetorch.log2",
        "X": "What is Tensor.log2?",
        "Z": "Tensor.log2 Seetorch.log2()"
    },
    {
        "Y": "Tensor.log2",
        "X": "What is Seetorch.log2() function?",
        "Z": "Tensor.log2 Seetorch.log2()"
    },
    {
        "Y": "Tensor.log2_ In-place version oflog2()",
        "X": "What is In-place version oflog2()?",
        "Z": "Tensor.log2_ In-place version oflog2()"
    },
    {
        "Y": "Tensor.log2_ In-place",
        "X": "What is version oflog2()?",
        "Z": "Tensor.log2_ In-place version oflog2()"
    },
    {
        "Y": "Tensor.log_normal",
        "X": "What fillsselftensor with numbers samples from the log-normal distribution parameterized by the given meanmuand standard deviations",
        "Z": "Tensor.log_normal_ Fillsselftensor with numbers samples from the log-normal distribution parameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3."
    },
    {
        "Y": "Tensor.logaddexp",
        "X": "What does Seetorch.logaddexp call?",
        "Z": "Tensor.logaddexp Seetorch.logaddexp()"
    },
    {
        "Y": "Seetorch.logaddexp",
        "X": "What is Tensor.logaddexp?",
        "Z": "Tensor.logaddexp Seetorch.logaddexp()"
    },
    {
        "Y": "Tensor.logaddexp",
        "X": "What is Seetorch.logaddexp() function?",
        "Z": "Tensor.logaddexp Seetorch.logaddexp()"
    },
    {
        "Y": "Tensor.logaddexp2 Seetorch.logaddexp2()",
        "X": "What does Tensor.logaddexp2 do?",
        "Z": "Tensor.logaddexp2 Seetorch.logaddexp2()"
    },
    {
        "Y": "Tensor.logaddexp2 Seetorch.logaddexp2()",
        "X": "What is Tensor.logaddexp2 Seetorch.logaddexp2()?",
        "Z": "Tensor.logaddexp2 Seetorch.logaddexp2()"
    },
    {
        "Y": "Tensor.logsumexp",
        "X": "What does Seetorch.logsumexp use?",
        "Z": "Tensor.logsumexp Seetorch.logsumexp()"
    },
    {
        "Y": "Seetorch.logsumexp",
        "X": "What is Tensor.logsumexp?",
        "Z": "Tensor.logsumexp Seetorch.logsumexp()"
    },
    {
        "Y": "Tensor.logsumexp",
        "X": "What is Seetorch.logsumexp function?",
        "Z": "Tensor.logsumexp Seetorch.logsumexp()"
    },
    {
        "Y": "Tensor.orgqr Seetorch.orgqr",
        "X": "What is Seetorch?",
        "Z": "Tensor.orgqr Seetorch.orgqr()"
    },
    {
        "Y": "Tensor.logical_and Seetorch.logical_and",
        "X": "What are two examples of what?",
        "Z": "Tensor.logical_and Seetorch.logical_and()"
    },
    {
        "Y": "Tensor.logical_and_ In-place version oflogical_and()",
        "X": "What is Tensor.logical_and_ In-place version oflogical_and()?",
        "Z": "Tensor.logical_and_ In-place version oflogical_and()"
    },
    {
        "Y": "Tensor.logical_and_ In-place",
        "X": "What is version oflogical_and()?",
        "Z": "Tensor.logical_and_ In-place version oflogical_and()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.logical_not?",
        "Z": "Tensor.logical_not Seetorch.logical_not()"
    },
    {
        "Y": "Tensor.logical_not",
        "X": "What is Seetorch.logical_not()?",
        "Z": "Tensor.logical_not Seetorch.logical_not()"
    },
    {
        "Y": "Tensor.logical_not_ In-place version oflogical_not()",
        "X": "What is In-place version of logical_not()?",
        "Z": "Tensor.logical_not_ In-place version oflogical_not()"
    },
    {
        "Y": "Tensor.logical_not",
        "X": "What is In-place version oflogical_not()?",
        "Z": "Tensor.logical_not_ In-place version oflogical_not()"
    },
    {
        "Y": "Seetorch",
        "X": "What is a Tensor.logical_or?",
        "Z": "Tensor.logical_or Seetorch.logical_or()"
    },
    {
        "Y": "Tensor.logical_or",
        "X": "What is Seetorch.logical_or?",
        "Z": "Tensor.logical_or Seetorch.logical_or()"
    },
    {
        "Y": "Tensor.logical_or_ In-place version oflogical_or()",
        "X": "What is In-place version oflogical_or?",
        "Z": "Tensor.logical_or_ In-place version oflogical_or()"
    },
    {
        "Y": "Tensor.logical_or",
        "X": "What is the In-place version oflogical_or?",
        "Z": "Tensor.logical_or_ In-place version oflogical_or()"
    },
    {
        "Y": "Tensor.logical_xor Seetorch.logical_xor",
        "X": "What is logical xor?",
        "Z": "Tensor.logical_xor Seetorch.logical_xor()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.logical_xor?",
        "Z": "Tensor.logical_xor Seetorch.logical_xor()"
    },
    {
        "Y": "Tensor.logical_xor",
        "X": "What is Seetorch.logical_xor?",
        "Z": "Tensor.logical_xor Seetorch.logical_xor()"
    },
    {
        "Y": "Tensor.logical_xor_ In-place version oflogical_xor()",
        "X": "What is In-place version of logical_xor?",
        "Z": "Tensor.logical_xor_ In-place version oflogical_xor()"
    },
    {
        "Y": "Tensor.logical_xor",
        "X": "What is In-place version oflogical_xor()?",
        "Z": "Tensor.logical_xor_ In-place version oflogical_xor()"
    },
    {
        "Y": "Tensor.logit Seetorch.logit()",
        "X": "What does Tensor.logit Seetorch.logit do?",
        "Z": "Tensor.logit Seetorch.logit()"
    },
    {
        "Y": "Seetorch.logit",
        "X": "What is Tensor.logit?",
        "Z": "Tensor.logit Seetorch.logit()"
    },
    {
        "Y": "Tensor.logit",
        "X": "What is Seetorch.logit function?",
        "Z": "Tensor.logit Seetorch.logit()"
    },
    {
        "Y": "Tensor.logit_ In-place version oflogit()",
        "X": "What does Tensor.logit_ In-place version oflogit() do?",
        "Z": "Tensor.logit_ In-place version oflogit()"
    },
    {
        "Y": "Tensor.logit_ In-place",
        "X": "What is version oflogit()?",
        "Z": "Tensor.logit_ In-place version oflogit()"
    },
    {
        "Y": "self.to",
        "X": "What is tensor.long self.long() equivalent to?",
        "Z": "Tensor.long self.long()is equivalent toself.to(torch.int64)."
    },
    {
        "Y": "Tensor.lstsq",
        "X": "What is Seetorch.lstsq?",
        "Z": "Tensor.lstsq Seetorch.lstsq()"
    },
    {
        "Y": "Seetorch.lstsq",
        "X": "What is Tensor.lstsq?",
        "Z": "Tensor.lstsq Seetorch.lstsq()"
    },
    {
        "Y": "Tensor.lt Seetorch.lt",
        "X": "What is company that owns the company?",
        "Z": "Tensor.lt Seetorch.lt()."
    },
    {
        "Y": "Seetorch.lt",
        "X": "What is Tensor.lt?",
        "Z": "Tensor.lt Seetorch.lt()."
    },
    {
        "Y": "Tensor.lt",
        "X": "What is the name of Seetorch.lt?",
        "Z": "Tensor.lt Seetorch.lt()."
    },
    {
        "Y": "Tensor.lt_ In-place",
        "X": "What is version oflt()?",
        "Z": "Tensor.lt_ In-place version oflt()."
    },
    {
        "Y": "Tensor",
        "X": "What is less lt(other) -> Tensor?",
        "Z": "Tensor.less lt(other) -> Tensor"
    },
    {
        "Y": "Tensor.less",
        "X": "What is lt(other) -> Tensor?",
        "Z": "Tensor.less lt(other) -> Tensor"
    },
    {
        "Y": "Tensor.less_ In-place version ofless()",
        "X": "What is Tensor.less_ In-place version ofless()?",
        "Z": "Tensor.less_ In-place version ofless()."
    },
    {
        "Y": "Tensor.less_ In-place",
        "X": "What is version ofless()?",
        "Z": "Tensor.less_ In-place version ofless()."
    },
    {
        "Y": "Tensor.lu Seetorch.lu",
        "X": "What is Tensor.lu Seetorch.lu?",
        "Z": "Tensor.lu Seetorch.lu()"
    },
    {
        "Y": "Seetorch.lu",
        "X": "What is Tensor.lu?",
        "Z": "Tensor.lu Seetorch.lu()"
    },
    {
        "Y": "Tensor.lu",
        "X": "What is Seetorch.lu?",
        "Z": "Tensor.lu Seetorch.lu()"
    },
    {
        "Y": "Tensor.lu_solve",
        "X": "What does Seetorch.lu_solve do?",
        "Z": "Tensor.lu_solve Seetorch.lu_solve()"
    },
    {
        "Y": "Seetorch.lu_solve",
        "X": "What does Tensor.lu_solve do?",
        "Z": "Tensor.lu_solve Seetorch.lu_solve()"
    },
    {
        "Y": "Seetorch.lu_solve()",
        "X": "What does Tensor.lu_solve?",
        "Z": "Tensor.lu_solve Seetorch.lu_solve()"
    },
    {
        "Y": "Tensor.lu_solve",
        "X": "What does Seetorch.lu_solve() do?",
        "Z": "Tensor.lu_solve Seetorch.lu_solve()"
    },
    {
        "Y": "Tensor.as_subclass",
        "X": "What makes aclsinstance with the same data pointer as self?",
        "Z": "Tensor.as_subclass Makes aclsinstance with the same data pointer asself."
    },
    {
        "Y": "Tensor.as_subclass",
        "X": "What makes aclsinstance with the same data pointer asself?",
        "Z": "Tensor.as_subclass Makes aclsinstance with the same data pointer asself."
    },
    {
        "Y": "Tensor.map_ Appliescallable",
        "X": "What is used for each element in selftensor and the giventensorand stores the results in selftensor?",
        "Z": "Tensor.map_ Appliescallablefor each element inselftensor and the giventensorand stores the results inselftensor."
    },
    {
        "Y": "Tensor.map",
        "X": "What _ Appliescallablefor each element inselftensor and the giventensorand stores the results inselftensor?",
        "Z": "Tensor.map_ Appliescallablefor each element inselftensor and the giventensorand stores the results inselftensor."
    },
    {
        "Y": "Tensor.masked_scatter",
        "X": "What is the Out-of-place version of of torch.Tensor.masked_scatter_()?",
        "Z": "Tensor.masked_scatter Out-of-place version of torch.Tensor.masked_scatter_()"
    },
    {
        "Y": "Tensor.masked_scatter Out-of-place",
        "X": "What version of of torch.Tensor.masked_scatter_()?",
        "Z": "Tensor.masked_scatter Out-of-place version of torch.Tensor.masked_scatter_()"
    },
    {
        "Y": "selftensor",
        "X": "What does Tensor.masked_fill_fill elements of?",
        "Z": "Tensor.masked_fill_ Fills elements ofselftensor withvaluewheremaskis True."
    },
    {
        "Y": "Tensor.masked_fill Out-of-place version of torch.Tensor.masked_fill_()",
        "X": "What is Out-of-place version of of torch.Tensor.masked_fill?",
        "Z": "Tensor.masked_fill Out-of-place version of torch.Tensor.masked_fill_()"
    },
    {
        "Y": "Tensor.masked_fill Out-of-place",
        "X": "What version of of torch.Tensor.masked_fill_()?",
        "Z": "Tensor.masked_fill Out-of-place version of torch.Tensor.masked_fill_()"
    },
    {
        "Y": "Seetorch.masked_select",
        "X": "What does Tensor.masked_select do?",
        "Z": "Tensor.masked_select Seetorch.masked_select()"
    },
    {
        "Y": "Tensor.masked_select Seetorch.masked_select()",
        "X": "What does Tensor.masked_select Seetorch.masked_select() do?",
        "Z": "Tensor.masked_select Seetorch.masked_select()"
    },
    {
        "Y": "Tensor.matmul Seetorch.matmul",
        "X": "What is Tensor.matmul Seetorch.matmul?",
        "Z": "Tensor.matmul Seetorch.matmul()"
    },
    {
        "Y": "Seetorch.matmul",
        "X": "What is Tensor.matmul?",
        "Z": "Tensor.matmul Seetorch.matmul()"
    },
    {
        "Y": "Tensor.matmul",
        "X": "What is Seetorch.matmul?",
        "Z": "Tensor.matmul Seetorch.matmul()"
    },
    {
        "Y": "Tensor.matrix _power",
        "X": "What is deprecated?",
        "Z": "Tensor.matrix _power Notematrix _power()is deprecated, usetorch.linalg.matrix _power()instead."
    },
    {
        "Y": "usetorch.linalg.matrix _power()",
        "X": "What is the replacement for Tensor.matrix _power?",
        "Z": "Tensor.matrix _power Notematrix _power()is deprecated, usetorch.linalg.matrix _power()instead."
    },
    {
        "Y": "Tensor.matrix _exp Seetorch.matrix _exp()",
        "X": "What does Tensor.matrix _exp do?",
        "Z": "Tensor.matrix _exp Seetorch.matrix _exp()"
    },
    {
        "Y": "Tensor.matrix _exp",
        "X": "What is Seetorch.matrix _exp() function?",
        "Z": "Tensor.matrix _exp Seetorch.matrix _exp()"
    },
    {
        "Y": "Seetorch",
        "X": "What is tensor?",
        "Z": "Tensor.positive Seetorch.positive()"
    },
    {
        "Y": "Tensor.max Seetorch.max()",
        "X": "What does Tensor.max Seetorch.max() do?",
        "Z": "Tensor.max Seetorch.max()"
    },
    {
        "Y": "Tensor",
        "X": "What is the maximum Seetorch.maximum?",
        "Z": "Tensor.maximum Seetorch.maximum()"
    },
    {
        "Y": "Seetorch.maximum",
        "X": "What is the Tensor.maximum?",
        "Z": "Tensor.maximum Seetorch.maximum()"
    },
    {
        "Y": "Seetorch",
        "X": "What is the maximum of a Tensor?",
        "Z": "Tensor.maximum Seetorch.maximum()"
    },
    {
        "Y": "Seetorch.mean",
        "X": "What does Tensor mean?",
        "Z": "Tensor.mean Seetorch.mean()"
    },
    {
        "Y": "Tensor.mean",
        "X": "What does Seetorch.mean mean?",
        "Z": "Tensor.mean Seetorch.mean()"
    },
    {
        "Y": "Tensor",
        "X": "What is.median Seetorch.median?",
        "Z": "Tensor.median Seetorch.median()"
    },
    {
        "Y": "Tensor.median",
        "X": "What is Seetorch.median?",
        "Z": "Tensor.median Seetorch.median()"
    },
    {
        "Y": "Tensor.nanmedian",
        "X": "What does Seetorch.nanmedian stand for?",
        "Z": "Tensor.nanmedian Seetorch.nanmedian()"
    },
    {
        "Y": "Seetorch.nanmedian",
        "X": "What is Tensor.nanmedian?",
        "Z": "Tensor.nanmedian Seetorch.nanmedian()"
    },
    {
        "Y": "Tensor.nanmedian",
        "X": "What is Seetorch.nanmedian?",
        "Z": "Tensor.nanmedian Seetorch.nanmedian()"
    },
    {
        "Y": "Tensor.min Seetorch.min()",
        "X": "What does Tensor.min Seetorch.min do?",
        "Z": "Tensor.min Seetorch.min()"
    },
    {
        "Y": "Tensor.min",
        "X": "What is Seetorch.min function?",
        "Z": "Tensor.min Seetorch.min()"
    },
    {
        "Y": "Tensor.minimum Seetorch.minimum",
        "X": "What is the minimum value for a Seetorch?",
        "Z": "Tensor.minimum Seetorch.minimum()"
    },
    {
        "Y": "Seetorch",
        "X": "What is the minimum of a Tensor?",
        "Z": "Tensor.minimum Seetorch.minimum()"
    },
    {
        "Y": "Tensor.mm Seetorch.mm",
        "X": "What is Tensor.mm Seetorch.mm?",
        "Z": "Tensor.mm Seetorch.mm()"
    },
    {
        "Y": "Seetorch.smm",
        "X": "What is Tensor.smm?",
        "Z": "Tensor.smm Seetorch.smm()"
    },
    {
        "Y": "Tensor",
        "X": "What is function that determines the mode Seetorch.mode?",
        "Z": "Tensor.mode Seetorch.mode()"
    },
    {
        "Y": "Seetorch.mode",
        "X": "What is Tensor.mode?",
        "Z": "Tensor.mode Seetorch.mode()"
    },
    {
        "Y": "Tensor.mode Seetorch.mode",
        "X": "What is Tensor.mode Seetorch.mode?",
        "Z": "Tensor.mode Seetorch.mode()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that moves the Seetorch?",
        "Z": "Tensor.movedim Seetorch.movedim()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.movedim do?",
        "Z": "Tensor.movedim Seetorch.movedim()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.movedim?",
        "Z": "Tensor.movedim Seetorch.movedim()"
    },
    {
        "Y": "Tensor",
        "X": "What is movedim Seetorch.movedim()?",
        "Z": "Tensor.movedim Seetorch.movedim()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.moveaxis?",
        "Z": "Tensor.moveaxis Seetorch.moveaxis()"
    },
    {
        "Y": "Tensor.moveaxis",
        "X": "What is the name of Seetorch.moveaxis()?",
        "Z": "Tensor.moveaxis Seetorch.moveaxis()"
    },
    {
        "Y": "Tensor.msort Seetorch.msort()",
        "X": "What does Tensor.msort?",
        "Z": "Tensor.msort Seetorch.msort()"
    },
    {
        "Y": "Seetorch.msort",
        "X": "What is Tensor.msort?",
        "Z": "Tensor.msort Seetorch.msort()"
    },
    {
        "Y": "Tensor.msort",
        "X": "What is Seetorch.msort function?",
        "Z": "Tensor.msort Seetorch.msort()"
    },
    {
        "Y": "Tensor.mul Seetorch.mul",
        "X": "What is mule?",
        "Z": "Tensor.mul Seetorch.mul()."
    },
    {
        "Y": "Tensor.mul_ In-place version ofmul()",
        "X": "What is mul function?",
        "Z": "Tensor.mul_ In-place version ofmul()."
    },
    {
        "Y": "Tensor.mul_ In-place",
        "X": "What is version ofmul()?",
        "Z": "Tensor.mul_ In-place version ofmul()."
    },
    {
        "Y": "Tensor",
        "X": "What is component that is used to multiply Seetorch.multiply()?",
        "Z": "Tensor.multiply Seetorch.multiply()."
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.multiply?",
        "Z": "Tensor.multiply Seetorch.multiply()."
    },
    {
        "Y": "Tensor.multiply_ In-place version ofmultiply()",
        "X": "What is Tensor.multiply_ In-place version ofmultiply()?",
        "Z": "Tensor.multiply_ In-place version ofmultiply()."
    },
    {
        "Y": "Tensor.multiply_ In-place",
        "X": "What is version of multiply()?",
        "Z": "Tensor.multiply_ In-place version ofmultiply()."
    },
    {
        "Y": "Tensor.multinomial Seetorch.multinomial",
        "X": "What is Tensor.multinomial Seetorch.multinomial?",
        "Z": "Tensor.multinomial Seetorch.multinomial()"
    },
    {
        "Y": "Seetorch.multinomial",
        "X": "What is a Tensor.multinomial?",
        "Z": "Tensor.multinomial Seetorch.multinomial()"
    },
    {
        "Y": "Tensor.multinomial",
        "X": "What is Seetorch.multinomial()?",
        "Z": "Tensor.multinomial Seetorch.multinomial()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.mv?",
        "Z": "Tensor.mv Seetorch.mv()"
    },
    {
        "Y": "Tensor.mv",
        "X": "What is Seetorch.mv?",
        "Z": "Tensor.mv Seetorch.mv()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.mvlgamma?",
        "Z": "Tensor.mvlgamma Seetorch.mvlgamma()"
    },
    {
        "Y": "Tensor.mvlgamma",
        "X": "What is Seetorch.mvlgamma?",
        "Z": "Tensor.mvlgamma Seetorch.mvlgamma()"
    },
    {
        "Y": "Tensor.mvlgamma_ In-place version ofmvlgamma",
        "X": "What is mvlgamma version of Tensor?",
        "Z": "Tensor.mvlgamma_ In-place version ofmvlgamma()"
    },
    {
        "Y": "Tensor.mvlgamma",
        "X": "What is In-place version ofmvlgamma()?",
        "Z": "Tensor.mvlgamma_ In-place version ofmvlgamma()"
    },
    {
        "Y": "Tensor.nansum Seetorch.nansum",
        "X": "What is Tensor.nansum Seetorch.nansum?",
        "Z": "Tensor.nansum Seetorch.nansum()"
    },
    {
        "Y": "Seetorch.nansum",
        "X": "What is Tensor.nansum?",
        "Z": "Tensor.nansum Seetorch.nansum()"
    },
    {
        "Y": "Tensor.nansum",
        "X": "What is Seetorch.nansum?",
        "Z": "Tensor.nansum Seetorch.nansum()"
    },
    {
        "Y": "Tensor.narrow",
        "X": "What does Seetorch.narrow do?",
        "Z": "Tensor.narrow Seetorch.narrow()"
    },
    {
        "Y": "shared storage",
        "X": "Tensor.narrow_copy returns a copy instead of what?",
        "Z": "Tensor.narrow_copy Same asTensor.narrow()except returning a copy rather than shared storage."
    },
    {
        "Y": "shared storage",
        "X": "What does Tensor.narrow() return instead of?",
        "Z": "Tensor.narrow_copy Same asTensor.narrow()except returning a copy rather than shared storage."
    },
    {
        "Y": "Tensor.narrow()",
        "X": "What is the same as Tensor.narrow_copy?",
        "Z": "Tensor.narrow_copy Same asTensor.narrow()except returning a copy rather than shared storage."
    },
    {
        "Y": "Tensor.ndimension Alias fordim",
        "X": "What is Tensor.ndimension Alias fordim?",
        "Z": "Tensor.ndimension Alias fordim()"
    },
    {
        "Y": "Tensor.ndimension",
        "X": "What is Alias fordim()?",
        "Z": "Tensor.ndimension Alias fordim()"
    },
    {
        "Y": "Tensor.nan_to_num",
        "X": "What does Seetorch.nan_to_num() do?",
        "Z": "Tensor.nan_to_num Seetorch.nan_to_num()."
    },
    {
        "Y": "Tensor.nan_to_num_ In-place version ofnan_to_num()",
        "X": "What is In-place version ofnan_to_num()?",
        "Z": "Tensor.nan_to_num_ In-place version ofnan_to_num()."
    },
    {
        "Y": "Tensor.nan_to_num",
        "X": "What is the In-place version of ofnan_to_num()?",
        "Z": "Tensor.nan_to_num_ In-place version ofnan_to_num()."
    },
    {
        "Y": "Tensor.ne Seetorch.ne",
        "X": "What is Tensor.ne Seetorch.ne?",
        "Z": "Tensor.ne Seetorch.ne()."
    },
    {
        "Y": "Tensor.ne_ In-place version ofne()",
        "X": "What is function used by Tensor.ne?",
        "Z": "Tensor.ne_ In-place version ofne()."
    },
    {
        "Y": "Tensor.ne_ In-place",
        "X": "What is version of ofne()?",
        "Z": "Tensor.ne_ In-place version ofne()."
    },
    {
        "Y": "Tensor",
        "X": "What is tensor that is not equal to Seetorch?",
        "Z": "Tensor.not_equal Seetorch.not_equal()."
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.not_equal()?",
        "Z": "Tensor.not_equal Seetorch.not_equal()."
    },
    {
        "Y": "Tensor",
        "X": "What is not_equal Seetorch.not_equal()?",
        "Z": "Tensor.not_equal Seetorch.not_equal()."
    },
    {
        "Y": "Tensor",
        "X": "What is In-place version of not_equal()?",
        "Z": "Tensor.not_equal_ In-place version ofnot_equal()."
    },
    {
        "Y": "Tensor.not_equal",
        "X": "What is the In-place version of not_equal()?",
        "Z": "Tensor.not_equal_ In-place version ofnot_equal()."
    },
    {
        "Y": "Seetorch.neg",
        "X": "What is Tensor.neg?",
        "Z": "Tensor.neg Seetorch.neg()"
    },
    {
        "Y": "Tensor.neg",
        "X": "What is Seetorch.neg?",
        "Z": "Tensor.neg Seetorch.neg()"
    },
    {
        "Y": "Tensor.neg_ In-place version ofneg()",
        "X": "What is function that is used by Tensor.neg?",
        "Z": "Tensor.neg_ In-place version ofneg()"
    },
    {
        "Y": "Tensor.neg_ In-place",
        "X": "What is version ofneg()?",
        "Z": "Tensor.neg_ In-place version ofneg()"
    },
    {
        "Y": "Tensor",
        "X": "What is a negative Seetorch?",
        "Z": "Tensor.negative Seetorch.negative()"
    },
    {
        "Y": "Tensor.negative_ In-place version ofnegative",
        "X": "What is negative?",
        "Z": "Tensor.negative_ In-place version ofnegative()"
    },
    {
        "Y": "Tensor.negative_ In-place",
        "X": "What is version of negative?",
        "Z": "Tensor.negative_ In-place version ofnegative()"
    },
    {
        "Y": "Alias fornumel",
        "X": "What is Tensor.nelement?",
        "Z": "Tensor.nelement Alias fornumel()"
    },
    {
        "Y": "Tensor.nelement Alias fornumel()",
        "X": "What does Tensor.nelement Alias fornumel do?",
        "Z": "Tensor.nelement Alias fornumel()"
    },
    {
        "Y": "Tensor.nelement",
        "X": "What is Alias fornumel?",
        "Z": "Tensor.nelement Alias fornumel()"
    },
    {
        "Y": "Tensor",
        "X": "What is next after Seetorch?",
        "Z": "Tensor.nextafter Seetorch.nextafter()"
    },
    {
        "Y": "Tensor.nextafter_ In-place version ofnextafter()",
        "X": "What is In-place version ofnextafter()?",
        "Z": "Tensor.nextafter_ In-place version ofnextafter()"
    },
    {
        "Y": "Tensor.nextafter",
        "X": "What is In-place version of ofnextafter()?",
        "Z": "Tensor.nextafter_ In-place version ofnextafter()"
    },
    {
        "Y": "Tensor.nonzero",
        "X": "What does Seetorch.nonzero do?",
        "Z": "Tensor.nonzero Seetorch.nonzero()"
    },
    {
        "Y": "Seetorch.nonzero",
        "X": "What is Tensor.nonzero?",
        "Z": "Tensor.nonzero Seetorch.nonzero()"
    },
    {
        "Y": "Tensor.norm",
        "X": "What is the name of Seetorch.norm?",
        "Z": "Tensor.norm Seetorch.norm()"
    },
    {
        "Y": "bymeanandstd",
        "X": "What parameter does Tensor.normal_ Fillsselftensor with elements samples from?",
        "Z": "Tensor.normal_ Fillsselftensor with elements samples from the normal distribution parameterized bymeanandstd."
    },
    {
        "Y": "Tensor.normal_ Fillsselftensor",
        "X": "What is filled with elements from the normal distribution parameterized bymeanandstd?",
        "Z": "Tensor.normal_ Fillsselftensor with elements samples from the normal distribution parameterized bymeanandstd."
    },
    {
        "Y": "Tensor.normal",
        "X": "What fillsselftensor with elements samples from the normal distribution parameterized bymeanandstd?",
        "Z": "Tensor.normal_ Fillsselftensor with elements samples from the normal distribution parameterized bymeanandstd."
    },
    {
        "Y": "NumPyndarray",
        "X": "Tensor.numpy Returns selftensor as what?",
        "Z": "Tensor.numpy Returnsselftensor as a NumPyndarray."
    },
    {
        "Y": "Tensor.numpy",
        "X": "What returns selftensor as a NumPyndarray?",
        "Z": "Tensor.numpy Returnsselftensor as a NumPyndarray."
    },
    {
        "Y": "Seetorch.orgqr",
        "X": "What is Tensor.orgqr?",
        "Z": "Tensor.orgqr Seetorch.orgqr()"
    },
    {
        "Y": "Tensor.orgqr",
        "X": "What is the name of Seetorch.orgqr?",
        "Z": "Tensor.orgqr Seetorch.orgqr()"
    },
    {
        "Y": "Seetorch.ormqr",
        "X": "What is Tensor.ormqr?",
        "Z": "Tensor.ormqr Seetorch.ormqr()"
    },
    {
        "Y": "Tensor.ormqr",
        "X": "What is Seetorch.ormqr?",
        "Z": "Tensor.ormqr Seetorch.ormqr()"
    },
    {
        "Y": "Tensor.outer Seetorch.outer()",
        "X": "What does Tensor.outer do?",
        "Z": "Tensor.outer Seetorch.outer()."
    },
    {
        "Y": "Tensor.outer",
        "X": "What is the name of Seetorch.outer()?",
        "Z": "Tensor.outer Seetorch.outer()."
    },
    {
        "Y": "Seetorch.permute",
        "X": "What is Tensor.permute?",
        "Z": "Tensor.permute Seetorch.permute()"
    },
    {
        "Y": "Tensor.permute",
        "X": "What is the name of Seetorch.permute()?",
        "Z": "Tensor.permute Seetorch.permute()"
    },
    {
        "Y": "Tensor.pin_memory",
        "X": "What copies the tensor to pinned memory?",
        "Z": "Tensor.pin_memory Copies the tensor to pinned memory, if it\u2019s not already pinned."
    },
    {
        "Y": "pinned memory",
        "X": "What does Tensor.pin_memory copy the tensor to?",
        "Z": "Tensor.pin_memory Copies the tensor to pinned memory, if it\u2019s not already pinned."
    },
    {
        "Y": "Tensor.pinverse Seetorch.pinverse()",
        "X": "What is inverse of the Seetorch?",
        "Z": "Tensor.pinverse Seetorch.pinverse()"
    },
    {
        "Y": "Tensor.pinverse",
        "X": "What is Seetorch.pinverse()?",
        "Z": "Tensor.pinverse Seetorch.pinverse()"
    },
    {
        "Y": "Seetorch.polygamma",
        "X": "What is Tensor.polygamma?",
        "Z": "Tensor.polygamma Seetorch.polygamma()"
    },
    {
        "Y": "Tensor.polygamma",
        "X": "What is Seetorch.polygamma?",
        "Z": "Tensor.polygamma Seetorch.polygamma()"
    },
    {
        "Y": "Tensor.polygamma",
        "X": "What is in-place version of polygamma?",
        "Z": "Tensor.polygamma_ In-place version ofpolygamma()"
    },
    {
        "Y": "Tensor",
        "X": "What is a positive Seetorch?",
        "Z": "Tensor.positive Seetorch.positive()"
    },
    {
        "Y": "Tensor.pow Seetorch.pow",
        "X": "What is Tensor.pow Seetorch.pow?",
        "Z": "Tensor.pow Seetorch.pow()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.pow?",
        "Z": "Tensor.pow Seetorch.pow()"
    },
    {
        "Y": "Tensor.pow",
        "X": "What is Seetorch.pow() function?",
        "Z": "Tensor.pow Seetorch.pow()"
    },
    {
        "Y": "Tensor.pow_ In-place version ofpow()",
        "X": "What does Tensor.pow_ In-place version ofpow() do?",
        "Z": "Tensor.pow_ In-place version ofpow()"
    },
    {
        "Y": "Tensor.pow_ In-place",
        "X": "What is version ofpow()?",
        "Z": "Tensor.pow_ In-place version ofpow()"
    },
    {
        "Y": "Tensor.prod Seetorch.prod()",
        "X": "What does Tensor.prod do?",
        "Z": "Tensor.prod Seetorch.prod()"
    },
    {
        "Y": "Seetorch.prod",
        "X": "What is Tensor.prod?",
        "Z": "Tensor.prod Seetorch.prod()"
    },
    {
        "Y": "Tensor.prod",
        "X": "What is Seetorch.prod?",
        "Z": "Tensor.prod Seetorch.prod()"
    },
    {
        "Y": "Tensor.qr Seetorch.qr()",
        "X": "What does Tensor.qr stand for?",
        "Z": "Tensor.qr Seetorch.qr()"
    },
    {
        "Y": "Tensor.qr",
        "X": "What is Seetorch.qr?",
        "Z": "Tensor.qr Seetorch.qr()"
    },
    {
        "Y": "Tensor.qscheme",
        "X": "What returns the quantization scheme of a given QTensor?",
        "Z": "Tensor.qscheme Returns the quantization scheme of a given QTensor."
    },
    {
        "Y": "QTensor",
        "X": "Tensor.qscheme returns the quantization scheme of a given what?",
        "Z": "Tensor.qscheme Returns the quantization scheme of a given QTensor."
    },
    {
        "Y": "Tensor.quantile Seetorch.quantile()",
        "X": "What does Tensor.quantile Seetorch.quantile() do?",
        "Z": "Tensor.quantile Seetorch.quantile()"
    },
    {
        "Y": "Tensor.quantile",
        "X": "What is Seetorch.quantile()?",
        "Z": "Tensor.quantile Seetorch.quantile()"
    },
    {
        "Y": "Tensor.nanquantile",
        "X": "What does Seetorch.nanquantile do?",
        "Z": "Tensor.nanquantile Seetorch.nanquantile()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.nanquantile?",
        "Z": "Tensor.nanquantile Seetorch.nanquantile()"
    },
    {
        "Y": "linear(affine) quantization",
        "X": "How is a Tensor quantized?",
        "Z": "Tensor.q_scale Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."
    },
    {
        "Y": "Tensor.q_scale",
        "X": "What returns the scale of the underlying quantizer()?",
        "Z": "Tensor.q_scale Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer()."
    },
    {
        "Y": "zero_point",
        "X": "What does Tensor.q_ return?",
        "Z": "Tensor.q_zero_point Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."
    },
    {
        "Y": "linear(affine) quantization",
        "X": "What is a Tensor quantized by?",
        "Z": "Tensor.q_zero_point Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."
    },
    {
        "Y": "zero_point",
        "X": "What is the value of Tensor.q_?",
        "Z": "Tensor.q_zero_point Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer()."
    },
    {
        "Y": "linear",
        "X": "What is an affine quantization of a Tensor?",
        "Z": "Tensor.q_per_channel_scales Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer."
    },
    {
        "Y": "linear",
        "X": "What is the term for affine per-channel quantization?",
        "Z": "Tensor.q_per_channel_zero_points Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer."
    },
    {
        "Y": "zero",
        "X": "What is the tensor of the underlying quantizer?",
        "Z": "Tensor.q_per_channel_zero_points Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer."
    },
    {
        "Y": "linear",
        "X": "What is an affine per-channel quantization?",
        "Z": "Tensor.q_per_channel_zero_points Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer."
    },
    {
        "Y": "affine",
        "X": "What is another term for linear quantization?",
        "Z": "Tensor.q_per_channel_zero_points Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer."
    },
    {
        "Y": "linear",
        "X": "Tensor.q_per_channel_axis Given a Tensor quantized by what type of per-channel quantization?",
        "Z": "Tensor.q_per_channel_axis Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."
    },
    {
        "Y": "index of dimension",
        "X": "What does Tensor.q_per_channel_axis return?",
        "Z": "Tensor.q_per_channel_axis Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."
    },
    {
        "Y": "linear",
        "X": "Tensor.q_per_channel_axis Given a Tensor quantized by what?",
        "Z": "Tensor.q_per_channel_axis Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."
    },
    {
        "Y": "Tensor.q_per_channel_axis",
        "X": "What returns the index of dimension on which per-channel quantization is applied?",
        "Z": "Tensor.q_per_channel_axis Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied."
    },
    {
        "Y": "Tensor.rad2deg",
        "X": "What does Seetorch.rad2deg do?",
        "Z": "Tensor.rad2deg Seetorch.rad2deg()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.rad2deg?",
        "Z": "Tensor.rad2deg Seetorch.rad2deg()"
    },
    {
        "Y": "Tensor.rad2deg",
        "X": "What is Seetorch.rad2deg?",
        "Z": "Tensor.rad2deg Seetorch.rad2deg()"
    },
    {
        "Y": "uniform distribution",
        "X": "Tensor.random_ Fillsselftensor with numbers sampled from what kind of distribution?",
        "Z": "Tensor.random_ Fillsselftensor with numbers sampled from the discrete uniform distribution over[from,to-1]."
    },
    {
        "Y": "Tensor.random",
        "X": "What type of tensor is filled with numbers sampled from the discrete uniform distribution over[from,to-1]?",
        "Z": "Tensor.random_ Fillsselftensor with numbers sampled from the discrete uniform distribution over[from,to-1]."
    },
    {
        "Y": "Tensor.ravel",
        "X": "What does seetorch.ravel do?",
        "Z": "Tensor.ravel seetorch.ravel()"
    },
    {
        "Y": "seetorch",
        "X": "What is Tensor.ravel?",
        "Z": "Tensor.ravel seetorch.ravel()"
    },
    {
        "Y": "Tensor",
        "X": "What is reciprocal Seetorch?",
        "Z": "Tensor.reciprocal Seetorch.reciprocal()"
    },
    {
        "Y": "Seetorch",
        "X": "What is a Tensor.reciprocal?",
        "Z": "Tensor.reciprocal Seetorch.reciprocal()"
    },
    {
        "Y": "Tensor.reciprocal_ In-place version ofreciprocal()",
        "X": "What is reciprocal function?",
        "Z": "Tensor.reciprocal_ In-place version ofreciprocal()"
    },
    {
        "Y": "Tensor.reciprocal_ In-place",
        "X": "What is version ofreciprocal()?",
        "Z": "Tensor.reciprocal_ In-place version ofreciprocal()"
    },
    {
        "Y": "all current work queued onstreamare complete",
        "X": "When is the tensor memory not reused for another tensor?",
        "Z": "Tensor.record_stream Ensures that the tensor memory is not reused for another tensor until all current work queued onstreamare complete."
    },
    {
        "Y": "Tensor.record_stream",
        "X": "What ensures that the tensor memory is not reused for another tensor?",
        "Z": "Tensor.record_stream Ensures that the tensor memory is not reused for another tensor until all current work queued onstreamare complete."
    },
    {
        "Y": "backward hook",
        "X": "What type of hook does Tensor register?",
        "Z": "Tensor.register_hook Registers a backward hook."
    },
    {
        "Y": "Tensor.register_hook",
        "X": "What registers a backward hook?",
        "Z": "Tensor.register_hook Registers a backward hook."
    },
    {
        "Y": "Seetorch.remainder()",
        "X": "What does Tensor.remainder do?",
        "Z": "Tensor.remainder Seetorch.remainder()"
    },
    {
        "Y": "Tensor.remainder",
        "X": "What does Seetorch.remainder do?",
        "Z": "Tensor.remainder Seetorch.remainder()"
    },
    {
        "Y": "Seetorch.remainder",
        "X": "What is Tensor.remainder?",
        "Z": "Tensor.remainder Seetorch.remainder()"
    },
    {
        "Y": "Tensor.remainder",
        "X": "What is Seetorch.remainder?",
        "Z": "Tensor.remainder Seetorch.remainder()"
    },
    {
        "Y": "Tensor.remainder_ In-place version ofremainder()",
        "X": "What is in-place version ofmainder()?",
        "Z": "Tensor.remainder_ In-place version ofremainder()"
    },
    {
        "Y": "Tensor.remainder_ In-place",
        "X": "What is version ofremainder()?",
        "Z": "Tensor.remainder_ In-place version ofremainder()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.renorm?",
        "Z": "Tensor.renorm Seetorch.renorm()"
    },
    {
        "Y": "Tensor.renorm",
        "X": "What is Seetorch.renorm?",
        "Z": "Tensor.renorm Seetorch.renorm()"
    },
    {
        "Y": "Tensor.renorm_ In-place version ofrenorm()",
        "X": "What is in-place version ofrenorm?",
        "Z": "Tensor.renorm_ In-place version ofrenorm()"
    },
    {
        "Y": "Tensor.renorm",
        "X": "What is In-place version ofrenorm?",
        "Z": "Tensor.renorm_ In-place version ofrenorm()"
    },
    {
        "Y": "Tensor",
        "X": "What repeats the tensor along the specified dimensions?",
        "Z": "Tensor.repeat Repeats this tensor along the specified dimensions."
    },
    {
        "Y": "Tensor",
        "X": "What is tensor that repeats along the specified dimensions?",
        "Z": "Tensor.repeat Repeats this tensor along the specified dimensions."
    },
    {
        "Y": "Tensor",
        "X": "What is component that performs the repeat interleave of Seetorch?",
        "Z": "Tensor.repeat_interleave Seetorch.repeat_interleave()."
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.repeat_interleave?",
        "Z": "Tensor.repeat_interleave Seetorch.repeat_interleave()."
    },
    {
        "Y": "Tensor",
        "X": "What is.repeat_interleave Seetorch.repeat_interleave()?",
        "Z": "Tensor.repeat_interleave Seetorch.repeat_interleave()."
    },
    {
        "Y": "is True if",
        "X": "What type of gradients need to be computed for the Tensor?",
        "Z": "Tensor.requires_grad Is True if gradients need to be computed for this Tensor,False otherwise."
    },
    {
        "Y": "autograd",
        "X": "What should record operations on this tensor?",
        "Z": "Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place."
    },
    {
        "Y": "sets this tensor\u2019srequires_gradattribute in-place",
        "X": "How does autograd record operations on a tensor?",
        "Z": "Tensor.requires_grad_ Change if autograd should record operations on this tensor: sets this tensor\u2019srequires_gradattribute in-place."
    },
    {
        "Y": "the specified shape",
        "X": "Tensor.reshape Returns a tensor with the same data and number of elements as selfbut with what?",
        "Z": "Tensor.reshape Returns a tensor with the same data and number of elements asselfbut with the specified shape."
    },
    {
        "Y": "Tensor.reshape",
        "X": "What returns a tensor with the same data and number of elements asselfbut with the specified shape?",
        "Z": "Tensor.reshape Returns a tensor with the same data and number of elements asselfbut with the specified shape."
    },
    {
        "Y": "same shape",
        "X": "What shape is returned to the tensor?",
        "Z": "Tensor.reshape_as Returns this tensor as the same shape asother."
    },
    {
        "Y": "Tensor.reshape_as",
        "X": "What returns this tensor as the same shape asother?",
        "Z": "Tensor.reshape_as Returns this tensor as the same shape asother."
    },
    {
        "Y": "Tensor.resize_ Resizesselftensor to the specified size",
        "X": "What does Tensor.resize_ do?",
        "Z": "Tensor.resize_ Resizesselftensor to the specified size."
    },
    {
        "Y": "Tensor.resize",
        "X": "What _ Resizesselftensor to the specified size?",
        "Z": "Tensor.resize_ Resizesselftensor to the specified size."
    },
    {
        "Y": "the same size as the specifiedtensor",
        "X": "What does Tensor.resize_as_ Resize the self tensor to be?",
        "Z": "Tensor.resize_as_ Resizes the self tensor to be the same size as the specifiedtensor."
    },
    {
        "Y": "Tensor.resize_as",
        "X": "What resizes the self tensor to be the same size as the specifiedtensor?",
        "Z": "Tensor.resize_as_ Resizes the self tensor to be the same size as the specifiedtensor."
    },
    {
        "Y": "non-leaf Tensors",
        "X": "Tensor.retain_grad Enables.grad attribute for what?",
        "Z": "Tensor.retain_grad Enables .grad attribute for non-leaf Tensors."
    },
    {
        "Y": "non-leaf",
        "X": "Tensor.retain_grad Enables.grad attribute for what type of Tensors?",
        "Z": "Tensor.retain_grad Enables .grad attribute for non-leaf Tensors."
    },
    {
        "Y": "torch.Tensor.retain_grad()",
        "X": "What Enables.grad attribute for non-leaf Tensors?",
        "Z": "torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors."
    },
    {
        "Y": "Tensor.roll Seetorch.roll",
        "X": "What is Tensor.roll Seetorch.roll?",
        "Z": "Tensor.roll Seetorch.roll()"
    },
    {
        "Y": "Tensor.roll Seetorch.roll()",
        "X": "What is Tensor.roll Seetorch.roll()?",
        "Z": "Tensor.roll Seetorch.roll()"
    },
    {
        "Y": "Tensor",
        "X": "What is rot90 Seetorch.rot90?",
        "Z": "Tensor.rot90 Seetorch.rot90()"
    },
    {
        "Y": "Seetorch.rot90",
        "X": "What is Tensor.rot90?",
        "Z": "Tensor.rot90 Seetorch.rot90()"
    },
    {
        "Y": "Tensor.rot90",
        "X": "What is Seetorch.rot90?",
        "Z": "Tensor.rot90 Seetorch.rot90()"
    },
    {
        "Y": "Tensor.round Seetorch.round",
        "X": "What is Tensor.round Seetorch.round?",
        "Z": "Tensor.round Seetorch.round()"
    },
    {
        "Y": "Seetorch.round",
        "X": "What is Tensor.round?",
        "Z": "Tensor.round Seetorch.round()"
    },
    {
        "Y": "Tensor.round_ In-place version ofround()",
        "X": "What is In-place version ofround()?",
        "Z": "Tensor.round_ In-place version ofround()"
    },
    {
        "Y": "Tensor.round",
        "X": "What is In-place version ofround?",
        "Z": "Tensor.round_ In-place version ofround()"
    },
    {
        "Y": "Tensor.rsqrt Seetorch.rsqrt()",
        "X": "What does Tensor.rsqrt do?",
        "Z": "Tensor.rsqrt Seetorch.rsqrt()"
    },
    {
        "Y": "Seetorch.rsqrt",
        "X": "What is Tensor.rsqrt?",
        "Z": "Tensor.rsqrt Seetorch.rsqrt()"
    },
    {
        "Y": "Tensor.rsqrt",
        "X": "What is Seetorch.rsqrt?",
        "Z": "Tensor.rsqrt Seetorch.rsqrt()"
    },
    {
        "Y": "Tensor.rsqrt_ In-place version ofrsqrt()",
        "X": "What is in-place version of ofrsqrt?",
        "Z": "Tensor.rsqrt_ In-place version ofrsqrt()"
    },
    {
        "Y": "Tensor.rsqrt",
        "X": "What is In-place version of ofrsqrt()?",
        "Z": "Tensor.rsqrt_ In-place version ofrsqrt()"
    },
    {
        "Y": "Tensor.scatter Out-of-place",
        "X": "What is the version of of torch.Tensor.scatter_()?",
        "Z": "Tensor.scatter Out-of-place version of torch.Tensor.scatter_()"
    },
    {
        "Y": "Tensor.scatter Out-of-place",
        "X": "What is version of of torch.Tensor.scatter_()?",
        "Z": "Tensor.scatter Out-of-place version of torch.Tensor.scatter_()"
    },
    {
        "Y": "theindextensor",
        "X": "Where are the indices specified?",
        "Z": "Tensor.scatter_ Writes all values from the tensorsrcintoselfat the indices specified in theindextensor."
    },
    {
        "Y": "all values",
        "X": "What does Tensor.scatter_ write from the tensorsrcintoselfat the indices specified in the",
        "Z": "Tensor.scatter_ Writes all values from the tensorsrcintoselfat the indices specified in theindextensor."
    },
    {
        "Y": "Tensor.scatter",
        "X": "What _ Writes all values from the tensorsrcintoselfat the indices specified in the indextensor",
        "Z": "Tensor.scatter_ Writes all values from the tensorsrcintoselfat the indices specified in theindextensor."
    },
    {
        "Y": "asscatter_()",
        "X": "How does Tensor.scatter_add_ add all values from the tensorotherintoselfat the indices specified",
        "Z": "Tensor.scatter_add_ Adds all values from the tensorotherintoselfat the indices specified in theindextensor in a similar fashion asscatter_()."
    },
    {
        "Y": "asscatter_()",
        "X": "How does Tensor.scatter_add_ Add all values from the tensorotherintoselfat the indices specified",
        "Z": "Tensor.scatter_add_ Adds all values from the tensorotherintoselfat the indices specified in theindextensor in a similar fashion asscatter_()."
    },
    {
        "Y": "Tensor.scatter_add Out-of-place",
        "X": "What is the version of of torch.Tensor.scatter_add_()?",
        "Z": "Tensor.scatter_add Out-of-place version of torch.Tensor.scatter_add_()"
    },
    {
        "Y": "Slices the self tensor along the selected dimension at the given index",
        "X": "What does Tensor.select do?",
        "Z": "Tensor.select Slices the self tensor along the selected dimension at the given index."
    },
    {
        "Y": "Slices the self tensor",
        "X": "What does Tensor.select along the selected dimension at the given index?",
        "Z": "Tensor.select Slices the self tensor along the selected dimension at the given index."
    },
    {
        "Y": "Tensor.set_",
        "X": "What sets the underlying storage, size, and strides?",
        "Z": "Tensor.set_ Sets the underlying storage, size, and strides."
    },
    {
        "Y": "Tensor.share_memory",
        "X": "What moves the underlying storage to shared memory?",
        "Z": "Tensor.share_memory_ Moves the underlying storage to shared memory."
    },
    {
        "Y": "Tensor.share_memory",
        "X": "What _ Moves the underlying storage to shared memory?",
        "Z": "Tensor.share_memory_ Moves the underlying storage to shared memory."
    },
    {
        "Y": "self.short",
        "X": "What is equivalent to self.to(torch.int16)?",
        "Z": "Tensor.short self.short()is equivalent toself.to(torch.int16)."
    },
    {
        "Y": "self.to",
        "X": "What is tensor.short self.short() equivalent to?",
        "Z": "Tensor.short self.short()is equivalent toself.to(torch.int16)."
    },
    {
        "Y": "Tensor",
        "X": "What is sigmoid Seetorch?",
        "Z": "Tensor.sigmoid Seetorch.sigmoid()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.sigmoid?",
        "Z": "Tensor.sigmoid Seetorch.sigmoid()"
    },
    {
        "Y": "Tensor.sigmoid_ In-place version ofsigmoid()",
        "X": "What is In-place version ofsigmoid()?",
        "Z": "Tensor.sigmoid_ In-place version ofsigmoid()"
    },
    {
        "Y": "Tensor.sigmoid_ In-place",
        "X": "What version ofsigmoid() does Tensor.sigmoid_ In-place?",
        "Z": "Tensor.sigmoid_ In-place version ofsigmoid()"
    },
    {
        "Y": "Tensor",
        "X": "What does Seetorch.sign() stand for?",
        "Z": "Tensor.sign Seetorch.sign()"
    },
    {
        "Y": "Tensor.sign Seetorch.sign()",
        "X": "What does Tensor.sign Seetorch.sign() do?",
        "Z": "Tensor.sign Seetorch.sign()"
    },
    {
        "Y": "Tensor.sign_ In-place version ofsign()",
        "X": "What does Tensor.sign_ In-place version ofsign() do?",
        "Z": "Tensor.sign_ In-place version ofsign()"
    },
    {
        "Y": "Tensor.sign_ In-place",
        "X": "What is version ofsign()?",
        "Z": "Tensor.sign_ In-place version ofsign()"
    },
    {
        "Y": "Tensor",
        "X": "What is.signbit Seetorch.signbit() function?",
        "Z": "Tensor.signbit Seetorch.signbit()"
    },
    {
        "Y": "Seetorch.signbit()",
        "X": "What does Tensor.signbit do?",
        "Z": "Tensor.signbit Seetorch.signbit()"
    },
    {
        "Y": "Tensor.signbit Seetorch.signbit()",
        "X": "What does Tensor.signbit Seetorch.signbit() do?",
        "Z": "Tensor.signbit Seetorch.signbit()"
    },
    {
        "Y": "Seetorch.sgn",
        "X": "What is Tensor.sgn?",
        "Z": "Tensor.sgn Seetorch.sgn()"
    },
    {
        "Y": "Tensor.sgn",
        "X": "What is Seetorch.sgn?",
        "Z": "Tensor.sgn Seetorch.sgn()"
    },
    {
        "Y": "Tensor.sgn_ In-place version ofsgn()",
        "X": "What is function used by Tensor.sgn?",
        "Z": "Tensor.sgn_ In-place version ofsgn()"
    },
    {
        "Y": "Tensor.sgn_ In-place",
        "X": "What is version ofsgn()?",
        "Z": "Tensor.sgn_ In-place version ofsgn()"
    },
    {
        "Y": "Tensor.sin",
        "X": "What is Seetorch.sin?",
        "Z": "Tensor.sin Seetorch.sin()"
    },
    {
        "Y": "Tensor.sin",
        "X": "What is Seetorch.sin?",
        "Z": "Tensor.sin Seetorch.sin()"
    },
    {
        "Y": "Tensor.sin_ In-place version ofsin()",
        "X": "What is In-place version ofsin()?",
        "Z": "Tensor.sin_ In-place version ofsin()"
    },
    {
        "Y": "Tensor.sin_ In-place",
        "X": "What is version ofsin()?",
        "Z": "Tensor.sin_ In-place version ofsin()"
    },
    {
        "Y": "Seetorch.sinc",
        "X": "What is Tensor.sinc?",
        "Z": "Tensor.sinc Seetorch.sinc()"
    },
    {
        "Y": "Tensor.sinc",
        "X": "What is Seetorch.sinc?",
        "Z": "Tensor.sinc Seetorch.sinc()"
    },
    {
        "Y": "Tensor.sinc_ In-place version ofsinc()",
        "X": "What does Tensor.sinc_ In-place version ofsinc() do?",
        "Z": "Tensor.sinc_ In-place version ofsinc()"
    },
    {
        "Y": "Tensor.sinc",
        "X": "What is In-place version ofsinc()?",
        "Z": "Tensor.sinc_ In-place version ofsinc()"
    },
    {
        "Y": "Seetorch.sinh",
        "X": "What is Tensor.sinh?",
        "Z": "Tensor.sinh Seetorch.sinh()"
    },
    {
        "Y": "Tensor.sinh",
        "X": "What is Seetorch.sinh function?",
        "Z": "Tensor.sinh Seetorch.sinh()"
    },
    {
        "Y": "Tensor.sinh_ In-place version ofsinh()",
        "X": "What does Tensor.sinh_ In-place version ofsinh() do?",
        "Z": "Tensor.sinh_ In-place version ofsinh()"
    },
    {
        "Y": "Tensor.sinh_ In-place",
        "X": "What is version ofsinh()?",
        "Z": "Tensor.sinh_ In-place version ofsinh()"
    },
    {
        "Y": "Seetorch.asinh",
        "X": "What is Tensor.asinh?",
        "Z": "Tensor.asinh Seetorch.asinh()"
    },
    {
        "Y": "Tensor.asinh",
        "X": "What is Seetorch.asinh?",
        "Z": "Tensor.asinh Seetorch.asinh()"
    },
    {
        "Y": "Tensor.asinh_ In-place version ofasinh()",
        "X": "What does Tensor.asinh do?",
        "Z": "Tensor.asinh_ In-place version ofasinh()"
    },
    {
        "Y": "Tensor.asinh",
        "X": "What is In-place version ofasinh()?",
        "Z": "Tensor.asinh_ In-place version ofasinh()"
    },
    {
        "Y": "Seetorch.arcsinh",
        "X": "What is Tensor.arcsinh?",
        "Z": "Tensor.arcsinh Seetorch.arcsinh()"
    },
    {
        "Y": "Tensor.arcsinh",
        "X": "What is Seetorch.arcsinh function?",
        "Z": "Tensor.arcsinh Seetorch.arcsinh()"
    },
    {
        "Y": "Tensor.arcsinh_ In-place version ofarcsinh()",
        "X": "What is in-place version of arcsinh?",
        "Z": "Tensor.arcsinh_ In-place version ofarcsinh()"
    },
    {
        "Y": "Tensor.arcsinh",
        "X": "What is In-place version ofarcsinh()?",
        "Z": "Tensor.arcsinh_ In-place version ofarcsinh()"
    },
    {
        "Y": "Tensor",
        "X": "What returns the size of the selftensor?",
        "Z": "Tensor.size Returns the size of the self tensor."
    },
    {
        "Y": "the size of the self tensor",
        "X": "What does Tensor.size return?",
        "Z": "Tensor.size Returns the size of the self tensor."
    },
    {
        "Y": "Tensor.size",
        "X": "What returns the size of the self tensor?",
        "Z": "Tensor.size Returns the size of the self tensor."
    },
    {
        "Y": "Tensor.slogdet",
        "X": "What does Seetorch.slogdet do?",
        "Z": "Tensor.slogdet Seetorch.slogdet()"
    },
    {
        "Y": "Seetorch.slogdet",
        "X": "What does Tensor.slogdet stand for?",
        "Z": "Tensor.slogdet Seetorch.slogdet()"
    },
    {
        "Y": "Seetorch.slogdet",
        "X": "What is Tensor.slogdet?",
        "Z": "Tensor.slogdet Seetorch.slogdet()"
    },
    {
        "Y": "Tensor.slogdet",
        "X": "What is Seetorch.slogdet() function?",
        "Z": "Tensor.slogdet Seetorch.slogdet()"
    },
    {
        "Y": "Tensor.solve",
        "X": "What does Seetorch.solve() do?",
        "Z": "Tensor.solve Seetorch.solve()"
    },
    {
        "Y": "Seetorch.solve",
        "X": "What is Tensor.solve?",
        "Z": "Tensor.solve Seetorch.solve()"
    },
    {
        "Y": "Tensor",
        "X": "What is.sort Seetorch.sort() function?",
        "Z": "Tensor.sort Seetorch.sort()"
    },
    {
        "Y": "Tensor.sort Seetorch.sort()",
        "X": "What does Tensor.sort Seetorch.sort() do?",
        "Z": "Tensor.sort Seetorch.sort()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that splits a Seetorch?",
        "Z": "Tensor.split Seetorch.split()"
    },
    {
        "Y": "Seetorch.split",
        "X": "What is Tensor.split?",
        "Z": "Tensor.split Seetorch.split()"
    },
    {
        "Y": "Tensor.split",
        "X": "What is Seetorch.split() function?",
        "Z": "Tensor.split Seetorch.split()"
    },
    {
        "Y": "indices of the sparse tensormask",
        "X": "What is used to filter the strided tensor?",
        "Z": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask."
    },
    {
        "Y": "newsparse tensor",
        "X": "What type of tensor does Tensor.sparse_mask return?",
        "Z": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask."
    },
    {
        "Y": "indices of the sparse tensormask",
        "X": "What does Tensor.sparse_mask return?",
        "Z": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask."
    },
    {
        "Y": "sparse dimensions",
        "X": "What is the number of in the asparse tensorself?",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself."
    },
    {
        "Y": "Tensor.sparse_dim",
        "X": "What returns the number of sparse dimensions in asparse tensorself?",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself."
    },
    {
        "Y": "Tensor.sqrt Seetorch.sqrt()",
        "X": "What does Tensor.sqrt do?",
        "Z": "Tensor.sqrt Seetorch.sqrt()"
    },
    {
        "Y": "Seetorch.sqrt",
        "X": "What is Tensor.sqrt?",
        "Z": "Tensor.sqrt Seetorch.sqrt()"
    },
    {
        "Y": "Tensor.sqrt_ In-place version ofsqrt()",
        "X": "What is in-place version ofsqrt()?",
        "Z": "Tensor.sqrt_ In-place version ofsqrt()"
    },
    {
        "Y": "Tensor.sqrt_ In-place",
        "X": "What is version ofsqrt()?",
        "Z": "Tensor.sqrt_ In-place version ofsqrt()"
    },
    {
        "Y": "Tensor.square Seetorch.square",
        "X": "What is Tensor.square Seetorch.square?",
        "Z": "Tensor.square Seetorch.square()"
    },
    {
        "Y": "Tensor.square Seetorch.square()",
        "X": "What is Tensor.square Seetorch.square()?",
        "Z": "Tensor.square Seetorch.square()"
    },
    {
        "Y": "Tensor.square_ In-place version ofsquare()",
        "X": "What is In-place version of square()?",
        "Z": "Tensor.square_ In-place version ofsquare()"
    },
    {
        "Y": "Tensor.square_ In-place",
        "X": "What is version of square()?",
        "Z": "Tensor.square_ In-place version ofsquare()"
    },
    {
        "Y": "Tensor.squeeze",
        "X": "What is Seetorch.squeeze() function?",
        "Z": "Tensor.squeeze Seetorch.squeeze()"
    },
    {
        "Y": "Tensor.squeeze_ In-place version ofsqueeze()",
        "X": "What does Tensor.squeeze_ In-place version ofsqueeze() do?",
        "Z": "Tensor.squeeze_ In-place version ofsqueeze()"
    },
    {
        "Y": "Tensor.squeeze_ In-place",
        "X": "What is version ofsqueeze()?",
        "Z": "Tensor.squeeze_ In-place version ofsqueeze()"
    },
    {
        "Y": "Seetorch.std",
        "X": "What is Tensor.std?",
        "Z": "Tensor.std Seetorch.std()"
    },
    {
        "Y": "Seetorch.stft",
        "X": "What is Tensor.stft?",
        "Z": "Tensor.stft Seetorch.stft()"
    },
    {
        "Y": "Tensor.stft",
        "X": "What is Seetorch.stft?",
        "Z": "Tensor.stft Seetorch.stft()"
    },
    {
        "Y": "storage",
        "X": "What returns the underlying storage?",
        "Z": "Tensor.storage Returns the underlying storage."
    },
    {
        "Y": "Tensor",
        "X": "What type of storage returns the underlying storage?",
        "Z": "Tensor.storage Returns the underlying storage."
    },
    {
        "Y": "number of storage elements",
        "X": "Tensor.storage_offset Returns selftensor's offset in the underlying storage in terms of what?",
        "Z": "Tensor.storage_offset Returnsselftensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes)."
    },
    {
        "Y": "bytes",
        "X": "What is the offset in selftensor's underlying storage?",
        "Z": "Tensor.storage_offset Returnsselftensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes)."
    },
    {
        "Y": "Tensor.storage_type",
        "X": "What returns the type of the underlying storage?",
        "Z": "Tensor.storage_type Returns the type of the underlying storage."
    },
    {
        "Y": "stride",
        "X": "What returns the stride of selftensor?",
        "Z": "Tensor.stride Returns the stride ofselftensor."
    },
    {
        "Y": "Tensor",
        "X": "What is sub Seetorch.sub()?",
        "Z": "Tensor.sub Seetorch.sub()."
    },
    {
        "Y": "Tensor.sub",
        "X": "What is the name of Seetorch.sub()?",
        "Z": "Tensor.sub Seetorch.sub()."
    },
    {
        "Y": "Tensor.sub_ In-place",
        "X": "What version of sub() does Tensor.sub_ In-place?",
        "Z": "Tensor.sub_ In-place version ofsub()"
    },
    {
        "Y": "Tensor",
        "X": "What does Seetorch.subtract() do?",
        "Z": "Tensor.subtract Seetorch.subtract()."
    },
    {
        "Y": "Seetorch.subtract",
        "X": "What does Tensor.subtract do?",
        "Z": "Tensor.subtract Seetorch.subtract()."
    },
    {
        "Y": "Tensor",
        "X": "What is the term for Seetorch?",
        "Z": "Tensor.subtract Seetorch.subtract()."
    },
    {
        "Y": "Tensor.subtract_ In-place version ofsubtract()",
        "X": "What does Tensor.subtract_ In-place version ofsubtract() do?",
        "Z": "Tensor.subtract_ In-place version ofsubtract()."
    },
    {
        "Y": "Tensor.subtract_ In-place",
        "X": "What is version of subtract()?",
        "Z": "Tensor.subtract_ In-place version ofsubtract()."
    },
    {
        "Y": "Tensor.sum Seetorch.sum()",
        "X": "What does Tensor.sum Seetorch.sum do?",
        "Z": "Tensor.sum Seetorch.sum()"
    },
    {
        "Y": "Tensor.sum Seetorch.sum",
        "X": "What is the Tensor.sum Seetorch.sum?",
        "Z": "Tensor.sum Seetorch.sum()"
    },
    {
        "Y": "Tensor",
        "X": "What is a Sumthistensor?",
        "Z": "Tensor.sum_to_size Sumthistensor tosize."
    },
    {
        "Y": "Sumthistensor",
        "X": "What is a Tensor.sum_to_size?",
        "Z": "Tensor.sum_to_size Sumthistensor tosize."
    },
    {
        "Y": "Sumthistensor",
        "X": "Tosize what is a Tensor.sum_to_size?",
        "Z": "Tensor.sum_to_size Sumthistensor tosize."
    },
    {
        "Y": "Tensor.sum_to_size",
        "X": "What is the Sumthistensor tosize?",
        "Z": "Tensor.sum_to_size Sumthistensor tosize."
    },
    {
        "Y": "Seetorch.svd",
        "X": "What is Tensor.svd?",
        "Z": "Tensor.svd Seetorch.svd()"
    },
    {
        "Y": "Tensor.svd",
        "X": "What is Seetorch.svd?",
        "Z": "Tensor.svd Seetorch.svd()"
    },
    {
        "Y": "Tensor.swapaxes",
        "X": "What does Seetorch.swapaxes do?",
        "Z": "Tensor.swapaxes Seetorch.swapaxes()"
    },
    {
        "Y": "Seetorch.swapaxes",
        "X": "What is Tensor.swapaxes?",
        "Z": "Tensor.swapaxes Seetorch.swapaxes()"
    },
    {
        "Y": "Tensor.swapdims",
        "X": "What does Seetorch.swapdims do?",
        "Z": "Tensor.swapdims Seetorch.swapdims()"
    },
    {
        "Y": "Seetorch.swapdims",
        "X": "What is Tensor.swapdims?",
        "Z": "Tensor.swapdims Seetorch.swapdims()"
    },
    {
        "Y": "Seetorch.symeig",
        "X": "What is Tensor.symeig?",
        "Z": "Tensor.symeig Seetorch.symeig()"
    },
    {
        "Y": "Tensor.symeig",
        "X": "What is Seetorch.symeig?",
        "Z": "Tensor.symeig Seetorch.symeig()"
    },
    {
        "Y": "Tensor.t Seetorch.t()",
        "X": "What does Tensor.t Seetorch.t do?",
        "Z": "Tensor.t Seetorch.t()"
    },
    {
        "Y": "Tensor.t Seetorch.t",
        "X": "What is Tensor.t Seetorch?",
        "Z": "Tensor.t Seetorch.t()"
    },
    {
        "Y": "Tensor.t Seetorch.t",
        "X": "What is Tensor.t Seetorch.t?",
        "Z": "Tensor.t Seetorch.t()"
    },
    {
        "Y": "Tensor.t_ In-place",
        "X": "What is version of oft?",
        "Z": "Tensor.t_ In-place version oft()"
    },
    {
        "Y": "Seetorch.tensor_split",
        "X": "What is Tensor.tensor_split?",
        "Z": "Tensor.tensor_split Seetorch.tensor_split()"
    },
    {
        "Y": "Tensor.tensor_split",
        "X": "What does Seetorch.tensor_split call?",
        "Z": "Tensor.tensor_split Seetorch.tensor_split()"
    },
    {
        "Y": "Tensor.tile Seetorch.tile()",
        "X": "What is function that is used to determine the type of tile?",
        "Z": "Tensor.tile Seetorch.tile()"
    },
    {
        "Y": "Seetorch.tile",
        "X": "What is Tensor.tile?",
        "Z": "Tensor.tile Seetorch.tile()"
    },
    {
        "Y": "Tensor.tile",
        "X": "What is Seetorch.tile?",
        "Z": "Tensor.tile Seetorch.tile()"
    },
    {
        "Y": "Tensor.to",
        "X": "What performs Tensor dtype and/or device conversion?",
        "Z": "Tensor.to Performs Tensor dtype and/or device conversion."
    },
    {
        "Y": "Tensor dtype and/or device conversion",
        "X": "What does Tensor.to perform?",
        "Z": "Tensor.to Performs Tensor dtype and/or device conversion."
    },
    {
        "Y": "intorch.mkldnnlayout",
        "X": "What does Tensor.to_mkldnn return a copy of?",
        "Z": "Tensor.to_mkldnn Returns a copy of the tensor intorch.mkldnnlayout."
    },
    {
        "Y": "a copy",
        "X": "What does Tensor.to_mkldnn return?",
        "Z": "Tensor.to_mkldnn Returns a copy of the tensor intorch.mkldnnlayout."
    },
    {
        "Y": "Tensor",
        "X": "What is function that takes Seetorch.take()?",
        "Z": "Tensor.take Seetorch.take()"
    },
    {
        "Y": "Tensor.take Seetorch.take",
        "X": "What is Tensor.take Seetorch.take?",
        "Z": "Tensor.take Seetorch.take()"
    },
    {
        "Y": "Tensor",
        "X": "Take_along_dim Seetorch.take_along_dim() what?",
        "Z": "Tensor.take_along_dim Seetorch.take_along_dim()"
    },
    {
        "Y": "Tensor.tan Seetorch.tan",
        "X": "What is Tensor.tan Seetorch.tan?",
        "Z": "Tensor.tan Seetorch.tan()"
    },
    {
        "Y": "Tensor.tan Seetorch.tan",
        "X": "What is Tensor.tan?",
        "Z": "Tensor.tan Seetorch.tan()"
    },
    {
        "Y": "Tensor.tan_ In-place version oftan()",
        "X": "What is Tensor.tan_ In-place version of oftan?",
        "Z": "Tensor.tan_ In-place version oftan()"
    },
    {
        "Y": "Tensor.tan_ In-place",
        "X": "What is version of oftan?",
        "Z": "Tensor.tan_ In-place version oftan()"
    },
    {
        "Y": "Tensor.tanh Seetorch.tanh",
        "X": "What is Tensor.tanh Seetorch.tanh?",
        "Z": "Tensor.tanh Seetorch.tanh()"
    },
    {
        "Y": "Seetorch.tanh",
        "X": "What is Tensor.tanh?",
        "Z": "Tensor.tanh Seetorch.tanh()"
    },
    {
        "Y": "Tensor.tanh",
        "X": "What is Seetorch.tanh?",
        "Z": "Tensor.tanh Seetorch.tanh()"
    },
    {
        "Y": "Tensor.tanh_ In-place version oftanh()",
        "X": "What does Tensor.tanh_ In-place version of oftanh do?",
        "Z": "Tensor.tanh_ In-place version oftanh()"
    },
    {
        "Y": "Tensor.tanh",
        "X": "What is In-place version of oftanh()?",
        "Z": "Tensor.tanh_ In-place version oftanh()"
    },
    {
        "Y": "Tensor.atanh Seetorch.atanh",
        "X": "What is Tensor.atanh Seetorch.atanh?",
        "Z": "Tensor.atanh Seetorch.atanh()"
    },
    {
        "Y": "Tensor.atanh_ In-place version ofatanh()",
        "X": "What is in-place version of atanh?",
        "Z": "Tensor.atanh_ In-place version ofatanh()"
    },
    {
        "Y": "Tensor.atanh",
        "X": "What is In-place version of atanh()?",
        "Z": "Tensor.atanh_ In-place version ofatanh()"
    },
    {
        "Y": "Seetorch",
        "X": "What is Tensor.arctanh?",
        "Z": "Tensor.arctanh Seetorch.arctanh()"
    },
    {
        "Y": "Tensor.arctanh",
        "X": "What is Seetorch.arctanh function?",
        "Z": "Tensor.arctanh Seetorch.arctanh()"
    },
    {
        "Y": "Tensor.arctanh_ In-place version ofarctanh()",
        "X": "What is in-place version of arctanh?",
        "Z": "Tensor.arctanh_ In-place version ofarctanh()"
    },
    {
        "Y": "Tensor.arctanh",
        "X": "What is In-place version ofarctanh?",
        "Z": "Tensor.arctanh_ In-place version ofarctanh()"
    },
    {
        "Y": "Tensor",
        "X": "What returns the tensor as a list?",
        "Z": "Tensor.tolist Returns the tensor as a (nested) list."
    },
    {
        "Y": "Tensor.tolist",
        "X": "What returns the tensor as a (nested) list?",
        "Z": "Tensor.tolist Returns the tensor as a (nested) list."
    },
    {
        "Y": "Tensor.topk Seetorch.topk",
        "X": "What is Tensor.topk Seetorch.topk?",
        "Z": "Tensor.topk Seetorch.topk()"
    },
    {
        "Y": "Seetorch.topk",
        "X": "What is Tensor.topk?",
        "Z": "Tensor.topk Seetorch.topk()"
    },
    {
        "Y": "Tensor.topk",
        "X": "What is Seetorch.topk?",
        "Z": "Tensor.topk Seetorch.topk()"
    },
    {
        "Y": "Tensor.to_sparse",
        "X": "What returns a sparse copy of the tensor?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor."
    },
    {
        "Y": "a sparse copy",
        "X": "What does Tensor.to_sparse return?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor."
    },
    {
        "Y": "Seetorch.trace",
        "X": "What is Tensor.trace?",
        "Z": "Tensor.trace Seetorch.trace()"
    },
    {
        "Y": "Tensor.trace",
        "X": "What is Seetorch.trace?",
        "Z": "Tensor.trace Seetorch.trace()"
    },
    {
        "Y": "Tensor",
        "X": "What is.transpose Seetorch.transpose() function?",
        "Z": "Tensor.transpose Seetorch.transpose()"
    },
    {
        "Y": "Seetorch.transpose",
        "X": "What is Tensor.transpose?",
        "Z": "Tensor.transpose Seetorch.transpose()"
    },
    {
        "Y": "Tensor.transpose",
        "X": "What is Seetorch.transpose() function?",
        "Z": "Tensor.transpose Seetorch.transpose()"
    },
    {
        "Y": "Tensor.transpose_ In-place version oftranspose()",
        "X": "What does Tensor.transpose_ In-place version oftranspose() do?",
        "Z": "Tensor.transpose_ In-place version oftranspose()"
    },
    {
        "Y": "Tensor.transpose_ In-place",
        "X": "What version oftranspose() does Tensor.transpose_ In-place?",
        "Z": "Tensor.transpose_ In-place version oftranspose()"
    },
    {
        "Y": "Tensor.triangular_solve",
        "X": "What does Seetorch.triangular_solve do?",
        "Z": "Tensor.triangular_solve Seetorch.triangular_solve()"
    },
    {
        "Y": "Seetorch.triangular_solve()",
        "X": "What is Tensor.triangular_solve?",
        "Z": "Tensor.triangular_solve Seetorch.triangular_solve()"
    },
    {
        "Y": "Tensor.triangular_solve",
        "X": "What is Seetorch.triangular_solve?",
        "Z": "Tensor.triangular_solve Seetorch.triangular_solve()"
    },
    {
        "Y": "Tensor.tril Seetorch.tril()",
        "X": "What does Tensor.tril Seetorch.tril do?",
        "Z": "Tensor.tril Seetorch.tril()"
    },
    {
        "Y": "Seetorch.tril",
        "X": "What is Tensor.tril?",
        "Z": "Tensor.tril Seetorch.tril()"
    },
    {
        "Y": "Tensor.tril",
        "X": "What is Seetorch.tril?",
        "Z": "Tensor.tril Seetorch.tril()"
    },
    {
        "Y": "Tensor.tril_ In-place version oftril()",
        "X": "What is In-place version oftril()?",
        "Z": "Tensor.tril_ In-place version oftril()"
    },
    {
        "Y": "Tensor.tril_ In-place",
        "X": "What is version oftril()?",
        "Z": "Tensor.tril_ In-place version oftril()"
    },
    {
        "Y": "Tensor.triu Seetorch.triu",
        "X": "What is Tensor.triu Seetorch.triu?",
        "Z": "Tensor.triu Seetorch.triu()"
    },
    {
        "Y": "Seetorch.triu",
        "X": "What is Tensor.triu?",
        "Z": "Tensor.triu Seetorch.triu()"
    },
    {
        "Y": "Tensor.triu",
        "X": "What is Seetorch.triu?",
        "Z": "Tensor.triu Seetorch.triu()"
    },
    {
        "Y": "Tensor.triu_ In-place version oftriu()",
        "X": "What is In-place version oftriu()?",
        "Z": "Tensor.triu_ In-place version oftriu()"
    },
    {
        "Y": "Tensor.triu_ In-place",
        "X": "What is version oftriu()?",
        "Z": "Tensor.triu_ In-place version oftriu()"
    },
    {
        "Y": "Tensor",
        "X": "What is component that does Seetorch.true_divide()?",
        "Z": "Tensor.true_divide Seetorch.true_divide()"
    },
    {
        "Y": "Tensor.true_divide",
        "X": "What does Seetorch.true_divide() use?",
        "Z": "Tensor.true_divide Seetorch.true_divide()"
    },
    {
        "Y": "Tensor.true_divide_ In-place version oftrue_divide_()",
        "X": "What is In-place version oftrue_divide_()?",
        "Z": "Tensor.true_divide_ In-place version oftrue_divide_()"
    },
    {
        "Y": "Tensor.true_divide_ In-place",
        "X": "What version oftrue_divide_() is used?",
        "Z": "Tensor.true_divide_ In-place version oftrue_divide_()"
    },
    {
        "Y": "Seetorch.trunc()",
        "X": "What is Tensor.trunc?",
        "Z": "Tensor.trunc Seetorch.trunc()"
    },
    {
        "Y": "Tensor.trunc",
        "X": "What is the name of Seetorch.trunc()?",
        "Z": "Tensor.trunc Seetorch.trunc()"
    },
    {
        "Y": "Tensor.trunc_ In-place version oftrunc()",
        "X": "What does Tensor.trunc_ In-place version of oftrunc() do?",
        "Z": "Tensor.trunc_ In-place version oftrunc()"
    },
    {
        "Y": "Tensor.trunc_ In-place",
        "X": "What is version of oftrunc()?",
        "Z": "Tensor.trunc_ In-place version oftrunc()"
    },
    {
        "Y": "ifdtypeis not provided",
        "X": "What type does Tensor.type return?",
        "Z": "Tensor.type Returns the type ifdtypeis not provided, else casts this object to the specified type."
    },
    {
        "Y": "Tensor.type",
        "X": "What returns the type ifdtypeis not provided?",
        "Z": "Tensor.type Returns the type ifdtypeis not provided, else casts this object to the specified type."
    },
    {
        "Y": "the type of the given tensor",
        "X": "What does Tensor.type_as return?",
        "Z": "Tensor.type_as Returns this tensor cast to the type of the given tensor."
    },
    {
        "Y": "Tensor.type_as",
        "X": "What returns this tensor cast to the type of the given tensor?",
        "Z": "Tensor.type_as Returns this tensor cast to the type of the given tensor."
    },
    {
        "Y": "Tensor.unbind Seetorch.unbind()",
        "X": "What does Tensor.unbind Seetorch.unbind() do?",
        "Z": "Tensor.unbind Seetorch.unbind()"
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.unbind?",
        "Z": "Tensor.unbind Seetorch.unbind()"
    },
    {
        "Y": "Tensor.unbind",
        "X": "What does Seetorch.unbind() do?",
        "Z": "Tensor.unbind Seetorch.unbind()"
    },
    {
        "Y": "dimension dimension",
        "X": "The original tensor contains all slices of sizesizefromselftensor in what dimension?",
        "Z": "Tensor.unfold Returns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimension dimension."
    },
    {
        "Y": "sizesizefromselftensor",
        "X": "The original tensor contains all slices of what?",
        "Z": "Tensor.unfold Returns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimension dimension."
    },
    {
        "Y": "uniform",
        "X": "What type of tensor is filled with numbers sampled from the continuous uniform distribution?",
        "Z": "Tensor.uniform_ Fillsselftensor with numbers sampled from the continuous uniform distribution:"
    },
    {
        "Y": "Tensor.uniform",
        "X": "What is Fillsselftensor?",
        "Z": "Tensor.uniform_ Fillsselftensor with numbers sampled from the continuous uniform distribution:"
    },
    {
        "Y": "unique elements",
        "X": "What does Tensor.unique return?",
        "Z": "Tensor.unique Returns the unique elements of the input tensor."
    },
    {
        "Y": "Seetorch",
        "X": "What does Tensor.unsqueeze?",
        "Z": "Tensor.unsqueeze Seetorch.unsqueeze()"
    },
    {
        "Y": "Tensor.unsqueeze",
        "X": "What does Seetorch.unsqueeze() do?",
        "Z": "Tensor.unsqueeze Seetorch.unsqueeze()"
    },
    {
        "Y": "Tensor.unsqueeze_ In-place version ofunsqueeze()",
        "X": "What is In-place version ofunsqueeze()?",
        "Z": "Tensor.unsqueeze_ In-place version ofunsqueeze()"
    },
    {
        "Y": "Tensor",
        "X": "What value returns the values tensor of asparse COO tensor?",
        "Z": "Tensor.values Return the values tensor of asparse COO tensor."
    },
    {
        "Y": "Return the values tensor of asparse COO tensor",
        "X": "What does Tensor.values do?",
        "Z": "Tensor.values Return the values tensor of asparse COO tensor."
    },
    {
        "Y": "Tensor.var Seetorch.var()",
        "X": "What does Tensor.var Seetorch.var do?",
        "Z": "Tensor.var Seetorch.var()"
    },
    {
        "Y": "Tensor.var Seetorch.var",
        "X": "What is Tensor.var Seetorch.var?",
        "Z": "Tensor.var Seetorch.var()"
    },
    {
        "Y": "Seetorch.vdot",
        "X": "What is Tensor.vdot?",
        "Z": "Tensor.vdot Seetorch.vdot()"
    },
    {
        "Y": "Tensor.vdot",
        "X": "What is Seetorch.vdot?",
        "Z": "Tensor.vdot Seetorch.vdot()"
    },
    {
        "Y": "differentshape",
        "X": "Tensor.view Returns a new tensor with the same data as the self tensor but of what shape?",
        "Z": "Tensor.view Returns a new tensor with the same data as the self tensor but of a differentshape."
    },
    {
        "Y": "the self tensor",
        "X": "Tensor.view Returns a new tensor with the same data as what?",
        "Z": "Tensor.view Returns a new tensor with the same data as the self tensor but of a differentshape."
    },
    {
        "Y": "Tensor.view",
        "X": "What returns a new tensor with the same data as the self tensor but of a different shape?",
        "Z": "Tensor.view Returns a new tensor with the same data as the self tensor but of a differentshape."
    },
    {
        "Y": "same size",
        "X": "What is the size of the tensor?",
        "Z": "Tensor.view_as View this tensor as the same size asother."
    },
    {
        "Y": "Tensor",
        "X": "What does view_as view as the same size asother?",
        "Z": "Tensor.view_as View this tensor as the same size asother."
    },
    {
        "Y": "Tensor.vsplit Seetorch.vsplit()",
        "X": "What does Tensor.vsplit Seetorch.vsplit() do?",
        "Z": "Tensor.vsplit Seetorch.vsplit()"
    },
    {
        "Y": "Seetorch.vsplit",
        "X": "What is Tensor.vsplit?",
        "Z": "Tensor.vsplit Seetorch.vsplit()"
    },
    {
        "Y": "Tensor.vsplit",
        "X": "What is Seetorch.vsplit() function?",
        "Z": "Tensor.vsplit Seetorch.vsplit()"
    },
    {
        "Y": "Tensor",
        "X": "What is the equivalent of a totorch?",
        "Z": "Tensor.where self.where(condition,y)is equivalent totorch.where(condition,self,y)."
    },
    {
        "Y": "totorch",
        "X": "What is a tensor equivalent to?",
        "Z": "Tensor.where self.where(condition,y)is equivalent totorch.where(condition,self,y)."
    },
    {
        "Y": "Tensor.xlogy Seetorch.xlogy",
        "X": "What is Tensor.xlogy?",
        "Z": "Tensor.xlogy Seetorch.xlogy()"
    },
    {
        "Y": "Tensor.xlogy",
        "X": "What is Seetorch.xlogy?",
        "Z": "Tensor.xlogy Seetorch.xlogy()"
    },
    {
        "Y": "Tensor.xlogy_ In-place version ofxlogy()",
        "X": "What does Tensor.xlogy_ In-place version ofxlogy() do?",
        "Z": "Tensor.xlogy_ In-place version ofxlogy()"
    },
    {
        "Y": "Tensor.xlogy",
        "X": "What is In-place version ofxlogy()?",
        "Z": "Tensor.xlogy_ In-place version ofxlogy()"
    },
    {
        "Y": "zeros",
        "X": "What does Tensor.zero_ Fillsselftensor with?",
        "Z": "Tensor.zero_ Fillsselftensor with zeros."
    },
    {
        "Y": "torch.mv()",
        "X": "What function does no M[sparse_coo]@V[strided]->V[strided] use",
        "Z": "torch.mv() no M[sparse_coo]@V[strided]->V[strided]"
    },
    {
        "Y": "torch.mv()",
        "X": "What does no M[sparse_coo]@V[strided]->V[strided]?",
        "Z": "torch.mv() no M[sparse_coo]@V[strided]->V[strided]"
    },
    {
        "Y": "torch.mv()",
        "X": "What does no M[sparse_csr] do?",
        "Z": "torch.mv() no M[sparse_csr]@V[strided]->V[strided]"
    },
    {
        "Y": "torch.mv()",
        "X": "What does no M[sparse_csr]@V[strided]->V[strided]?",
        "Z": "torch.mv() no M[sparse_csr]@V[strided]->V[strided]"
    },
    {
        "Y": "torch.mm()",
        "X": "What function does no M[sparse_coo]@M[strided]->M[strided] use",
        "Z": "torch.mm() no M[sparse_coo]@M[strided]->M[strided]"
    },
    {
        "Y": "torch.matmul()",
        "X": "What function does no M[sparse_coo]@M[strided]->M[strided]?",
        "Z": "torch.matmul() no M[sparse_coo]@M[strided]->M[strided]"
    },
    {
        "Y": "torch.matmul()",
        "X": "What function does no M[sparse_csr]@M[strided]->M[strided]",
        "Z": "torch.matmul() no M[sparse_csr]@M[strided]->M[strided]"
    },
    {
        "Y": "M[sparse_coo]",
        "X": "What does torch.sparse.mm() do?",
        "Z": "torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided]"
    },
    {
        "Y": "torch.smm()",
        "X": "What function does no M[sparse_coo]@M[strided]->M[sparse_co",
        "Z": "torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo]"
    },
    {
        "Y": "torch.smm()",
        "X": "What does no M[sparse_coo]@M[strided]->M[sparse_coo",
        "Z": "torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo]"
    },
    {
        "Y": "torch.hspmm()",
        "X": "What function does no M[sparse_coo]@M[strided]->M[hybridspar",
        "Z": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo]"
    },
    {
        "Y": "torch.hspmm()",
        "X": "What does no M[sparse_coo]@M[strided]->M[hybridspars",
        "Z": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo]"
    },
    {
        "Y": "torch.bmm()",
        "X": "What function does no T[sparse_coo]@T[strided]->T[strided] use",
        "Z": "torch.bmm() no T[sparse_coo]@T[strided]->T[strided]"
    },
    {
        "Y": "torch.bmm()",
        "X": "What does no T[sparse_coo]@T[strided]->T[strided]?",
        "Z": "torch.bmm() no T[sparse_coo]@T[strided]->T[strided]"
    },
    {
        "Y": "yes",
        "X": "f*M[strided]+f*(M[sparse_coo]@M[strided",
        "Z": "torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]"
    },
    {
        "Y": "no",
        "X": "What does torch.lobpcg() do?",
        "Z": "torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided]"
    },
    {
        "Y": "torch.pca_lowrank()",
        "X": "What function does PCA(M[sparse_coo]) use?",
        "Z": "torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided]"
    },
    {
        "Y": "torch.pca_lowrank()",
        "X": "What does PCA(M[sparse_coo])->M[strided],M[strided],",
        "Z": "torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided]"
    },
    {
        "Y": "yes",
        "X": "SVD(M[sparse_coo])->M[strided],M[strided],M",
        "Z": "torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided]"
    },
    {
        "Y": "SVD",
        "X": "What does torch.svd_lowrank() do?",
        "Z": "torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided]"
    },
    {
        "Y": "Tensor._to_sparse_csr",
        "X": "What converts a tensor to compressed row storage format?",
        "Z": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format."
    },
    {
        "Y": "Tensor",
        "X": "Convert a tensor to compressed row storage format?",
        "Z": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format."
    },
    {
        "Y": "compressed row storage format",
        "X": "What format does tensor._to_sparse_csr convert a tensor to?",
        "Z": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format."
    },
    {
        "Y": "Tensor",
        "X": "What returns a coalesced copy ofselfifselfis anuncoalesced tensor?",
        "Z": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor."
    },
    {
        "Y": "a coalesced copy ofselfifselfis",
        "X": "What does Tensor.coalesce return?",
        "Z": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor."
    },
    {
        "Y": "the desired size",
        "X": "What is a tensorto?",
        "Z": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions."
    },
    {
        "Y": "Tensor.sparse_resize",
        "X": "What resizes itselfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "Z": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions."
    },
    {
        "Y": "the desired size",
        "X": "What does Tensor.sparse_resize_and_clear_ resize itself to?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions."
    },
    {
        "Y": "Tensor.sparse_resize_and_clear",
        "X": "What removes all specified elements from asparse tensorselfand resizes itself to the desired size and the number of spar",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions."
    },
    {
        "Y": "coalesced",
        "X": "What is tensorthat?",
        "Z": "Tensor.is_coalesced ReturnsTrue ifselfis asparse COO tensorthat is coalesced,False otherwise."
    },
    {
        "Y": "Tensor.to_dense",
        "X": "What creates a strided copy of self?",
        "Z": "Tensor.to_dense Creates a strided copy ofself."
    },
    {
        "Y": "strided",
        "X": "What type of copy of self does Tensor.to_dense create?",
        "Z": "Tensor.to_dense Creates a strided copy ofself."
    },
    {
        "Y": "self",
        "X": "Tensor.to_dense Creates a strided copy of what?",
        "Z": "Tensor.to_dense Creates a strided copy ofself."
    },
    {
        "Y": "tensor",
        "X": "What returns the compressed row indices of the self tensor whenselfis a sparse CSR tensor of layouts",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor whenselfis a sparse CSR tensor of layoutsparse_csr."
    },
    {
        "Y": "layoutsparse_csr",
        "X": "Selfis a sparse CSR tensor of what?",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor whenselfis a sparse CSR tensor of layoutsparse_csr."
    },
    {
        "Y": "tensor",
        "X": "What returns the column indices of the self tensor whenselfis a sparse CSR tensor of layoutspar",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor whenselfis a sparse CSR tensor of layoutsparse_csr."
    },
    {
        "Y": "the self tensor",
        "X": "Whenselfis a sparse CSR tensor of layoutsparse_csr?",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor whenselfis a sparse CSR tensor of layoutsparse_csr."
    },
    {
        "Y": "givencrow_indicesandcol_indices",
        "X": "Where are the specified values for the asparse tensor?",
        "Z": "_sparse_csr_tensor Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices."
    },
    {
        "Y": "CSR",
        "X": "What is the Compressed Sparse Row?",
        "Z": "_sparse_csr_tensor Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices."
    },
    {
        "Y": "sparse.sum",
        "X": "What returns the sum of each row of the sparse tensorinputin the given dimensionsdim?",
        "Z": "sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim."
    },
    {
        "Y": "backward",
        "X": "What direction does sparse.addmm support?",
        "Z": "sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrix mat1."
    },
    {
        "Y": "sparse matrix mat1",
        "X": "What does sparse.addmm support backward for?",
        "Z": "sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrix mat1."
    },
    {
        "Y": "astorch.addmm()",
        "X": "What does sparse.addmm do in the forward?",
        "Z": "sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrix mat1."
    },
    {
        "Y": "sparse",
        "X": "What type of matrix mat performs a matrix  multiplication of the sparse matrix mat1 and the (sparse or strided",
        "Z": "sparse.mm Performs a matrix  multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2."
    },
    {
        "Y": "sparse",
        "X": "What type of matrix mat1 performs a matrix  multiplication?",
        "Z": "sparse.mm Performs a matrix  multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2."
    },
    {
        "Y": "sspaddmm matrix ",
        "X": "What multiplies a sparse tensormat1with a dense tensormat2?",
        "Z": "sspaddmm matrix  multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result."
    },
    {
        "Y": "sparse",
        "X": "sspaddmm matrix  multiplies what tensormat1 with a dense tensormat2?",
        "Z": "sspaddmm matrix  multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result."
    },
    {
        "Y": "dense",
        "X": "sspaddmm matrix  multiplies a sparse tensormat1with a what?",
        "Z": "sspaddmm matrix  multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result."
    },
    {
        "Y": "hspmm",
        "X": "What performs a matrix  multiplication of asparse COO matrix mat1 and a strided matrix mat2?",
        "Z": "hspmm Performs a matrix  multiplication of asparse COO matrix mat1 and a strided matrix mat2."
    },
    {
        "Y": "a strided matrix mat2",
        "X": "hspmm performs a matrix  multiplication of asparse COO matrix mat1 and what else?",
        "Z": "hspmm Performs a matrix  multiplication of asparse COO matrix mat1 and a strided matrix mat2."
    },
    {
        "Y": "hspmm",
        "X": "What performs a matrix  multiplication of asparse COO matrix mat1 and a strided matrix mat2?",
        "Z": "hspmm Performs a matrix  multiplication of asparse COO matrix mat1 and a strided matrix mat2."
    },
    {
        "Y": "smm",
        "X": "What performs a matrix  multiplication of the sparse matrix input with the dense matrix mat?",
        "Z": "smm Performs a matrix  multiplication of the sparse matrix input with the dense matrix mat."
    },
    {
        "Y": "dense matrix mat",
        "X": "smm Performs a matrix  multiplication of the sparse matrix input with what?",
        "Z": "smm Performs a matrix  multiplication of the sparse matrix input with the dense matrix mat."
    },
    {
        "Y": "sparse matrix input",
        "X": "What does smm perform a matrix  multiplication of?",
        "Z": "smm Performs a matrix  multiplication of the sparse matrix input with the dense matrix mat."
    },
    {
        "Y": "softmax function",
        "X": "What does sparse.softmax apply?",
        "Z": "sparse.softmax Applies a softmax function."
    },
    {
        "Y": "sparse",
        "X": "Softmax Applies a softmax function that is what?",
        "Z": "sparse.softmax Applies a softmax function."
    },
    {
        "Y": "logarithm",
        "X": "What is followed by a softmax function?",
        "Z": "sparse.log_softmax Applies a softmax function followed by logarithm."
    },
    {
        "Y": "sparse.log_softmax",
        "X": "What applies a softmax function followed by logarithm?",
        "Z": "sparse.log_softmax Applies a softmax function followed by logarithm."
    },
    {
        "Y": "sparse",
        "X": "What type of function does.log_softmax Applies a softmax function followed by logarithm?",
        "Z": "sparse.log_softmax Applies a softmax function followed by logarithm."
    },
    {
        "Y": "a vector or matrix  norm",
        "X": "What does norm compute?",
        "Z": "norm Computes a vector or matrix  norm."
    },
    {
        "Y": "a vector or matrix  norm",
        "X": "What does norm Compute?",
        "Z": "norm Computes a vector or matrix  norm."
    },
    {
        "Y": "vector norm",
        "X": "What does vector_norm compute?",
        "Z": "vector_norm Computes a vector norm."
    },
    {
        "Y": "vector_norm",
        "X": "What Computes a vector norm?",
        "Z": "vector_norm Computes a vector norm."
    },
    {
        "Y": "matrix  norm",
        "X": "What does matrix _norm compute?",
        "Z": "matrix _norm Computes a matrix  norm."
    },
    {
        "Y": "matrix _norm",
        "X": "What Computes a matrix  norm?",
        "Z": "matrix _norm Computes a matrix  norm."
    },
    {
        "Y": "Computes",
        "X": "What does det do with the determinant of a square matrix ?",
        "Z": "det Computes the determinant of a square matrix ."
    },
    {
        "Y": "det",
        "X": "What computes the determinant of a square matrix ?",
        "Z": "det Computes the determinant of a square matrix ."
    },
    {
        "Y": "slogdet",
        "X": "What computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ?",
        "Z": "slogdet Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ."
    },
    {
        "Y": "the sign and natural logarithm",
        "X": "What does slogdet compute of the absolute value of the determinant of a square matrix ?",
        "Z": "slogdet Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ."
    },
    {
        "Y": "slogdet",
        "X": "What Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ?",
        "Z": "slogdet Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix ."
    },
    {
        "Y": "the condition number of a matrix  with respect to a matrix  norm",
        "X": "What does cond compute?",
        "Z": "cond Computes the condition number of a matrix  with respect to a matrix  norm."
    },
    {
        "Y": "cond",
        "X": "What computes the condition number of a matrix  with respect to a matrix  norm?",
        "Z": "cond Computes the condition number of a matrix  with respect to a matrix  norm."
    },
    {
        "Y": "matrix  norm",
        "X": "cond Computes the condition number of a matrix  with respect to what?",
        "Z": "cond Computes the condition number of a matrix  with respect to a matrix  norm."
    },
    {
        "Y": "matrix ",
        "X": "What computes the numerical rank of a matrix ?",
        "Z": "matrix _rank Computes the numerical rank of a matrix ."
    },
    {
        "Y": "cholesky_ex",
        "X": "What computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix ?",
        "Z": "cholesky_ex Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix ."
    },
    {
        "Y": "Hermitian",
        "X": "cholesky computes the Cholesky decomposition of a complex what?",
        "Z": "cholesky Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix ."
    },
    {
        "Y": "qr",
        "X": "What computes the QR decomposition of a matrix ?",
        "Z": "qr Computes the QR decomposition of a matrix ."
    },
    {
        "Y": "eigenvalue",
        "X": "eig Computes the decomposition of a square matrix  if it exists?",
        "Z": "eig Computes the eigenvalue decomposition of a square matrix  if it exists."
    },
    {
        "Y": "eig",
        "X": "What computes the eigenvalue decomposition of a square matrix  if it exists?",
        "Z": "eig Computes the eigenvalue decomposition of a square matrix  if it exists."
    },
    {
        "Y": "eigenvalues",
        "X": "eigvals Computes what of a square matrix ?",
        "Z": "eigvals Computes the eigenvalues of a square matrix ."
    },
    {
        "Y": "eigvals",
        "X": "What computes the eigenvalues of a square matrix ?",
        "Z": "eigvals Computes the eigenvalues of a square matrix ."
    },
    {
        "Y": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ",
        "X": "What does eigh do?",
        "Z": "eigh Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "eigh",
        "X": "What computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ?",
        "Z": "eigh Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "Hermitian",
        "X": "eigh Computes the eigenvalue decomposition of a complex what?",
        "Z": "eigh Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "eigenvalues",
        "X": "eigvalsh Computes what of a complex Hermitian or real symmetric matrix ?",
        "Z": "eigvalsh Computes the eigenvalues of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "symmetric",
        "X": "eigvalsh Computes the eigenvalues of a complex Hermitian or real what type of matrix ?",
        "Z": "eigvalsh Computes the eigenvalues of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "eigvalsh",
        "X": "What computes the eigenvalues of a complex Hermitian or real symmetric matrix ?",
        "Z": "eigvalsh Computes the eigenvalues of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "Hermitian",
        "X": "eigvalsh Computes the eigenvalues of a complex what?",
        "Z": "eigvalsh Computes the eigenvalues of a complex Hermitian or real symmetric matrix ."
    },
    {
        "Y": "svd",
        "X": "What computes the singular value decomposition of a matrix ?",
        "Z": "svd Computes the singular value decomposition (SVD) of a matrix ."
    },
    {
        "Y": "SVD",
        "X": "What is svd?",
        "Z": "svd Computes the singular value decomposition (SVD) of a matrix ."
    },
    {
        "Y": "svdvals",
        "X": "What computes the singular values of a matrix ?",
        "Z": "svdvals Computes the singular values of a matrix ."
    },
    {
        "Y": "svdvals",
        "X": "What Computes the singular values of a matrix ?",
        "Z": "svdvals Computes the singular values of a matrix ."
    },
    {
        "Y": "Computes",
        "X": "What does solve the solution of a square system of linear equations with a unique solution?",
        "Z": "solve Computes the solution of a square system of linear equations with a unique solution."
    },
    {
        "Y": "solve Computes the solution of a square system of linear equations",
        "X": "What does a unique solution do to a square system of linear equations?",
        "Z": "solve Computes the solution of a square system of linear equations with a unique solution."
    },
    {
        "Y": "lstsq",
        "X": "What computes a solution to the least squares problem of a system of linear equations?",
        "Z": "lstsq Computes a solution to the least squares problem of a system of linear equations."
    },
    {
        "Y": "least squares",
        "X": "lstsq Computes a solution to what problem of a system of linear equations?",
        "Z": "lstsq Computes a solution to the least squares problem of a system of linear equations."
    },
    {
        "Y": "inverse",
        "X": "inv Computes the what of a square matrix  if it exists?",
        "Z": "inv Computes the inverse of a square matrix  if it exists."
    },
    {
        "Y": "inv",
        "X": "What computes the inverse of a square matrix  if it exists?",
        "Z": "inv Computes the inverse of a square matrix  if it exists."
    },
    {
        "Y": "pseudoinverse",
        "X": "What does pinv compute?",
        "Z": "pinv Computes the pseudoinverse (Moore-Penrose inverse) of a matrix ."
    },
    {
        "Y": "the pseudoinverse",
        "X": "What does pinv compute of a matrix ?",
        "Z": "pinv Computes the pseudoinverse (Moore-Penrose inverse) of a matrix ."
    },
    {
        "Y": "matrix _power",
        "X": "What computes the then-th power of a square matrix  for an integern?",
        "Z": "matrix _power Computes then-th power of a square matrix  for an integern."
    },
    {
        "Y": "matrix _power",
        "X": "What computes then-th power of a square matrix  for an integern?",
        "Z": "matrix _power Computes then-th power of a square matrix  for an integern."
    },
    {
        "Y": "multi_dot",
        "X": "What multiplies two or more matrices by reordering the multiplications?",
        "Z": "multi_dot Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed."
    },
    {
        "Y": "reordering",
        "X": "multi_dot multiplies two or more matrices by what?",
        "Z": "multi_dot Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed."
    },
    {
        "Y": "householder_product",
        "X": "What computes the firstncolumns of a product of Householder matrices?",
        "Z": "householder_product Computes the firstncolumns of a product of Householder matrices."
    },
    {
        "Y": "firstncolumns",
        "X": "householder_product Computes what of a product of Householder matrices?",
        "Z": "householder_product Computes the firstncolumns of a product of Householder matrices."
    },
    {
        "Y": "Householder matrices",
        "X": "householder_product Computes the firstncolumns of a product of what?",
        "Z": "householder_product Computes the firstncolumns of a product of Householder matrices."
    },
    {
        "Y": "tensorinv",
        "X": "What computes the multiplicative inverse of torch.tensordot()?",
        "Z": "tensorinv Computes the multiplicative inverse of torch.tensordot()."
    },
    {
        "Y": "tensorinv",
        "X": "What Computes the multiplicative inverse of torch.tensordot()?",
        "Z": "tensorinv Computes the multiplicative inverse of torch.tensordot()."
    },
    {
        "Y": "systemtorch",
        "X": "What does tensorsolve compute the solutionXto?",
        "Z": "tensorsolve Computes the solutionXto the systemtorch.tensordot(A, X) = B."
    },
    {
        "Y": "tensorsolve",
        "X": "What Computes the solutionXto the systemtorch?",
        "Z": "tensorsolve Computes the solutionXto the systemtorch.tensordot(A, X) = B."
    },
    {
        "Y": "Hermitian",
        "X": "cholesky_ex Computes the Cholesky decomposition of a complex what?",
        "Z": "cholesky_ex Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix ."
    },
    {
        "Y": "inv_ex",
        "X": "What computes the inverse of a square matrix  if it is invertible?",
        "Z": "inv_ex Computes the inverse of a square matrix  if it is invertible."
    },
    {
        "Y": "inverse",
        "X": "inv_ex computes what of a square matrix  if it is invertible?",
        "Z": "inv_ex Computes the inverse of a square matrix  if it is invertible."
    },
    {
        "Y": "StreamContext Context-manager",
        "X": "What is program that selects a given stream?",
        "Z": "StreamContext Context-manager that selects a given stream."
    },
    {
        "Y": "StreamContext Context-manager",
        "X": "What is manager that selects a given stream?",
        "Z": "StreamContext Context-manager that selects a given stream."
    },
    {
        "Y": "peer access",
        "X": "Can_device_access_peer Checks if what is possible between two devices?",
        "Z": "can_device_access_peer Checks if peer access between two devices is possible."
    },
    {
        "Y": "can_device_access_peer",
        "X": "What checks if peer access between two devices is possible?",
        "Z": "can_device_access_peer Checks if peer access between two devices is possible."
    },
    {
        "Y": "peer access between two devices",
        "X": "Can_device_access_peer Checks if what is possible?",
        "Z": "can_device_access_peer Checks if peer access between two devices is possible."
    },
    {
        "Y": "cublasHandle_t",
        "X": "what pointer does current_blas_handle return?",
        "Z": "current_blas_handle Returns cublasHandle_t pointer to current cuBLAS handle"
    },
    {
        "Y": "current cuBLAS handle",
        "X": "current_blas_handle Returns cublasHandle_t pointer to what?",
        "Z": "current_blas_handle Returns cublasHandle_t pointer to current cuBLAS handle"
    },
    {
        "Y": "current_blas_handle",
        "X": "What returns cublasHandle_t pointer to current cuBLAS handle?",
        "Z": "current_blas_handle Returns cublasHandle_t pointer to current cuBLAS handle"
    },
    {
        "Y": "current_device",
        "X": "What returns the index of a currently selected device?",
        "Z": "current_device Returns the index of a currently selected device."
    },
    {
        "Y": "current_stream Returns the currently selectedStreamfor a given device",
        "X": "What does current_stream return?",
        "Z": "current_stream Returns the currently selectedStreamfor a given device."
    },
    {
        "Y": "current_stream",
        "X": "What returns the current selectedStream for a given device?",
        "Z": "current_stream Returns the currently selectedStreamfor a given device."
    },
    {
        "Y": "default_stream",
        "X": "What returns the defaultStream for a given device?",
        "Z": "default_stream Returns the defaultStreamfor a given device."
    },
    {
        "Y": "defaultStream",
        "X": "What does default_stream return for a given device?",
        "Z": "default_stream Returns the defaultStreamfor a given device."
    },
    {
        "Y": "default_stream",
        "X": "What returns the default stream for a given device?",
        "Z": "default_stream Returns the defaultStreamfor a given device."
    },
    {
        "Y": "Context-manager",
        "X": "What is device that changes the selected device?",
        "Z": "device Context-manager that changes the selected device."
    },
    {
        "Y": "device Context-manager",
        "X": "What changes the selected device?",
        "Z": "device Context-manager that changes the selected device."
    },
    {
        "Y": "device_count",
        "X": "What returns the number of GPUs available?",
        "Z": "device_count Returns the number of GPUs available."
    },
    {
        "Y": "device_of Context-manager",
        "X": "What changes the current device to that of given object?",
        "Z": "device_of Context-manager that changes the current device to that of given object."
    },
    {
        "Y": "device",
        "X": "What is Context-manager that changes the current device to that of given object?",
        "Z": "device_of Context-manager that changes the current device to that of given object."
    },
    {
        "Y": "get_arch_list",
        "X": "What returns the CUDA architectures this library was compiled for?",
        "Z": "get_arch_list Returns list CUDA architectures this library was compiled for."
    },
    {
        "Y": "CUDA architectures",
        "X": "What does get_arch_list return?",
        "Z": "get_arch_list Returns list CUDA architectures this library was compiled for."
    },
    {
        "Y": "get_arch_list",
        "X": "What Returns list CUDA architectures this library was compiled for?",
        "Z": "get_arch_list Returns list CUDA architectures this library was compiled for."
    },
    {
        "Y": "cuda capability",
        "X": "What does get_device_capability get from a device?",
        "Z": "get_device_capability Gets the cuda capability of a device."
    },
    {
        "Y": "get_device_capability",
        "X": "What gets the cuda capability of a device?",
        "Z": "get_device_capability Gets the cuda capability of a device."
    },
    {
        "Y": "the name of a device",
        "X": "What does get_device_name get?",
        "Z": "get_device_name Gets the name of a device."
    },
    {
        "Y": "get_device_name",
        "X": "What gets the name of a device?",
        "Z": "get_device_name Gets the name of a device."
    },
    {
        "Y": "properties",
        "X": "What does get the properties of a device?",
        "Z": "get_device_properties Gets the properties of a device."
    },
    {
        "Y": "get_device_properties",
        "X": "What gets the properties of a device?",
        "Z": "get_device_properties Gets the properties of a device."
    },
    {
        "Y": "NVCC",
        "X": "What gencode flags does get_gencode_flags return?",
        "Z": "get_gencode_flags Returns NVCC gencode flags this library was compiled with."
    },
    {
        "Y": "get_gencode_flags",
        "X": "What returns the NVCC gencode flags this library was compiled with?",
        "Z": "get_gencode_flags Returns NVCC gencode flags this library was compiled with."
    },
    {
        "Y": "get_gencode_flags",
        "X": "What returns NVCC gencode flags this library was compiled with?",
        "Z": "get_gencode_flags Returns NVCC gencode flags this library was compiled with."
    },
    {
        "Y": "Initialize PyTorch\u2019s CUDA state",
        "X": "What does init do?",
        "Z": "init Initialize PyTorch\u2019s CUDA state."
    },
    {
        "Y": "CUDA",
        "X": "Init Initialize PyTorch's state what?",
        "Z": "init Initialize PyTorch\u2019s CUDA state."
    },
    {
        "Y": "CUDA IPC",
        "X": "Who released the GPU memory for ipc_collect Force?",
        "Z": "ipc_collect Force collects GPU memory after it has been released by CUDA IPC."
    },
    {
        "Y": "ipc_collect Force",
        "X": "What collects GPU memory after it has been released by CUDA IPC?",
        "Z": "ipc_collect Force collects GPU memory after it has been released by CUDA IPC."
    },
    {
        "Y": "GPU memory",
        "X": "What does ipc_collect Force collect after it has been released by CUDA IPC?",
        "Z": "ipc_collect Force collects GPU memory after it has been released by CUDA IPC."
    },
    {
        "Y": "CUDA",
        "X": "What is currently available?",
        "Z": "is_available Returns a bool indicating if CUDA is currently available."
    },
    {
        "Y": "bool",
        "X": "What type of return does is_available return?",
        "Z": "is_available Returns a bool indicating if CUDA is currently available."
    },
    {
        "Y": "a bool",
        "X": "is_available Returns what type of value indicating if CUDA is currently available?",
        "Z": "is_available Returns a bool indicating if CUDA is currently available."
    },
    {
        "Y": "PyTorch\u2019s CUDA state",
        "X": "What has been initialized?",
        "Z": "is_initialized Returns whether PyTorch\u2019s CUDA state has been initialized."
    },
    {
        "Y": "initialized",
        "X": "What returns whether PyTorch\u2019s CUDA state has been initialized?",
        "Z": "is_initialized Returns whether PyTorch\u2019s CUDA state has been initialized."
    },
    {
        "Y": "set_device",
        "X": "What sets the current device?",
        "Z": "set_device Sets the current device."
    },
    {
        "Y": "current device",
        "X": "What does set_device set?",
        "Z": "set_device Sets the current device."
    },
    {
        "Y": "current device",
        "X": "what does set_device set?",
        "Z": "set_device Sets the current device."
    },
    {
        "Y": "set_stream",
        "X": "What is a wrapper API to set the stream?",
        "Z": "set_stream Sets the current stream.This is a wrapper API to set the stream."
    },
    {
        "Y": "set_stream",
        "X": "What is a wrapper API to set the current stream?",
        "Z": "set_stream Sets the current stream.This is a wrapper API to set the stream."
    },
    {
        "Y": "Wrapper",
        "X": "What is stream that wraps around the Context-manager StreamContext that selects a given stream?",
        "Z": "stream Wrapper around the Context-manager StreamContext that selects a given stream."
    },
    {
        "Y": "Context-manager StreamContext",
        "X": "What is the stream Wrapper around that selects a given stream?",
        "Z": "stream Wrapper around the Context-manager StreamContext that selects a given stream."
    },
    {
        "Y": "CUDA device",
        "X": "On what device should Waits for all kernels in all streams be synchronized?",
        "Z": "synchronize Waits for all kernels in all streams on a CUDA device to complete."
    },
    {
        "Y": "Waits",
        "X": "What is synchronized for all kernels in all streams on a CUDA device?",
        "Z": "synchronize Waits for all kernels in all streams on a CUDA device to complete."
    },
    {
        "Y": "get_rng_state",
        "X": "What returns the random number generator state of the specified GPU as a ByteTensor?",
        "Z": "get_rng_state Returns the random number generator state of the specified GPU as a ByteTensor."
    },
    {
        "Y": "ByteTensor",
        "X": "Get_rng_state Returns the random number generator state of the specified GPU as a what?",
        "Z": "get_rng_state Returns the random number generator state of the specified GPU as a ByteTensor."
    },
    {
        "Y": "random number generator state",
        "X": "What does get_rng_state return?",
        "Z": "get_rng_state Returns the random number generator state of the specified GPU as a ByteTensor."
    },
    {
        "Y": "a list of ByteTensor",
        "X": "What does get_rng_state_all return?",
        "Z": "get_rng_state_all Returns a list of ByteTensor representing the random number states of all devices."
    },
    {
        "Y": "set_rng_state",
        "X": "What sets the random number generator state of the specified GPU?",
        "Z": "set_rng_state Sets the random number generator state of the specified GPU."
    },
    {
        "Y": "random number generator state",
        "X": "set_rng_state Sets what state of the specified GPU?",
        "Z": "set_rng_state Sets the random number generator state of the specified GPU."
    },
    {
        "Y": "set_rng_state_all",
        "X": "What sets the random number generator state of all devices?",
        "Z": "set_rng_state_all Sets the random number generator state of all devices."
    },
    {
        "Y": "random number generator state",
        "X": "What does set_rng_state_all set?",
        "Z": "set_rng_state_all Sets the random number generator state of all devices."
    },
    {
        "Y": "manual_seed",
        "X": "What sets the seed for generating random numbers for the current GPU?",
        "Z": "manual_seed Sets the seed for generating random numbers for the current GPU."
    },
    {
        "Y": "manual_seed_all",
        "X": "What sets the seed for generating random numbers on all GPUs?",
        "Z": "manual_seed_all Sets the seed for generating random numbers on all GPUs."
    },
    {
        "Y": "random numbers",
        "X": "Manual_seed_all Sets the seed for generating what on all GPUs?",
        "Z": "manual_seed_all Sets the seed for generating random numbers on all GPUs."
    },
    {
        "Y": "generating random numbers",
        "X": "What is seed used for?",
        "Z": "seed Sets the seed for generating random numbers to a random number for the current GPU."
    },
    {
        "Y": "seed",
        "X": "What sets the seed for generating random numbers to a random number for the current GPU?",
        "Z": "seed Sets the seed for generating random numbers to a random number for the current GPU."
    },
    {
        "Y": "all GPUs",
        "X": "On which GPUs does seed_all set the seed for generating random numbers to a random number?",
        "Z": "seed_all Sets the seed for generating random numbers to a random number on all GPUs."
    },
    {
        "Y": "seed_all",
        "X": "What sets the seed for generating random numbers to a random number on all GPUs?",
        "Z": "seed_all Sets the seed for generating random numbers to a random number on all GPUs."
    },
    {
        "Y": "GPUs",
        "X": "The seed_all Sets the seed for generating random numbers to a random number on all what?",
        "Z": "seed_all Sets the seed for generating random numbers to a random number on all GPUs."
    },
    {
        "Y": "initial_seed",
        "X": "What returns the current random seed of the current GPU?",
        "Z": "initial_seed Returns the current random seed of the current GPU."
    },
    {
        "Y": "comm.broadcast Broadcasts",
        "X": "What is a tensor to specified GPU devices?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices."
    },
    {
        "Y": "comm.broadcast",
        "X": "What Broadcasts a tensor to specified GPU devices?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices."
    },
    {
        "Y": "GPU devices",
        "X": "comm.broadcast Broadcasts a tensor to specified what?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices."
    },
    {
        "Y": "comm.broadcast_coalesced Broadcasts",
        "X": "What is a sequence of tensors to the specified GPUs?",
        "Z": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs."
    },
    {
        "Y": "comm.broadcast_coalesced",
        "X": "What Broadcasts a sequence tensors to the specified GPUs?",
        "Z": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs."
    },
    {
        "Y": "GPUs",
        "X": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified what?",
        "Z": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs."
    },
    {
        "Y": "comm.reduce_add Sums tensors",
        "X": "What does comm.reduce_add Sums tensors do from multiple GPUs?",
        "Z": "comm.reduce_add Sums tensors from multiple GPUs."
    },
    {
        "Y": "Sums tensors",
        "X": "What does comm.reduce_add from multiple GPUs?",
        "Z": "comm.reduce_add Sums tensors from multiple GPUs."
    },
    {
        "Y": "comm.scatter Scatters tensor",
        "X": "What does comm.scatter Scatters tensor do across multiple GPUs?",
        "Z": "comm.scatter Scatters tensor across multiple GPUs."
    },
    {
        "Y": "comm.scatter",
        "X": "What Scatters tensor across multiple GPUs?",
        "Z": "comm.scatter Scatters tensor across multiple GPUs."
    },
    {
        "Y": "GPUs",
        "X": "comm.scatter Scatters tensor across multiple what?",
        "Z": "comm.scatter Scatters tensor across multiple GPUs."
    },
    {
        "Y": "comm.gather",
        "X": "What gathers tensors from multiple GPU devices?",
        "Z": "comm.gather Gathers tensors from multiple GPU devices."
    },
    {
        "Y": "tensors",
        "X": "What does comm.gather gather from multiple GPU devices?",
        "Z": "comm.gather Gathers tensors from multiple GPU devices."
    },
    {
        "Y": "CUDA",
        "X": "Stream Wrapper wraps around what stream?",
        "Z": "Stream Wrapper around a CUDA stream."
    },
    {
        "Y": "CUDA",
        "X": "Stream Wrapper around what stream?",
        "Z": "Stream Wrapper around a CUDA stream."
    },
    {
        "Y": "Stream Wrapper",
        "X": "What wrapper is used around a CUDA stream?",
        "Z": "Stream Wrapper around a CUDA stream."
    },
    {
        "Y": "Event Wrapper",
        "X": "What is wrapper that wraps around a CUDA event?",
        "Z": "Event Wrapper around a CUDA event."
    },
    {
        "Y": "Event Wrapper",
        "X": "What wrapper is used around a CUDA event?",
        "Z": "Event Wrapper around a CUDA event."
    },
    {
        "Y": "unoccupied cached memory",
        "X": "What does empty_cache release?",
        "Z": "empty_cache Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi."
    },
    {
        "Y": "empty_cache",
        "X": "What releases all unoccupied cached memory currently held by the caching allocator?",
        "Z": "empty_cache Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi."
    },
    {
        "Y": "list_gpu_processes",
        "X": "What returns a human-readable printout of the running processes and their GPU memory use for a given device?",
        "Z": "list_gpu_processes Returns a human-readable printout of the running processes and their GPU memory use for a given device."
    },
    {
        "Y": "CUDA",
        "X": "memory_stats Returns a dictionary of what memory allocator statistics for a given device?",
        "Z": "memory_stats Returns a dictionary of CUDA memory allocator statistics for a given device."
    },
    {
        "Y": "a dictionary of CUDA memory allocator statistics",
        "X": "What does memory_stats return for a given device?",
        "Z": "memory_stats Returns a dictionary of CUDA memory allocator statistics for a given device."
    },
    {
        "Y": "memory_stats",
        "X": "What returns a dictionary of CUDA memory allocator statistics for a given device?",
        "Z": "memory_stats Returns a dictionary of CUDA memory allocator statistics for a given device."
    },
    {
        "Y": "CUDA memory allocator statistics",
        "X": "memory_stats Returns a dictionary of what for a given device?",
        "Z": "memory_stats Returns a dictionary of CUDA memory allocator statistics for a given device."
    },
    {
        "Y": "a human-readable printout of the current memory allocator statistics",
        "X": "What does memory_summary return?",
        "Z": "memory_summary Returns a human-readable printout of the current memory allocator statistics for a given device."
    },
    {
        "Y": "memory_summary",
        "X": "What returns a human-readable printout of the current memory allocator statistics for a given device?",
        "Z": "memory_summary Returns a human-readable printout of the current memory allocator statistics for a given device."
    },
    {
        "Y": "CUDA",
        "X": "What memory allocator does memory_snapshot return a snapshot of?",
        "Z": "memory_snapshot Returns a snapshot of the CUDA memory allocator state across all devices."
    },
    {
        "Y": "memory_snapshot",
        "X": "What returns a snapshot of the CUDA memory allocator state across all devices?",
        "Z": "memory_snapshot Returns a snapshot of the CUDA memory allocator state across all devices."
    },
    {
        "Y": "tensors",
        "X": "memory_allocated Returns the current GPU memory occupied by what?",
        "Z": "memory_allocated Returns the current GPU memory occupied by tensors in bytes for a given device."
    },
    {
        "Y": "memory_allocated",
        "X": "What returns the current GPU memory occupied by tensors in bytes for a given device?",
        "Z": "memory_allocated Returns the current GPU memory occupied by tensors in bytes for a given device."
    },
    {
        "Y": "max_memory_allocated",
        "X": "What returns the maximum GPU memory occupied by tensors in bytes for a given device?",
        "Z": "max_memory_allocated Returns the maximum GPU memory occupied by tensors in bytes for a given device."
    },
    {
        "Y": "tensors",
        "X": "What does max_memory_allocated return?",
        "Z": "max_memory_allocated Returns the maximum GPU memory occupied by tensors in bytes for a given device."
    },
    {
        "Y": "reset_max_memory_allocated",
        "X": "What resets the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "Z": "reset_max_memory_allocated Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device."
    },
    {
        "Y": "tensors",
        "X": "Reset_max_memory_allocated Resets the starting point in tracking maximum GPU memory occupied by what?",
        "Z": "reset_max_memory_allocated Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device."
    },
    {
        "Y": "the current GPU memory",
        "X": "What does memory_reserved return?",
        "Z": "memory_reserved Returns the current GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "bytes",
        "X": "memory_reserved Returns the current GPU memory managed by the caching allocator in what format for a given device?",
        "Z": "memory_reserved Returns the current GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "memory_reserved",
        "X": "What returns the current GPU memory managed by the caching allocator in bytes for a given device?",
        "Z": "memory_reserved Returns the current GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "bytes",
        "X": "memory_reserved Returns the current GPU memory managed by the caching allocator in what for a given device?",
        "Z": "memory_reserved Returns the current GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "max_memory_reserved",
        "X": "What returns the maximum GPU memory managed by the caching allocator in bytes for a given device?",
        "Z": "max_memory_reserved Returns the maximum GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "bytes",
        "X": "How many bytes does max_memory_reserved return for a given device?",
        "Z": "max_memory_reserved Returns the maximum GPU memory managed by the caching allocator in bytes for a given device."
    },
    {
        "Y": "set_per_process_memory_fraction",
        "X": "What is setting that sets memory fraction for a process?",
        "Z": "set_per_process_memory_fraction Set memory fraction for a process."
    },
    {
        "Y": "set_per_process_memory_fraction",
        "X": "What is set memory fraction for a process?",
        "Z": "set_per_process_memory_fraction Set memory fraction for a process."
    },
    {
        "Y": "seememory_reserved()",
        "X": "What is memory_cached Deprecated?",
        "Z": "memory_cached Deprecated; seememory_reserved()."
    },
    {
        "Y": "reset_max_memory_cached",
        "X": "What resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device?",
        "Z": "reset_max_memory_cached Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device."
    },
    {
        "Y": "caching allocator",
        "X": "Reset_max_memory_cached Resets the starting point in tracking maximum GPU memory managed by what?",
        "Z": "reset_max_memory_cached Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device."
    },
    {
        "Y": "reset_peak_memory_stats",
        "X": "What resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator?",
        "Z": "reset_peak_memory_stats Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator."
    },
    {
        "Y": "CUDA memory allocator",
        "X": "What does reset_peak_memory_stats do?",
        "Z": "reset_peak_memory_stats Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator."
    },
    {
        "Y": "instantaneous",
        "X": "What type of event occurs at some point?",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point."
    },
    {
        "Y": "instantaneous",
        "X": "What type of event does nvtx.mark describe?",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point."
    },
    {
        "Y": "nvtx.range_push",
        "X": "What pushes a range onto a stack of nested range span?",
        "Z": "nvtx.range_push Pushes a range onto a stack of nested range span."
    },
    {
        "Y": "stack",
        "X": "nvtx.range_push Pushes a range onto what?",
        "Z": "nvtx.range_push Pushes a range onto a stack of nested range span."
    },
    {
        "Y": "nested range spans",
        "X": "nvtx.range_pop Pops a range off of a stack of what?",
        "Z": "nvtx.range_pop Pops a range off of a stack of nested range spans."
    },
    {
        "Y": "nvtx.range_pop",
        "X": "What pops a range off of a stack of nested range spans?",
        "Z": "nvtx.range_pop Pops a range off of a stack of nested range spans."
    },
    {
        "Y": "a stack of nested range spans",
        "X": "nvtx.range_pop Pops a range off of what?",
        "Z": "nvtx.range_pop Pops a range off of a stack of nested range spans."
    },
    {
        "Y": "reduce_scatter",
        "X": "What is feature that reduces the amount of scatter?",
        "Z": "reduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713"
    },
    {
        "Y": "reduce_scatter",
        "X": "What is the term for what?",
        "Z": "reduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713"
    },
    {
        "Y": "placeholder x x ()",
        "X": "What is a placeholder for?",
        "Z": "placeholder x x () {}"
    },
    {
        "Y": "get_attr linear_weight linear.weight",
        "X": "What is function that determines the weight of a linear weight?",
        "Z": "get_attr linear_weight linear.weight () {}"
    },
    {
        "Y": "get_attr linear_weight",
        "X": "What is a linear.weight?",
        "Z": "get_attr linear_weight linear.weight () {}"
    },
    {
        "Y": "add_1",
        "X": "What is call_function?",
        "Z": "call_function add_1 <built-in function add> (x, linear_weight) {}"
    },
    {
        "Y": "call_function add_1",
        "X": "What is built-in function add?",
        "Z": "call_function add_1 <built-in function add> (x, linear_weight) {}"
    },
    {
        "Y": "add_1",
        "X": "What is call_module linear_1 linear?",
        "Z": "call_module linear_1 linear (add_1,) {}"
    },
    {
        "Y": "call_module linear",
        "X": "What is _1 linear?",
        "Z": "call_module linear_1 linear (add_1,) {}"
    },
    {
        "Y": "linear_1",
        "X": "what is call_module?",
        "Z": "call_module linear_1 linear (add_1,) {}"
    },
    {
        "Y": "linear",
        "X": "call_module is what?",
        "Z": "call_module linear_1 linear (add_1,) {}"
    },
    {
        "Y": "call_method",
        "X": "What does relu_1 refer to?",
        "Z": "call_method relu_1 relu (linear_1,) {}"
    },
    {
        "Y": "call_method",
        "X": "What is relu_1 relu (linear_1)?",
        "Z": "call_method relu_1 relu (linear_1,) {}"
    },
    {
        "Y": "-1",
        "X": "What is the decimal value of the call_function sum_1?",
        "Z": "call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}"
    },
    {
        "Y": "-1",
        "X": "\u2018dim\u2019: what?",
        "Z": "call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}"
    },
    {
        "Y": "call_function sum_1",
        "X": "What is built-in method sum...>?",
        "Z": "call_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}"
    },
    {
        "Y": "call_function topk_1",
        "X": "What is built-in method topk?",
        "Z": "call_function topk_1 <built-in method topk \u2026> (sum_1, 3) {}"
    },
    {
        "Y": "topk_1",
        "X": "What is the output output output?",
        "Z": "output output output (topk_1,) {}"
    },
    {
        "Y": "FX Graph Mode Quantization",
        "X": "What is Eager Mode Quantization?",
        "Z": " Eager Mode Quantization FX Graph Mode Quantization"
    },
    {
        "Y": "Eager Mode Quantization FX Graph Mode Quantization",
        "X": "What is FX Graph Mode Quantization?",
        "Z": " Eager Mode Quantization FX Graph Mode Quantization"
    },
    {
        "Y": "Eager Mode Quantization",
        "X": "What is the term for FX Graph Mode Quantization?",
        "Z": " Eager Mode Quantization FX Graph Mode Quantization"
    },
    {
        "Y": "Graph Mode Quantization",
        "X": "What is Eager Mode Quantization FX?",
        "Z": " Eager Mode Quantization FX Graph Mode Quantization"
    },
    {
        "Y": "Release Status beta prototype",
        "X": "What is the status of the beta prototype?",
        "Z": "Release Status beta prototype"
    },
    {
        "Y": "beta prototype",
        "X": "What is the status of the release?",
        "Z": "Release Status beta prototype"
    },
    {
        "Y": "prototype",
        "X": "What is the release status beta?",
        "Z": "Release Status beta prototype"
    },
    {
        "Y": "Operator Fusion Manual",
        "X": "What type of manual is used?",
        "Z": "Operator Fusion Manual Automatic"
    },
    {
        "Y": "Automatic",
        "X": "What type of manual is the operator Fusion?",
        "Z": "Operator Fusion Manual Automatic"
    },
    {
        "Y": "Operator Fusion Manual Automatic",
        "X": "What is manual?",
        "Z": "Operator Fusion Manual Automatic"
    },
    {
        "Y": "Automatic",
        "X": "What type of manual is the Quant/DeQuant Placement Manual?",
        "Z": "Quant/DeQuant Placement Manual Automatic"
    },
    {
        "Y": "Automatic",
        "X": "Quant/DeQuant Placement Manual is what?",
        "Z": "Quant/DeQuant Placement Manual Automatic"
    },
    {
        "Y": "Quant/DeQuant Placement Manual",
        "X": "What is Automatic Placement Manual?",
        "Z": "Quant/DeQuant Placement Manual Automatic"
    },
    {
        "Y": "Quantizing Modules",
        "X": "What is Supported Supported?",
        "Z": "Quantizing Modules Supported Supported"
    },
    {
        "Y": "Quantizing Modules Supported Supported",
        "X": "What are Quantizing Modules Supported Supported?",
        "Z": "Quantizing Modules Supported Supported"
    },
    {
        "Y": "Quantizing Modules Supported",
        "X": "What type of Modules are Supported?",
        "Z": "Quantizing Modules Supported Supported"
    },
    {
        "Y": "Quantizing Functionals/Torch Ops Manual",
        "X": "What type of manual is automatic?",
        "Z": "Quantizing Functionals/Torch Ops Manual Automatic"
    },
    {
        "Y": "Automatic",
        "X": "What type of manual is Quantizing Functionals/Torch Ops Manual?",
        "Z": "Quantizing Functionals/Torch Ops Manual Automatic"
    },
    {
        "Y": "Torch Ops Manual Automatic",
        "X": "Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quantizing Functionals/ Quant",
        "Z": "Quantizing Functionals/Torch Ops Manual Automatic"
    },
    {
        "Y": "Quantizing Functionals",
        "X": "What does Torch Ops Manual Automatically do?",
        "Z": "Quantizing Functionals/Torch Ops Manual Automatic"
    },
    {
        "Y": "Customization",
        "X": "What is limited support for?",
        "Z": "Support for Customization Limited Support Fully Supported"
    },
    {
        "Y": "Fully Supported",
        "X": "What type of Support does Customization Limited Support offer?",
        "Z": "Support for Customization Limited Support Fully Supported"
    },
    {
        "Y": "Customization Limited Support Fully Supported",
        "X": "Support for what type of support?",
        "Z": "Support for Customization Limited Support Fully Supported"
    },
    {
        "Y": "Static",
        "X": "Quantization Mode Support Post Training Quantization: Dynamic, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training Quantization",
        "Z": "Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static"
    },
    {
        "Y": "Weight Only",
        "X": "Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight Only, Quantiztion Aware Training: Static Post Training Quant",
        "Z": "Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static"
    },
    {
        "Y": "Quantization Mode Support",
        "X": "What is the term for Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training?",
        "Z": "Quantization Mode Support Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static Post Training Quantization: Static, Dynamic, Weight OnlyQuantiztion Aware Training: Static"
    },
    {
        "Y": "Module",
        "X": "What type of torch may need some refactoring to make the model compatible with FX Graph Mode Quantization?",
        "Z": "Input/Output Model Type torch.nn.Module torch.nn.Module(May need some refactors to make the model compatible with FX Graph Mode Quantization)"
    },
    {
        "Y": "FX Graph Mode Quantization",
        "X": "What may need some refactoring to make the model compatible with?",
        "Z": "Input/Output Model Type torch.nn.Module torch.nn.Module(May need some refactors to make the model compatible with FX Graph Mode Quantization)"
    },
    {
        "Y": "FX Graph Mode Quantization",
        "X": "What does the torch need some refactors to make it compatible with?",
        "Z": "Input/Output Model Type torch.nn.Module torch.nn.Module(May need some refactors to make the model compatible with FX Graph Mode Quantization)"
    },
    {
        "Y": "torch.quantization",
        "X": "What module implements the functions you call directly to convert your model from FP32 to quantized form?",
        "Z": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example theprepare()is used in post training quantization to prepares your model for the calibration step andconvert()actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu."
    },
    {
        "Y": "theprepare()is",
        "X": "What is used in post training quantization to prepare your model for the calibration step?",
        "Z": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example theprepare()is used in post training quantization to prepares your model for the calibration step andconvert()actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu."
    },
    {
        "Y": "conv+relu",
        "X": "What is an example of a critical fusion function?",
        "Z": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example theprepare()is used in post training quantization to prepares your model for the calibration step andconvert()actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu."
    },
    {
        "Y": "int8",
        "X": "What are the weights converted to?",
        "Z": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example theprepare()is used in post training quantization to prepares your model for the calibration step andconvert()actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu."
    },
    {
        "Y": "conv+relu",
        "X": "What is an example of a critical fusion?",
        "Z": "torch.quantization This module implements the functions you call directly to convert your model from FP32 to quantized form. For example theprepare()is used in post training quantization to prepares your model for the calibration step andconvert()actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu."
    },
    {
        "Y": "conv + relu",
        "X": "What modules does torch.nn.intrinsic implement?",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can then be quantized."
    },
    {
        "Y": "quantized",
        "X": "What can be done with the combined modules conv + relu?",
        "Z": "torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can then be quantized."
    },
    {
        "Y": "torch.nn.intrinsic.qat",
        "X": "What is module that implements the versions of the fused operations needed for quantization aware training?",
        "Z": "torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training."
    },
    {
        "Y": "quantization aware training",
        "X": "This module implements the versions of those fused operations needed for what?",
        "Z": "torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training."
    },
    {
        "Y": "torch.nn.intrinsic.qat",
        "X": "What module implements the versions of those fused operations needed for quantization aware training?",
        "Z": "torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for quantization aware training."
    },
    {
        "Y": "conv + relu",
        "X": "What is an example of a fused operation implemented by torch.nn.intrinsic.quantized?",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations like conv + relu."
    },
    {
        "Y": "quantized implementations",
        "X": "What does torch.nn.intrinsic.quantized implement?",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations like conv + relu."
    },
    {
        "Y": "conv + relu",
        "X": "What are some of the quantized implementations of fused operations?",
        "Z": "torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations like conv + relu."
    },
    {
        "Y": "FP32",
        "X": "In what platform do the nn modulesConv2d() andLinear() run?",
        "Z": "torch.nn.qat This module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the effect of INT8 quantization."
    },
    {
        "Y": "INT8 quantization",
        "X": "What effect does the rounding of the nn modules simulate?",
        "Z": "torch.nn.qat This module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the effect of INT8 quantization."
    },
    {
        "Y": "FP32",
        "X": "In what platform do the modulesConv2d() andLinear() run?",
        "Z": "torch.nn.qat This module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the effect of INT8 quantization."
    },
    {
        "Y": "INT8",
        "X": "What quantization does torch.nn.qat simulate?",
        "Z": "torch.nn.qat This module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the effect of INT8 quantization."
    },
    {
        "Y": "torch.nn.quantized",
        "X": "What module implements the quantized versions of the nn layers?",
        "Z": "torch.nn.quantized This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` andtorch.nn.ReLU."
    },
    {
        "Y": "torch.nn.Conv2d module implements  a quantized version of the nn layers",
        "X": "What does torch.nn.Conv2d module implement ?",
        "Z": "torch.nn.quantized This module implements the quantized versions of the nn layers such as ~`torch.nn.Conv2d` andtorch.nn.ReLU."
    },
    {
        "Y": "Dynamically quantized Linear,LSTM,LSTMCell,GRUCell, andRNNCell ",
        "X": "How is torch.nn.quantized.dynamically?",
        "Z": "torch.nn.quantized.dynamic Dynamically quantizedLinear,LSTM,LSTMCell,GRUCell, andRNNCell."
    },
    {
        "Y": "torch.nn.quantized",
        "X": "Dynamically quantized Linear,LSTM,LSTMCell,GRUCell, andRNNCell?",
        "Z": "torch.nn.quantized.dynamic Dynamically quantizedLinear,LSTM,LSTMCell,GRUCell, andRNNCell."
    },
    {
        "Y": "graph leaves",
        "X": "With respect to what  the backward Computes the sum of gradients of given tensors ?",
        "Z": "backward Computes the sum of gradients of given tensors with respect to graph leaves."
    },
    {
        "Y": "backward",
        "X": "What computes the sum of gradients of given tensors with respect to graph leaves?",
        "Z": "backward Computes the sum of gradients of given tensors with respect to graph leaves."
    },
    {
        "Y": "grad Computes",
        "X": "What returns the sum of gradients of outputs with respect to the inputs?",
        "Z": "grad Computes and returns the sum of gradients of outputs with respect to the inputs."
    },
    {
        "Y": "grad",
        "X": "What computes and returns the sum of gradients of outputs with respect to the inputs?",
        "Z": "grad Computes and returns the sum of gradients of outputs with respect to the inputs."
    },
    {
        "Y": "Jacobian",
        "X": "What type of function does functional.jacobian Function compute?",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function."
    },
    {
        "Y": "computes the Jacobian of a given function",
        "X": "What does functional.jacobian Function do?",
        "Z": "functional.jacobian Function that computes the Jacobian of a given function."
    },
    {
        "Y": "functional.hessian Function",
        "X": "What computes the Hessian of a given scalar function?",
        "Z": "functional.hessian Function that computes the Hessian of a given scalar function."
    },
    {
        "Y": "computes the Hessian of a given scalar function",
        "X": "What is a functional.hessian function?",
        "Z": "functional.hessian Function that computes the Hessian of a given scalar function."
    },
    {
        "Y": "functional.vjp Function",
        "X": "What computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs?",
        "Z": "functional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs."
    },
    {
        "Y": "the Jacobian",
        "X": "A functional.vjp Function computes the dot product between a vector vand and what of the given function?",
        "Z": "functional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs."
    },
    {
        "Y": "the dot product",
        "X": "What does functional.vjp compute between a vector vand the Jacobian of the given function at the point given by the inputs?",
        "Z": "functional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs."
    },
    {
        "Y": "functional",
        "X": "What type of function computes the dot product between a vector vand the Jacobian of the given function at the point given by the input",
        "Z": "functional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs."
    },
    {
        "Y": "the Jacobian",
        "X": "The functional.vjp Function computes the dot product between a vector vand and what?",
        "Z": "functional.vjp Function that computes the dot product between a vector vand the Jacobian of the given function at the point given by the inputs."
    },
    {
        "Y": "the Jacobian",
        "X": "What is the dot product between a given function and a vector v?",
        "Z": "functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v."
    },
    {
        "Y": "the point given by the inputs and a vector v",
        "X": "Where is the dot product computed between the Jacobian of the given function?",
        "Z": "functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v."
    },
    {
        "Y": "functional.jvp Function",
        "X": "What computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v?",
        "Z": "functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v."
    },
    {
        "Y": "functional.vhp Function",
        "X": "What computes the dot product between a vector vand the Hessian of a given scalar function at the point given by",
        "Z": "functional.vhp Function that computes the dot product between a vector vand the Hessian of a given scalar function at the point given by the inputs."
    },
    {
        "Y": "the Hessian",
        "X": "What is the vector vand of a given scalar function?",
        "Z": "functional.vhp Function that computes the dot product between a vector vand the Hessian of a given scalar function at the point given by the inputs."
    },
    {
        "Y": "the dot product",
        "X": "What does functional.vhp compute between a vector vand the Hessian of a given scalar function at the point given",
        "Z": "functional.vhp Function that computes the dot product between a vector vand the Hessian of a given scalar function at the point given by the inputs."
    },
    {
        "Y": "functional.hvp Function",
        "X": "What computes the dot product between the Hessian of a given scalar function and a vector vat the point given by",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector vat the point given by the inputs."
    },
    {
        "Y": "the Hessian",
        "X": "What is the dot product between a given scalar function and a vector vat?",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector vat the point given by the inputs."
    },
    {
        "Y": "the dot product",
        "X": "What does functional.hvp compute between the Hessian of a given scalar function and a vector vat the point given",
        "Z": "functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector vat the point given by the inputs."
    },
    {
        "Y": "torch.Tensor.grad",
        "X": "What attribute is None bydefault?",
        "Z": "torch.Tensor.grad This attribute is None bydefault and becomes a Tensor the first time a call to backward() computes gradients forself."
    },
    {
        "Y": "Tensor",
        "X": "What requires gradients to be computed for?",
        "Z": "torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor,False otherwise."
    },
    {
        "Y": "leaf Tensors",
        "X": "What are all Tensors that require grad which is False?",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention."
    },
    {
        "Y": "leaf",
        "X": "What type of Tensor is a torch?",
        "Z": "torch.Tensor.is_leaf All Tensors that have requires_gradwhich is False wii be leaf Tensors by convention."
    },
    {
        "Y": "current tensor w.r.t.",
        "X": "What does torch.Tensor.backward compute the gradient of?",
        "Z": "torch.Tensor.backward([gradient,\u00a0\u2026]) Computes the gradient of current tensor w.r.t."
    },
    {
        "Y": "current graph",
        "X": "What does torch.Tensor.detach remove a new Tensor from?",
        "Z": "torch.Tensor.detach Returns a new Tensor, detached from the current graph."
    },
    {
        "Y": "Tensor",
        "X": "What is detached from the current graph?",
        "Z": "torch.Tensor.detach Returns a new Tensor, detached from the current graph."
    },
    {
        "Y": "leaf",
        "X": "What does the Tensor become?",
        "Z": "torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf."
    },
    {
        "Y": "leaf",
        "X": "What does torch.Tensor.detach_ Detaches the Tensor from the graph that created it?",
        "Z": "torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf."
    },
    {
        "Y": "backward hook",
        "X": "What type of hook does torch.Tensor.register_hook register?",
        "Z": "torch.Tensor.register_hook(hook) Registers a backward hook."
    },
    {
        "Y": "backward hook",
        "X": "What does torch.Tensor.register_hook(hook) register?",
        "Z": "torch.Tensor.register_hook(hook) Registers a backward hook."
    },
    {
        "Y": "non-leaf Tensors",
        "X": "What type of Tensors does torch.Tensor.retain_grad() enable?",
        "Z": "torch.Tensor.retain_grad() Enables .grad attribute for non-leaf Tensors."
    },
    {
        "Y": "Function.backward",
        "X": "What defines a formula for differentiating the operation?",
        "Z": "Function.backward Defines a formula for differentiating the operation."
    },
    {
        "Y": "Function.forward Performs the operation",
        "X": "What does Function.forward do?",
        "Z": "Function.forward Performs the operation."
    },
    {
        "Y": "Function.forward",
        "X": "What performs the operation?",
        "Z": "Function.forward Performs the operation."
    },
    {
        "Y": "in-place operation",
        "X": "How are tensors modified in function._ContextMethodMixin.mark_dirty?",
        "Z": "function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation."
    },
    {
        "Y": "in-place operation",
        "X": "In what operation are tensors modified?",
        "Z": "function._ContextMethodMixin.mark_dirty Marks given tensors as modified in an in-place operation."
    },
    {
        "Y": "non-differentiable",
        "X": "Function._ContextMethodMixin.mark_non_differentiable Marks outputs as what?",
        "Z": "function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable."
    },
    {
        "Y": "function._ContextMethodMixin.mark_non_differentiable",
        "X": "What mark does function._ContextMethodMixin.mark_non_differentiable?",
        "Z": "function._ContextMethodMixin.mark_non_differentiable Marks outputs as non-differentiable."
    },
    {
        "Y": "tobackward",
        "X": "To what direction does function._ContextMethodMixin.save_for_backward save given tensor",
        "Z": "function._ContextMethodMixin.save_for_backward Saves given tensors for a future call tobackward()."
    },
    {
        "Y": "given tensors",
        "X": "What does function._ContextMethodMixin.save_for_backward save?",
        "Z": "function._ContextMethodMixin.save_for_backward Saves given tensors for a future call tobackward()."
    },
    {
        "Y": "grad tensors",
        "X": "What does function._ContextMethodMixin.set_materialize_grads set whether to materialize output?",
        "Z": "function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors."
    },
    {
        "Y": "whether to materialize output grad tensors",
        "X": "What does function._ContextMethodMixin.set_materialize_grads Set?",
        "Z": "function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors."
    },
    {
        "Y": "function._ContextMethodMixin",
        "X": "What function sets whether to materialize output grad tensors?",
        "Z": "function._ContextMethodMixin.set_materialize_grads Sets whether to materialize output grad tensors."
    },
    {
        "Y": "gradcheck Check gradients",
        "X": "What is computed via small finite differences against analytical gradients w.r.t?",
        "Z": "gradcheck Check gradients computed via small finite differences against analytical gradients w.r.t."
    },
    {
        "Y": "gradcheck",
        "X": "What is check gradients computed via small finite differences against analytical gradients w.r.t.",
        "Z": "gradcheck Check gradients computed via small finite differences against analytical gradients w.r.t."
    },
    {
        "Y": "gradgradcheck Check gradients of gradients computed via small finite differences against analytical gradients",
        "X": "What does gradgradcheck do?",
        "Z": "gradgradcheck Check gradients of gradients computed via small finite differences against analytical gradients w.r.t."
    },
    {
        "Y": "gradgradcheck",
        "X": "What check gradients of gradients computed via small finite differences against analytical gradients?",
        "Z": "gradgradcheck Check gradients of gradients computed via small finite differences against analytical gradients w.r.t."
    },
    {
        "Y": "Chrome tracing tools file",
        "X": "What browser does profiler.profile.export_chrome_trace export an EventList as?",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file."
    },
    {
        "Y": "EventList",
        "X": "What does profiler.profile.export_chrome_trace export?",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file."
    },
    {
        "Y": "profiler.profile.export_chrome_trace",
        "X": "What does export an EventList as a Chrome tracing tools file?",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file."
    },
    {
        "Y": "all function events over their keys ",
        "X": "What does profiler.profile.key_averages average over their keys?",
        "Z": "profiler.profile.key_averages Averages all function events over their keys."
    },
    {
        "Y": "profiler.profile.key_averages",
        "X": "What averages all function events over their keys?",
        "Z": "profiler.profile.key_averages Averages all function events over their keys."
    },
    {
        "Y": "otal time spent on CPU obtained as a sum of all self times across all the events",
        "X": "What does profiler.profile.self_cpu_time_total return?",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events."
    },
    {
        "Y": "profiler.profile.self_cpu_time_total",
        "X": "What returns total time spent on CPU obtained as a sum of all self times across all events?",
        "Z": "profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events."
    },
    {
        "Y": "average of all events",
        "X": "What does profiler.profile.total_average mean?",
        "Z": "profiler.profile.total_average Averages all events."
    },
    {
        "Y": "profiler.profile.total_average",
        "X": "What is the name of all events?",
        "Z": "profiler.profile.total_average Averages all events."
    },
    {
        "Y": "profiler.load_nvprof",
        "X": "What opens an nvprof trace file and parses autograd annotations?",
        "Z": "profiler.load_nvprof Opens an nvprof trace file and parses autograd annotations."
    },
    {
        "Y": "autograd annotations",
        "X": "What does profiler.load_nvprof parse?",
        "Z": "profiler.load_nvprof Opens an nvprof trace file and parses autograd annotations."
    },
    {
        "Y": "Linux",
        "X": "What operating system is ppc64le?",
        "Z": "Linux (ppc64le) GPU \u2014  \u2014"
    },
    {
        "Y": "ppc64le",
        "X": "What is the name of Linux's GPU?",
        "Z": "Linux (ppc64le) GPU \u2014  \u2014"
    },
    {
        "Y": "aarch64",
        "X": "What version of of Linux for pytorch?",
        "Z": "Linux (aarch64) CPU   "
    },
    {
        "Y": "aarch64",
        "X": "Which  Linux's CPU is for pytorch?",
        "Z": "Linux (aarch64) CPU   "
    },
    {
        "Y": "GPU support",
        "X": "What kind of support does NumPy have?",
        "Z": "torch a Tensor library like NumPy, with strong GPU support"
    },
    {
        "Y": "NumPy",
        "X": "What is a Tensor library similar to?",
        "Z": "torch a Tensor library like NumPy, with strong GPU support"
    },
    {
        "Y": "tape-based automatic differentiation library",
        "X": "What is torch.autograd?",
        "Z": "torch.autograd a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch"
    },
    {
        "Y": "all differentiable Tensor operations",
        "X": "What does torch.autograd support?",
        "Z": "torch.autograd a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch"
    },
    {
        "Y": "torch.autograd",
        "X": "What is tape-based automatic differentiation library that supports all differentiable Tensor operations in torch?",
        "Z": "torch.autograd a tape-based automatic differentiation library that supports all differentiable Tensor operations in torch"
    },
    {
        "Y": "autograd",
        "X": "What is torch.nn deeply integrated with?",
        "Z": "torch.nn a neural networks library deeply integrated with autograd designed for maximum flexibility"
    },
    {
        "Y": "maximum flexibility",
        "X": "What is torch.nn designed for?",
        "Z": "torch.nn a neural networks library deeply integrated with autograd designed for maximum flexibility"
    },
    {
        "Y": "autograd",
        "X": "What is torch.nn a neural networks library deeply integrated with?",
        "Z": "torch.nn a neural networks library deeply integrated with autograd designed for maximum flexibility"
    },
    {
        "Y": "Python",
        "X": "What language does torch.multiprocessing use?",
        "Z": "torch.multiprocessing Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training"
    },
    {
        "Y": "data loading and Hogwild training",
        "X": "What is torch.multiprocessing useful for?",
        "Z": "torch.multiprocessing Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training"
    },
    {
        "Y": "magical memory sharing",
        "X": "What do torch.multiprocessing haeve for torch Tensors  ?",
        "Z": "torch.multiprocessing Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training"
    },
    {
        "Y": "torch Tensors across processes. Useful for data loading and Hogwild training",
        "X": "What magical memory sharing does torch.multiprocessing Python multiprocessing have?",
        "Z": "torch.multiprocessing Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training"
    },
    {
        "Y": "torch.utils DataLoader",
        "X": "What is DataLoader and other utility function ?",
        "Z": "torch.utils DataLoader and other utility functions for convenience"
    },
    {
        "Y": "convenience",
        "X": "What is the purpose of torch.utils DataLoader?",
        "Z": "torch.utils DataLoader and other utility functions for convenience"
    }
]
