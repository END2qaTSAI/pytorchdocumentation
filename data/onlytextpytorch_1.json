{
    "What is the name of the context manager used when you are certain your operations will have no interactions with autograd?": {
        "answer": "tono_grad",
        "question": "What is the name of the context manager used when you are certain your operations will have no interactions with autograd?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "How does code run under InferenceMode get better performance?": {
        "answer": "view tracking and version counter bumps",
        "question": "How does code run under InferenceMode get better performance?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is the context manager?": {
        "answer": "thread local",
        "question": "What is the context manager?",
        "context": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does InferenceMode function as?": {
        "answer": "decorator",
        "question": "What does InferenceMode function as?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does the context manager function as a decorator?": {
        "answer": "instantiate with parenthesis",
        "question": "What does the context manager function as a decorator?",
        "context": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is the name of the new context manager?": {
        "answer": "Note",
        "question": "What is the name of the new context manager?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What function does this context manager function as?": {
        "answer": "decorator",
        "question": "What function does this context manager function as?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What do you need to make sure to do?": {
        "answer": "instantiate with parenthesis",
        "question": "What do you need to make sure to do?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is one of several mechanisms that can enable or disable gradients locally?": {
        "answer": "Note Inference mode",
        "question": "What is one of several mechanisms that can enable or disable gradients locally?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is mode(bool)?": {
        "answer": "Flag",
        "question": "What is mode(bool)?",
        "context": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What rule is used to estimate y,dxydxalongdim?": {
        "answer": "trapezoid rule",
        "question": "What rule is used to estimate y,dxydxalongdim?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What is the value of the function to integrate?": {
        "answer": "x(Tensor)",
        "question": "What is the value of the function to integrate?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What do intervals on which the functionyis is decreasing contribute negatively to the estimated integral?": {
        "answer": "Ifxis not in ascending order",
        "question": "What do intervals on which the functionyis is decreasing contribute negatively to the estimated integral?",
        "context": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What is the dimension along which to integrate?": {
        "answer": "dim(int)",
        "question": "What is the dimension along which to integrate?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What does dim(int) use by default?": {
        "answer": "last dimension",
        "question": "What does dim(int) use by default?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What does dim(int) mean?": {
        "answer": "last dimension",
        "question": "What does dim(int) mean?",
        "context": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "Ifinputis of typeFloatTensororDoubleTensor should be what?": {
        "answer": "integer",
        "question": "Ifinputis of typeFloatTensororDoubleTensor should be what?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the number to be multiplied to each element ofinput out?": {
        "answer": "output tensor",
        "question": "What is the number to be multiplied to each element ofinput out?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the corresponding element of the tensorinputis multiplied by?": {
        "answer": "Tensorother",
        "question": "What is the corresponding element of the tensorinputis multiplied by?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the resulting tensor?": {
        "answer": "returned",
        "question": "What is the resulting tensor?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What are the shapes ofinputandothermust?": {
        "answer": "bebroadcastable",
        "question": "What are the shapes ofinputandothermust?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the first multiplicand tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the first multiplicand tensor?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the name of the function that generates what?": {
        "answer": "Alias fortorch.ge()",
        "question": "What is the name of the function that generates what?",
        "context": "Alias fortorch.ge(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal"
    },
    "What is the return of each row of theinputtensor in the given dimensiondim?": {
        "answer": "the log of summed exponentials",
        "question": "What is the return of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the computation of summed exponentials of each row of theinputtensor in the given dimensiondim?": {
        "answer": "numerically stabilized",
        "question": "What is the computation of summed exponentials of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the result for summation indexjjgiven bydimand other indicesiiii?": {
        "answer": "IfkeepdimisTrue",
        "question": "What is the result for summation indexjjgiven bydimand other indicesiiii?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "How many dimensions does the output tensor have?": {
        "answer": "1",
        "question": "How many dimensions does the output tensor have?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What is the input tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the result of summation indexjjgiven bydim?": {
        "answer": "IfkeepdimisTrue",
        "question": "What is the result of summation indexjjgiven bydim?",
        "context": "For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the dimension of python:ints?": {
        "answer": "dim",
        "question": "What is the dimension of python:ints?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the name of the output tensor that hasdimretained or not?": {
        "answer": "keepdim",
        "question": "What is the name of the output tensor that hasdimretained or not?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the maximum value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What is the maximum value of all elements in?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What does this function produce deterministic (sub)gradients unlikemax(dim=0) input(Tensor)?": {
        "answer": "input tensor",
        "question": "What does this function produce deterministic (sub)gradients unlikemax(dim=0) input(Tensor)?",
        "context": "This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the maximum value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the maximum value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the index location of each maximum value found?": {
        "answer": "Andindicesis",
        "question": "What is the index location of each maximum value found?",
        "context": "This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is argmax?": {
        "answer": "Andindicesis the index location of each maximum value found",
        "question": "What is argmax?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the name of the output tensor?": {
        "answer": "IfkeepdimisTrue",
        "question": "What is the name of the output tensor?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "IfkeepdimisTrue, the output tensors have what?": {
        "answer": "1 fewer dimension thaninput",
        "question": "IfkeepdimisTrue, the output tensors have what?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the name of the input tensor?": {
        "answer": "Note",
        "question": "What is the name of the input tensor?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does the output tensors have the same size asinput?": {
        "answer": "IfkeepdimisTrue",
        "question": "What does the output tensors have the same size asinput?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does the output tensors have 1 fewer dimension thaninput?": {
        "answer": "Note",
        "question": "What does the output tensors have 1 fewer dimension thaninput?",
        "context": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the name of the output tensors of the same size asinput?": {
        "answer": "IfkeepdimisTrue",
        "question": "What is the name of the output tensors of the same size asinput?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the name of the tensor in the given dimensiondim?": {
        "answer": "Note",
        "question": "What is the name of the tensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "Where are the output tensors of the same size asinput?": {
        "answer": "size 1",
        "question": "Where are the output tensors of the same size asinput?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What happens if there are multiple maximum values in a reduced row?": {
        "answer": "the indices of the first maximal value are returned",
        "question": "What happens if there are multiple maximum values in a reduced row?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the dimension to reduce?": {
        "answer": "dim(int)",
        "question": "What is the dimension to reduce?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the default for the output tensors?": {
        "answer": "False",
        "question": "What is the default for the output tensors?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What are returned if there are multiple maximum values in a reduced row?": {
        "answer": "indices of the first maximal value",
        "question": "What are returned if there are multiple maximum values in a reduced row?",
        "context": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the result tuple of two output tensors?": {
        "answer": "Default:False",
        "question": "What is the result tuple of two output tensors?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the default for output tensors?": {
        "answer": "False",
        "question": "What is the default for output tensors?",
        "context": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is Default:False. out(tuple,optional)?": {
        "answer": "the result tuple of two output tensors",
        "question": "What is Default:False. out(tuple,optional)?",
        "context": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the name of the file that loads the Torch serialized object at the given URL?": {
        "answer": "Moved totorch.hub",
        "question": "What is the name of the file that loads the Torch serialized object at the given URL?",
        "context": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "Where is the Torch serialized object moved?": {
        "answer": "Loads the Torch serialized object at the given URL",
        "question": "Where is the Torch serialized object moved?",
        "context": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If downloaded file is a zip file, it will be automatically what?": {
        "answer": "decompressed",
        "question": "If downloaded file is a zip file, it will be automatically what?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If the object is already present in what directory, it's deserialized and returned?": {
        "answer": "model_dir",
        "question": "If the object is already present in what directory, it's deserialized and returned?",
        "context": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default value of model_dirishub_dir>/checkpointswherehub_diris the directory returned?": {
        "answer": "byget_dir()",
        "question": "What is the default value of model_dirishub_dir>/checkpointswherehub_diris the directory returned?",
        "context": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the name of the object to download model_dir?": {
        "answer": "URL",
        "question": "What is the name of the object to download model_dir?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default value of model_dirishub_dir>/checkpointswherehub_diris the directory returned byget_dir": {
        "answer": "the object is already present inmodel_dir",
        "question": "What is the default value of model_dirishub_dir>/checkpointswherehub_diris the directory returned byget_dir",
        "context": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What happens if downloaded file is a zip file?": {
        "answer": "decompressed",
        "question": "What happens if downloaded file is a zip file?",
        "context": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What happens if the object is already present inmodel_dir?": {
        "answer": "it\u2019s deserialized and returned",
        "question": "What happens if the object is already present inmodel_dir?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What does url specify how to remap storage locations?": {
        "answer": "a function or a dict",
        "question": "What does url specify how to remap storage locations?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If the object is already present inmodel_dir, it's deserialized and returned?": {
        "answer": "If the object is already present inmodel_dir",
        "question": "If the object is already present inmodel_dir, it's deserialized and returned?",
        "context": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default value of model_dirishub_dir>/checkpointswherehub_diris?": {
        "answer": "byget_dir()",
        "question": "What is the default value of model_dirishub_dir>/checkpointswherehub_diris?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What does stderr display?": {
        "answer": "a progress bar",
        "question": "What does stderr display?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is a progress bar displayed to?": {
        "answer": "stderr",
        "question": "What is a progress bar displayed to?",
        "context": "url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default for the object to download model_dir?": {
        "answer": "True",
        "question": "What is the default for the object to download model_dir?",
        "context": "url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the name of the directory in which to save the object map_location?": {
        "answer": "a function or a dict",
        "question": "What is the name of the directory in which to save the object map_location?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the name of the directory in which to display a progress bar?": {
        "answer": "stderr",
        "question": "What is the name of the directory in which to display a progress bar?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default for model_dir(string,optional)?": {
        "answer": "True",
        "question": "What is the default for model_dir(string,optional)?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is a function or dict specifying how to remap storage locations?": {
        "answer": "map_location",
        "question": "What is a function or dict specifying how to remap storage locations?",
        "context": "map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the first eight or more digits of the SHA256 hash of the contents of the file?": {
        "answer": "Default: True check_hash",
        "question": "What is the first eight or more digits of the SHA256 hash of the contents of the file?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the hash used to ensure?": {
        "answer": "unique names",
        "question": "What is the hash used to ensure?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default for the filename part of the URL?": {
        "answer": "False",
        "question": "What is the default for the filename part of the URL?",
        "context": "map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the hash used to do?": {
        "answer": "ensure unique names and to verify the contents of the file",
        "question": "What is the hash used to do?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the name for the downloaded file?": {
        "answer": "False file_name(string,optional)",
        "question": "What is the name for the downloaded file?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What filename will be used if not set?": {
        "answer": "fromurl",
        "question": "What filename will be used if not set?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "Ifdimis a list of dimensions, what over all of them?": {
        "answer": "reduce",
        "question": "Ifdimis a list of dimensions, what over all of them?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "IfkeepdimisTrue, the output tensor is of what size?": {
        "answer": "size 1",
        "question": "IfkeepdimisTrue, the output tensor is of what size?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What does the input tensor return?": {
        "answer": "the mean value of each row of theinputtensor in the given dimensiondim",
        "question": "What does the input tensor return?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What is the value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is an intortuple of python:ints?": {
        "answer": "dim",
        "question": "What is an intortuple of python:ints?",
        "context": "Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What does the input tensor return in the given dimensiondim?": {
        "answer": "the mean value of each row of theinputtensor",
        "question": "What does the input tensor return in the given dimensiondim?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What is intortuple of python:ints?": {
        "answer": "dim",
        "question": "What is intortuple of python:ints?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What is the quantile of each row of theinputtensor along the dimensiondim?": {
        "answer": "q-th",
        "question": "What is the quantile of each row of theinputtensor along the dimensiondim?",
        "context": "Computes the q-th quantiles of each row of theinputtensor\nalong the dimensiondim. To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What method computes the result of a+(b-a)*fraction?": {
        "answer": "linear interpolation",
        "question": "What method computes the result of a+(b-a)*fraction?",
        "context": "Computes the q-th quantiles of each row of theinputtensor\nalong the dimensiondim. To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "To compute the quantile, we map what to find the location of the quantile in the sorted input?": {
        "answer": "q in [0, 1] to the range of indices [0, n]",
        "question": "To compute the quantile, we map what to find the location of the quantile in the sorted input?",
        "context": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is the method used to compute the quantile?": {
        "answer": "linear interpolation",
        "question": "What is the method used to compute the quantile?",
        "context": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is a+(b-a)*fraction?": {
        "answer": "fractional part of the computed quantile index",
        "question": "What is a+(b-a)*fraction?",
        "context": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is the first dimension of the output?": {
        "answer": "Ifqis a 1D tensor",
        "question": "What is the first dimension of the output?",
        "context": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What resulted in theinputtensor being flattened before computation?": {
        "answer": "defaultdimisNone",
        "question": "What resulted in theinputtensor being flattened before computation?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is q(floatorTensor)?": {
        "answer": "a scalar or 1D tensor of values in the range",
        "question": "What is q(floatorTensor)?",
        "context": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "Ifqis a 1D tensor, what is the first dimension of the output?": {
        "answer": "the first dimension of the output represents the quantiles",
        "question": "Ifqis a 1D tensor, what is the first dimension of the output?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What does q(floatorTensor) mean?": {
        "answer": "a scalar or 1D tensor of values in the range",
        "question": "What does q(floatorTensor) mean?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "Which tensor hasdimretained or not?": {
        "answer": "keepdim(bool)",
        "question": "Which tensor hasdimretained or not?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What does the vdot(a, b) function handle complex numbers differently than dot(a, b)?": {
        "answer": "Computes the dot product of two 1D tensors",
        "question": "What does the vdot(a, b) function handle complex numbers differently than dot(a, b)?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What function handles complex numbers differently than dot?": {
        "answer": "vdot(a, b)",
        "question": "What function handles complex numbers differently than dot?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is used for the calculation of the dot product if the first argument is complex?": {
        "answer": "complex conjugate",
        "question": "What is used for the calculation of the dot product if the first argument is complex?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What only supports computing the dot product of two 1D tensors with the same number of elements?": {
        "answer": "torch.vdot",
        "question": "What only supports computing the dot product of two 1D tensors with the same number of elements?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is the first tensor in the dot product?": {
        "answer": "input(Tensor)",
        "question": "What is the first tensor in the dot product?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is the conjugate used for the first tensor in the dot product?": {
        "answer": "if it\u2019s complex",
        "question": "What is the conjugate used for the first tensor in the dot product?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What window function is the full window size?": {
        "answer": "Hann",
        "question": "What window function is the full window size?",
        "context": "Hann window function. whereNNNis the full window size. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the NNN?": {
        "answer": "full window size",
        "question": "What is the NNN?",
        "context": "Hann window function. whereNNNis the full window size. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is NNN?": {
        "answer": "full window size",
        "question": "What is NNN?",
        "context": "whereNNNis the full window size. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is a positive integer controlling the returned window size?": {
        "answer": "inputwindow_lengthis",
        "question": "What is a positive integer controlling the returned window size?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the NNNin above formula?": {
        "answer": "ifperiodicis true",
        "question": "What is the NNNin above formula?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What formula does the NNNin above formula always use?": {
        "answer": "havetorch.hann_window",
        "question": "What formula does the NNNin above formula always use?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the name of the name of a window that has been used as a periodic window with functions liketorch.stft": {
        "answer": "Note",
        "question": "What is the name of the name of a window that has been used as a periodic window with functions liketorch.stft",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the size of returned window periodic?": {
        "answer": "window_length",
        "question": "What is the size of returned window periodic?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What does return a symmetric window?": {
        "answer": "If False",
        "question": "What does return a symmetric window?",
        "context": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired data type of returned tensor?": {
        "answer": "dtype",
        "question": "What is the desired data type of returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default for a global default?": {
        "answer": "ifNone",
        "question": "What is the default for a global default?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What are supported by ifNone?": {
        "answer": "floating point types",
        "question": "What are supported by ifNone?",
        "context": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What type of tensor is supported?": {
        "answer": "floating point types",
        "question": "What type of tensor is supported?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired layout of returned window tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of returned window tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is supported as a layout of returned window tensor?": {
        "answer": "Onlytorch.strided",
        "question": "What is supported as a layout of returned window tensor?",
        "context": "periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What does periodic mean?": {
        "answer": "If True",
        "question": "What does periodic mean?",
        "context": "periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default for dtype?": {
        "answer": "ifNone",
        "question": "What is the default for dtype?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is supported by dtype?": {
        "answer": "Onlytorch.strided",
        "question": "What is supported by dtype?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is supported as a layout of a window tensor?": {
        "answer": "Onlytorch.strided",
        "question": "What is supported as a layout of a window tensor?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired device of returned tensor?": {
        "answer": "device(torch.device, optional)",
        "question": "What is the desired device of returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default for the current device for the default tensor type?": {
        "answer": "ifNone",
        "question": "What is the default for the current device for the default tensor type?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What should record operations on the returned tensor?": {
        "answer": "autograd",
        "question": "What should record operations on the returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default for a window tensor?": {
        "answer": "False",
        "question": "What is the default for a window tensor?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default device for the default tensor type?": {
        "answer": "ifNone",
        "question": "What is the default device for the default tensor type?",
        "context": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default for a 1-D tensor of size?": {
        "answer": "False",
        "question": "What is the default for a 1-D tensor of size?",
        "context": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What does a 1-D tensor contain?": {
        "answer": "window Tensor",
        "question": "What does a 1-D tensor contain?",
        "context": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is deprecated in favor oftorch.linalg.lstsq()?": {
        "answer": "torch.lstsq()",
        "question": "What is deprecated in favor oftorch.linalg.lstsq()?",
        "context": "torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    "What are the residuals of the solution in the lastm?": {
        "answer": "ncolumns",
        "question": "What are the residuals of the solution in the lastm?",
        "context": "torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    "What happens if the global deterministic flag is turned on?": {
        "answer": "Returns True",
        "question": "What happens if the global deterministic flag is turned on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What documentation does the global deterministic flag turn on?": {
        "answer": "totorch.use_deterministic_algorithms()",
        "question": "What documentation does the global deterministic flag turn on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What represents if each element isfiniteor not?": {
        "answer": "boolean elements",
        "question": "What represents if each element isfiniteor not?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What are finite when they are not NaN, negative infinity, or infinity?": {
        "answer": "Real values",
        "question": "What are finite when they are not NaN, negative infinity, or infinity?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "Complex values are what when both their real and imaginary parts are finite?": {
        "answer": "finite",
        "question": "Complex values are what when both their real and imaginary parts are finite?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What is the boolean tensor?": {
        "answer": "True",
        "question": "What is the boolean tensor?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What type of distribution does fillselftensor with elements drawn from?": {
        "answer": "geometric distribution",
        "question": "What type of distribution does fillselftensor with elements drawn from?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "What is an element drawn from the geometric distribution?": {
        "answer": "Fillsselftensor",
        "question": "What is an element drawn from the geometric distribution?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "What distribution does fillselftensor with elements drawn from?": {
        "answer": "geometric",
        "question": "What distribution does fillselftensor with elements drawn from?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "Which tensor returns a new tensor?": {
        "answer": "Flip tensor",
        "question": "Which tensor returns a new tensor?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "In what direction does the tensor return a new tensor?": {
        "answer": "Flip the entries in each column in the up/down direction",
        "question": "In what direction does the tensor return a new tensor?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What columns are preserved, but appear in a different order than before?": {
        "answer": "Rows",
        "question": "What columns are preserved, but appear in a different order than before?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "Note Requires the tensor to be at least what?": {
        "answer": "1-D",
        "question": "Note Requires the tensor to be at least what?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What makes a copy ofinput's data?": {
        "answer": "Note torch.flip",
        "question": "What makes a copy ofinput's data?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the name of'snp.flipud'?": {
        "answer": "NumPy",
        "question": "What is the name of'snp.flipud'?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is torch.flipudis expected to be?": {
        "answer": "slower thannp.flipud",
        "question": "What is torch.flipudis expected to be?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is the name of the tensor created?": {
        "answer": "sizesizefilled withfill_value",
        "question": "What is the name of the tensor created?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is the tensor's dtype inferred?": {
        "answer": "fromfill_value",
        "question": "What is the tensor's dtype inferred?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is a list, tuple, ortorch.Sizeof integers defining the shape of the output tens": {
        "answer": "size",
        "question": "What is a list, tuple, ortorch.Sizeof integers defining the shape of the output tens",
        "context": "size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is the value to fill the output tensor with?": {
        "answer": "fill_value",
        "question": "What is the value to fill the output tensor with?",
        "context": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is out a tensor?": {
        "answer": "output tensor",
        "question": "What is out a tensor?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the desired layout of returned Tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of returned Tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the name of the layout of returned Tensor?": {
        "answer": "Default:torch.strided",
        "question": "What is the name of the layout of returned Tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is out \u2013 the desired data type of returned tensor?": {
        "answer": "output tensor",
        "question": "What is out \u2013 the desired data type of returned tensor?",
        "context": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What is out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional)?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the desired data type of returned Tensor?": {
        "answer": "dtype",
        "question": "What is the desired data type of returned Tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What has numbers sampled from the continuous uniform distribution?": {
        "answer": "Fillsselftensor",
        "question": "What has numbers sampled from the continuous uniform distribution?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What distribution does fillselftensor with numbers sampled from?": {
        "answer": "uniform",
        "question": "What distribution does fillselftensor with numbers sampled from?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What is the current device of the generator?": {
        "answer": "Gets the current device of the generator",
        "question": "What is the current device of the generator?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the current device of the generator state?": {
        "answer": "atorch.ByteTensor",
        "question": "What is the current device of the generator state?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What contains all the necessary bits to restore a Generator to a specific point in time?": {
        "answer": "Atorch.ByteTensor",
        "question": "What contains all the necessary bits to restore a Generator to a specific point in time?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the first seed for generating random numbers?": {
        "answer": "Tensor",
        "question": "What is the name of the first seed for generating random numbers?",
        "context": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a Tensor example?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What is a Tensor example?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of a seed that has a good balance of 0 and 1 bits in the seed?": {
        "answer": "atorch.Generatorobject",
        "question": "What is the name of a seed that has a good balance of 0 and 1 bits in the seed?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is atorch.Generatorobject recommended to set?": {
        "answer": "a large seed",
        "question": "What is atorch.Generatorobject recommended to set?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a good balance of a large seed?": {
        "answer": "0 and 1 bits",
        "question": "What is a good balance of a large seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "How many bits are in the seed?": {
        "answer": "0 bits",
        "question": "How many bits are in the seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the generator state?": {
        "answer": "atorch.ByteTensor",
        "question": "What is the generator state?",
        "context": "Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a Tensor Example: Returns the initial seed for generating random numbers?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What is a Tensor Example: Returns the initial seed for generating random numbers?",
        "context": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a good balance for a large seed?": {
        "answer": "0 and 1 bits",
        "question": "What is a good balance for a large seed?",
        "context": "Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is an example of a seed that contains all the necessary bits to restore a Generator to a specific point in time?": {
        "answer": "Tensor",
        "question": "What is an example of a seed that contains all the necessary bits to restore a Generator to a specific point in time?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a Tensor Example?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What is a Tensor Example?",
        "context": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the seed that is recommended to set a large seed for generating random numbers?": {
        "answer": "atorch.Generatorobject",
        "question": "What is the name of the seed that is recommended to set a large seed for generating random numbers?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the seed that returns the initial seed for generating random numbers?": {
        "answer": "atorch.Generatorobject",
        "question": "What is the name of the seed that returns the initial seed for generating random numbers?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the seed that is recommended to set a large seed?": {
        "answer": "atorch.Generatorobject",
        "question": "What is the name of the seed that is recommended to set a large seed?",
        "context": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does a large seed have a good balance of 0 and 1 bits?": {
        "answer": "Returns the initial seed for generating random numbers",
        "question": "What does a large seed have a good balance of 0 and 1 bits?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does atorch.Generatorobject mean?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does atorch.Generatorobject mean?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "If inference mode is currently enabled, what is returned if inference mode is enabled?": {
        "answer": "True",
        "question": "If inference mode is currently enabled, what is returned if inference mode is enabled?",
        "context": "Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled"
    },
    "Which correction will be used to calculate the variance?": {
        "answer": "Bessel",
        "question": "Which correction will be used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is calculated if Bessel's correction is true?": {
        "answer": "the sample variance",
        "question": "What is calculated if Bessel's correction is true?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the dimension or dimensions to reduce?": {
        "answer": "dim",
        "question": "What is the dimension or dimensions to reduce?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What type of correction is used to calculate the variance?": {
        "answer": "unbiased",
        "question": "What type of correction is used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a tuple containing the variance and mean?": {
        "answer": "tuple",
        "question": "What is a tuple containing the variance and mean?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is bool?": {
        "answer": "unbiased",
        "question": "What is bool?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the term for the output tensor?": {
        "answer": "keepdim",
        "question": "What is the term for the output tensor?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "Out(Tensor,optional) \u2013 what?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) \u2013 what?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What does a tuple contain?": {
        "answer": "variance and mean",
        "question": "What does a tuple contain?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What calculates the variance and mean of all elements in theinputtensor?": {
        "answer": "Calculates the variance and mean of all elements in theinputtensor",
        "question": "What calculates the variance and mean of all elements in theinputtensor?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the term for the correction of python:ints?": {
        "answer": "unbiased",
        "question": "What is the term for the correction of python:ints?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the output tensor out?": {
        "answer": "output tensor",
        "question": "What is the output tensor out?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a tuple that calculates the variance and mean of all elements in?": {
        "answer": "theinputtensor",
        "question": "What is a tuple that calculates the variance and mean of all elements in?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is Bessel's correction?": {
        "answer": "IfunbiasedisTrue",
        "question": "What is Bessel's correction?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "IfunbiasedisTrue, Bessel\u2019s correction will be used?": {
        "answer": "the sample deviation",
        "question": "IfunbiasedisTrue, Bessel\u2019s correction will be used?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the term for bool?": {
        "answer": "unbiased",
        "question": "What is the term for bool?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a tuple containing the variance and mean of all elements in theinputtensor?": {
        "answer": "keepdim",
        "question": "What is a tuple containing the variance and mean of all elements in theinputtensor?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What type of correction is used to determine whether to use Bessel's correction?": {
        "answer": "unbiased",
        "question": "What type of correction is used to determine whether to use Bessel's correction?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a contiguous, one-dimensional array of a single data type?": {
        "answer": "Atorch.Storageis",
        "question": "What is a contiguous, one-dimensional array of a single data type?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Who has a corresponding storage of the same data type?": {
        "answer": "Everytorch.Tensor",
        "question": "Who has a corresponding storage of the same data type?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does Everytorch.Tensor have a corresponding storage of the same data type?": {
        "answer": "bfloat16",
        "question": "What type of type does Everytorch.Tensor have a corresponding storage of the same data type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has a corresponding storage of the same data type?": {
        "answer": "Everytorch.Tensor",
        "question": "What has a corresponding storage of the same data type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type Casts this storage to?": {
        "answer": "bfloat16",
        "question": "What type of type Casts this storage to?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type returns a copy of this storage?": {
        "answer": "char",
        "question": "What type of type returns a copy of this storage?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is device(int) called?": {
        "answer": "destination GPU id",
        "question": "What is device(int) called?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Device(int) \u2013 The destination GPU id. Defaults to what?": {
        "answer": "current device",
        "question": "Device(int) \u2013 The destination GPU id. Defaults to what?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does a copy of this storage return if it\u2019s not already on the CPU?": {
        "answer": "a CPU copy",
        "question": "What does a copy of this storage return if it\u2019s not already on the CPU?",
        "context": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type returns a CPU copy of this storage if it\u2019s not already on the CPU?": {
        "answer": "float",
        "question": "What type of type returns a CPU copy of this storage if it\u2019s not already on the CPU?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the device(int)?": {
        "answer": "destination GPU id",
        "question": "What is the device(int)?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the destination GPU id?": {
        "answer": "current device",
        "question": "What is the destination GPU id?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the float type return if it\u2019s not already on the CPU?": {
        "answer": "a CPU copy",
        "question": "What does the float type return if it\u2019s not already on the CPU?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens to a CPU copy of this storage?": {
        "answer": "if it\u2019s not already on the CPU",
        "question": "What happens to a CPU copy of this storage?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrueand the source is in pinned memory, the copy will be what?": {
        "answer": "asynchronous",
        "question": "IfTrueand the source is in pinned memory, the copy will be what?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect on the copy of a CUDA memory?": {
        "answer": "argument",
        "question": "What has no effect on the copy of a CUDA memory?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where does a copy of the object return?": {
        "answer": "CUDA memory",
        "question": "Where does a copy of the object return?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host?": {
        "answer": "the argument has no effect",
        "question": "IfTrueand the source is in pinned memory, the copy will be asynchronous with respect to the host?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "**kwargs\u2013 For compatibility, may contain what?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "**kwargs\u2013 For compatibility, may contain what?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in what memory, then no copy is performed and the original object is returned?": {
        "answer": "CUDA memory",
        "question": "If the object is already in what memory, then no copy is performed and the original object is returned?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the keyasyncin place of the keyasyncin place of thenon_blockingargument?": {
        "answer": "double type",
        "question": "What is the keyasyncin place of the keyasyncin place of thenon_blockingargument?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the keyasyncin place of thenon_blockingargument?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What is the keyasyncin place of thenon_blockingargument?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have ifTrueand the source is in pinned memory?": {
        "answer": "no effect",
        "question": "What effect does the argument have ifTrueand the source is in pinned memory?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the float type of the storage?": {
        "answer": "IfsharedisTrue",
        "question": "What is the float type of the storage?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "How are changes written to the file?": {
        "answer": "All changes are written to the file",
        "question": "How are changes written to the file?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the changes on the storage do not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What does the changes on the storage do not affect the file?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does **kwargs contain for compatibility?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What does **kwargs contain for compatibility?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the storage that can be used to float?": {
        "answer": "IfsharedisTrue",
        "question": "What is the name of the storage that can be used to float?",
        "context": "**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a float type?": {
        "answer": "IfsharedisTrue",
        "question": "What is a float type?",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the number of elements in the storage?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What is the number of elements in the storage?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisFalse, the file must contain what?": {
        "answer": "at leastsize * sizeof(Type)bytes",
        "question": "IfsharedisFalse, the file must contain what?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What file will be created if needed?": {
        "answer": "IfsharedisTruethe file will be created if needed",
        "question": "What file will be created if needed?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the file name to map?": {
        "answer": "filename",
        "question": "What is the file name to map?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the float type of storage?": {
        "answer": "IfsharedisTrue",
        "question": "What is the float type of storage?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the file that is shared between all processes?": {
        "answer": "All changes are written to the file",
        "question": "What is the name of the file that is shared between all processes?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the file name to map shared(bool)?": {
        "answer": "share memory size(int)",
        "question": "What is the file name to map shared(bool)?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the change on the storage do not affect the file?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What does the change on the storage do not affect the file?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What file must contain at leastsize * sizeof(Type)bytes?": {
        "answer": "IfsharedisFalse",
        "question": "What file must contain at leastsize * sizeof(Type)bytes?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the filename of the storage that is not already pinned?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What is the filename of the storage that is not already pinned?",
        "context": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory and CUDA storages?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "What is a no-op for storages already in shared memory and CUDA storages?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory?": {
        "answer": "CUDA storages",
        "question": "What is a no-op for storages already in shared memory?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the return of storages in shared memory?": {
        "answer": "self Casts this storage to short type",
        "question": "What is the return of storages in shared memory?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does self Casts this storage to long type?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What type of storage does self Casts this storage to long type?",
        "context": "shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the size of the storage Casts this storage to int type Casts this storage to long type?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What is the size of the storage Casts this storage to int type Casts this storage to long type?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is the storage to pinned memory if it\u2019s not already pinned?": {
        "answer": "Copies",
        "question": "What type of storage is the storage to pinned memory if it\u2019s not already pinned?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the return of a storage in shared memory?": {
        "answer": "self",
        "question": "What is the return of a storage in shared memory?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the storage to pinned memory need to long type?": {
        "answer": "Copies",
        "question": "What does the storage to pinned memory need to long type?",
        "context": "Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage can be cast to pinned memory if it\u2019s not already pinned?": {
        "answer": "Copies",
        "question": "What type of storage can be cast to pinned memory if it\u2019s not already pinned?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does self Cast this storage to short type Returns a list containing the elements of this storage?": {
        "answer": "ifdtypeis not provided",
        "question": "What does self Cast this storage to short type Returns a list containing the elements of this storage?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If this is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If this is already of the correct type, what is performed and the original object is returned?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type of storage?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type of storage?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What returns the type ifdtypeis not provided?": {
        "answer": "self Casts this storage to short type Returns a list containing the elements of this storage",
        "question": "What returns the type ifdtypeis not provided?",
        "context": "Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the list containing the elements of this storage return?": {
        "answer": "the type ifdtypeis not provided",
        "question": "What does the list containing the elements of this storage return?",
        "context": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does a list containing elements of this storage return?": {
        "answer": "ifdtypeis not provided",
        "question": "What does a list containing elements of this storage return?",
        "context": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already of the correct type, what is performed and the original object is returned?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type non_blocking(bool)?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type non_blocking(bool)?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect if the copy is performed asynchronously with respect to the host?": {
        "answer": "argument",
        "question": "What has no effect if the copy is performed asynchronously with respect to the host?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the type that returns the object to the specified type?": {
        "answer": "ifdtypeis not provided",
        "question": "What is the type that returns the object to the specified type?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have if the source is in pinned memory and destination is on the GPU or vice versa?": {
        "answer": "no effect",
        "question": "What effect does the argument have if the source is in pinned memory and destination is on the GPU or vice versa?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is performed if the original object is returned?": {
        "answer": "no copy",
        "question": "What is performed if the original object is returned?",
        "context": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the copy perform asynchronously with respect to the host?": {
        "answer": "argument has no effect",
        "question": "What does the copy perform asynchronously with respect to the host?",
        "context": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is deprecated?": {
        "answer": "Theasyncarg",
        "question": "What is deprecated?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is theasyncarg deprecated?": {
        "answer": "bfloat16",
        "question": "What type of storage is theasyncarg deprecated?",
        "context": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU?": {
        "answer": "no effect",
        "question": "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU?",
        "context": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type Casts this storage to bfloat16 type?": {
        "answer": "byte",
        "question": "What type of type Casts this storage to bfloat16 type?",
        "context": "dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, the copy is performed what?": {
        "answer": "asynchronously",
        "question": "IfTrue and the source are in pinned memory and destination is on the GPU or vice versa, the copy is performed what?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU or vice versa?": {
        "answer": "no effect",
        "question": "What effect does the argument have ifTrue and the source is in pinned memory and destination is on the GPU or vice versa?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does this storage have?": {
        "answer": "char",
        "question": "What type of type does this storage have?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of type does this storage go to?": {
        "answer": "bfloat16",
        "question": "What type of type does this storage go to?",
        "context": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the matrixAAA?": {
        "answer": "triangular coefficient",
        "question": "What is the matrixAAA?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solvesAX=bAX = bAX=band assume?": {
        "answer": "upper-triangular",
        "question": "What does solvesAX=bAX = bAX=band assume?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What can torch.triangular_solve take in?": {
        "answer": "2D inputsb",
        "question": "What can torch.triangular_solve take in?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "If the inputs are batches, what does X support input of float, double, cfloat and cdouble data types?": {
        "answer": "batched outputs",
        "question": "If the inputs are batches, what does X support input of float, double, cfloat and cdouble data types?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solvesAX=bAX = bAX=band assumesAAAis?": {
        "answer": "upper-triangular",
        "question": "What does solvesAX=bAX = bAX=band assumesAAAis?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve take in?": {
        "answer": "2D inputsb",
        "question": "What does torch.triangular_solve take in?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does the torch.triangular_solve return if the inputs are batches?": {
        "answer": "batched outputs",
        "question": "What does the torch.triangular_solve return if the inputs are batches?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is b(Tensor) of more batch dimensions?": {
        "answer": "zero",
        "question": "What is b(Tensor) of more batch dimensions?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What supports input of float, double, cfloat and cdouble data types?": {
        "answer": "batched outputsX",
        "question": "What supports input of float, double, cfloat and cdouble data types?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the input triangular coefficient matrix of size(,m,m,m,k)?": {
        "answer": "b(Tensor)",
        "question": "What is the input triangular coefficient matrix of size(,m,m,m,k)?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What types of data types support input?": {
        "answer": "float, double, cfloat and cdouble",
        "question": "What types of data types support input?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(": {
        "answer": "b(Tensor)",
        "question": "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for the lower-triangular system of equations?": {
        "answer": "True",
        "question": "What is the default for the lower-triangular system of equations?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the multiple right-hand sides of size(,m,k)(*, m, k)(,m,": {
        "answer": "b(Tensor)",
        "question": "What is the multiple right-hand sides of size(,m,k)(*, m, k)(,m,",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for transpose?": {
        "answer": "True",
        "question": "What is the default for transpose?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the Default:True. transpose?": {
        "answer": "whetherAAAshould be transposed before being sent into the solver",
        "question": "What is the Default:True. transpose?",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default for a b(Tensor)?": {
        "answer": "False",
        "question": "What is the default for a b(Tensor)?",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the input triangular coefficient matrix of size?": {
        "answer": "A(Tensor)",
        "question": "What is the input triangular coefficient matrix of size?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the Default:True?": {
        "answer": "transpose",
        "question": "What is the Default:True?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the Default for a transpose?": {
        "answer": "False",
        "question": "What is the Default for a transpose?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is a tensor filled with?": {
        "answer": "scalar value1,",
        "question": "What is a tensor filled with?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is a sequence of integers defining the shape of the output tensor?": {
        "answer": "size",
        "question": "What is a sequence of integers defining the shape of the output tensor?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What can be a variable number of arguments?": {
        "answer": "a collection like a list or tuple",
        "question": "What can be a variable number of arguments?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What can size be?": {
        "answer": "a variable number of arguments",
        "question": "What can size be?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is out a variable number of arguments or a collection like a list or tuple?": {
        "answer": "output tensor",
        "question": "What is out a variable number of arguments or a collection like a list or tuple?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What module is modeled after SciPy'sspecialmodule?": {
        "answer": "The torch.special module",
        "question": "What module is modeled after SciPy'sspecialmodule?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the torch.special module modeled after?": {
        "answer": "SciPy",
        "question": "What is the torch.special module modeled after?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the torch.special module in?": {
        "answer": "BETA",
        "question": "What is the torch.special module in?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the torch.special module in BETA?": {
        "answer": "New functions are still being added",
        "question": "What is the torch.special module in BETA?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does the torch.special module look for?": {
        "answer": "the documentation of each function for details",
        "question": "What does the torch.special module look for?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is input(Tensor)?": {
        "answer": "input tensor",
        "question": "What is input(Tensor)?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the output tensor?": {
        "answer": "out(Tensor,optional) \u2013 the output tensor",
        "question": "What is the output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does the torch.special module compute?": {
        "answer": "the error function ofinput",
        "question": "What does the torch.special module compute?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the error function defined as?": {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function defined as?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the error function defined as: input(Tensor) \u2013 the input tensor. out(Tensor,": {
        "answer": "Computes the error function ofinput",
        "question": "What is the error function defined as: input(Tensor) \u2013 the input tensor. out(Tensor,",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Out(Tensor,optional) \u2013 the what?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) \u2013 the what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is an example of the complementary error function ofinput?": {
        "answer": "Computes the complementary error function ofinput",
        "question": "What is an example of the complementary error function ofinput?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the complementary error function defined as: input(Tensor) \u2013 the input tensor?": {
        "answer": "output tensor",
        "question": "What is the complementary error function defined as: input(Tensor) \u2013 the input tensor?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the complementary error function ofinput?": {
        "answer": "inverse error function ofinput",
        "question": "What is the complementary error function ofinput?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the inverse error function defined in the range(1,1)(-1, 1)(1,1)as: input(Tens": {
        "answer": "output tensor",
        "question": "What is the inverse error function defined in the range(1,1)(-1, 1)(1,1)as: input(Tens",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the logistic sigmoid function?": {
        "answer": "the expit",
        "question": "What is the logistic sigmoid function?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the exponential of the elements ofinput?": {
        "answer": "1",
        "question": "What is the exponential of the elements ofinput?",
        "context": "Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does the output tensor have?": {
        "answer": "Note",
        "question": "What does the output tensor have?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is an example of a inverse error function?": {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is an example of a inverse error function?",
        "context": "Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the name of the inverse error function ofinput?": {
        "answer": "Note",
        "question": "What is the name of the inverse error function ofinput?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does the compiler compute?": {
        "answer": "the error function ofinput",
        "question": "What does the compiler compute?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the expit also known as?": {
        "answer": "the logistic sigmoid function",
        "question": "What is the expit also known as?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does this function provide greater precision than for small values of x. input(Tensor) \u2013 the input tensor?": {
        "answer": "exp(x) - 1",
        "question": "What does this function provide greater precision than for small values of x. input(Tensor) \u2013 the input tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the exponential function ofinput?": {
        "answer": "base two",
        "question": "What is the exponential function ofinput?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the exponential of the elements minus?": {
        "answer": "1",
        "question": "What is the exponential of the elements minus?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the natural value of the gamma function oninput?": {
        "answer": "logarithm",
        "question": "What is the natural value of the gamma function oninput?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the first kind of the first kind ofinput?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of the first kind ofinput?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the base two exponential function ofinput?": {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is the base two exponential function ofinput?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the absolute value of the gamma function oninput?": {
        "answer": "natural logarithm",
        "question": "What is the absolute value of the gamma function oninput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the order modified Bessel function of the first kind?": {
        "answer": "zeroth",
        "question": "What is the order modified Bessel function of the first kind?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the first kind of the input tensor?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of the input tensor?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the first kind of gamma function oninput?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind of gamma function oninput?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the logit of the elements ofinput.inputis clamped to when eps is not None?": {
        "answer": "eps",
        "question": "What is the logit of the elements ofinput.inputis clamped to when eps is not None?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "When eps is None andinput 0 orinput> 1, the function will yield what?": {
        "answer": "NaN",
        "question": "When eps is None andinput 0 orinput> 1, the function will yield what?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the epsilon for input clamp bound?": {
        "answer": "float",
        "question": "What is the epsilon for input clamp bound?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is Default:None out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is Default:None out(Tensor,optional)?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is another example of the output tensor?": {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is another example of the output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is sscipy.special.xlog1py similar to?": {
        "answer": "SciPy",
        "question": "What is sscipy.special.xlog1py similar to?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is a tensor whose shape isbroadcastablewith the first argument?": {
        "answer": "Computesinput",
        "question": "What is a tensor whose shape isbroadcastablewith the first argument?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What is the second argument?": {
        "answer": "a number or a tensor whose shape isbroadcastablewith the first argument",
        "question": "What is the second argument?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What is a boolean tensor that is True whereinputis greater thanotherand False elsewhere?": {
        "answer": "output tensor",
        "question": "What is a boolean tensor that is True whereinputis greater thanotherand False elsewhere?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What is a boolean tensor?": {
        "answer": "True",
        "question": "What is a boolean tensor?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What does PCA perform on a low-rank matrix?": {
        "answer": "linear Principal Component Analysis",
        "question": "What does PCA perform on a low-rank matrix?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What does U,S,V return?": {
        "answer": "namedtuple",
        "question": "What does U,S,V return?",
        "context": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represents the principal directions of PCA?": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What represents the principal directions of PCA?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the relation of the data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S": {
        "answer": "AAAis",
        "question": "What is the relation of the data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S",
        "context": "This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represents the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What represents the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "context": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "The size of returned matrices depend on the specified rank and what values?": {
        "answer": "q",
        "question": "The size of returned matrices depend on the specified rank and what values?",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is a data matrix withmsamples andnfeatures theVVVcolumns?": {
        "answer": "AAAis",
        "question": "What is a data matrix withmsamples andnfeatures theVVVcolumns?",
        "context": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What values do the size of returned matrices depend on?": {
        "answer": "q",
        "question": "What values do the size of returned matrices depend on?",
        "context": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represent the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont": {
        "answer": "theVVVcolumns",
        "question": "What represent the principal directions S2/(m1)S ** 2 / (m - 1)S2/(m1)cont",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the input tensor of size(,m,n)(*, m, n)(,m,": {
        "answer": "A(Tensor)",
        "question": "What is the input tensor of size(,m,n)(*, m, n)(,m,",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, what is the center of the input tensor?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, what is the center of the input tensor?",
        "context": "Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the center of the input tensor?": {
        "answer": "if True",
        "question": "What is the center of the input tensor?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, q=min(6,m,n). center(bool,optional) \u2013 if True, center the": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, q=min(6,m,n). center(bool,optional) \u2013 if True, center the",
        "context": "UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the number of subspace iterations to conduct?": {
        "answer": "niter",
        "question": "What is the number of subspace iterations to conduct?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "By default, what is the seed for the pseudorandom number generator A(Tensor)?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "By default, what is the seed for the pseudorandom number generator A(Tensor)?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the name of the name of the q-vector VVVVis n x q matrix?": {
        "answer": "References",
        "question": "What is the name of the name of the q-vector VVVVis n x q matrix?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is a new tensor?": {
        "answer": "a narrowed version",
        "question": "What is a new tensor?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the dimensiondimis input?": {
        "answer": "fromstarttostart+length",
        "question": "What is the dimensiondimis input?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What does the returned tensor andinputtensor share?": {
        "answer": "underlying storage",
        "question": "What does the returned tensor andinputtensor share?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the starting dimension length of the tensor?": {
        "answer": "the distance to the ending dimension",
        "question": "What is the starting dimension length of the tensor?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What function returns the minimum value of all elements in?": {
        "answer": "theinputtensor",
        "question": "What function returns the minimum value of all elements in?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does this function produce deterministic (sub)gradients unlikemin(dim=0) input(Tensor)?": {
        "answer": "input tensor",
        "question": "What does this function produce deterministic (sub)gradients unlikemin(dim=0) input(Tensor)?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is argmin?": {
        "answer": "index location",
        "question": "What is argmin?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the minimum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "index location",
        "question": "What is the minimum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the index location of each minimum value found?": {
        "answer": "Andindicesis",
        "question": "What is the index location of each minimum value found?",
        "context": "This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does indices mean for each minimum value found?": {
        "answer": "index location",
        "question": "What does indices mean for each minimum value found?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is argmin's minimum value?": {
        "answer": "index location",
        "question": "What is argmin's minimum value?",
        "context": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the minimum value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the minimum value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the index location of each minimum value in the given dimensiondim?": {
        "answer": "Andindicesis the index location of each minimum value found",
        "question": "What is the index location of each minimum value in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What does the output tensors have a fewer dimension thaninput?": {
        "answer": "Note",
        "question": "What does the output tensors have a fewer dimension thaninput?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What happens if there are multiple minimal values in a reduced row?": {
        "answer": "the indices of the first minimal value are returned",
        "question": "What happens if there are multiple minimal values in a reduced row?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "Out(tuple,optional) \u2013 what is the tuple of two output tensors?": {
        "answer": "the tuple of two output tensors",
        "question": "Out(tuple,optional) \u2013 what is the tuple of two output tensors?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What type of function does this function call LAPACK's geqrf directly?": {
        "answer": "low-level",
        "question": "What type of function does this function call LAPACK's geqrf directly?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does this function return for geqrf?": {
        "answer": "namedtuple",
        "question": "What does this function return for geqrf?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What are stored in the same output tensora?": {
        "answer": "BothQandRmatrices",
        "question": "What are stored in the same output tensora?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where are elements ofRare stored?": {
        "answer": "on and above the diagonal",
        "question": "Where are elements ofRare stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where is matrixQare stored?": {
        "answer": "below the diagonal",
        "question": "Where is matrixQare stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does withtorch.ormqr() use for?": {
        "answer": "matrix-matrix multiplication",
        "question": "What does withtorch.ormqr() use for?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "For more details, see the LAPACK documentation for what?": {
        "answer": "geqrf",
        "question": "For more details, see the LAPACK documentation for what?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does thedriver=\"gels\"option use to solve matrix equations?": {
        "answer": "a QR decomposition",
        "question": "What does thedriver=\"gels\"option use to solve matrix equations?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the output tuple of (Tensor, Tensor)?": {
        "answer": "input(Tensor)",
        "question": "What is the output tuple of (Tensor, Tensor)?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is Ignored by a function that can solve matrix equations using a QR decomposition?": {
        "answer": "ifNone",
        "question": "What is Ignored by a function that can solve matrix equations using a QR decomposition?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is Ignored ifNone?": {
        "answer": "Default:None",
        "question": "What is Ignored ifNone?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ": {
        "answer": "tensor",
        "question": "Where does each row containnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of ",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What do rows ofinputdo not need to sum to one?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What do rows ofinputdo not need to sum to one?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What are ordered from left to right according to when each was sampled?": {
        "answer": "Indices",
        "question": "What are ordered from left to right according to when each was sampled?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is a vector of sizenum_samples?": {
        "answer": "Ifinputis a vector",
        "question": "What is a vector of sizenum_samples?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What are ordered from left to right according to when each sample was sampled?": {
        "answer": "Indices",
        "question": "What are ordered from left to right according to when each sample was sampled?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a vector,outis a vector of what?": {
        "answer": "sizenum_samples",
        "question": "Ifinputis a vector,outis a vector of what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is ifinputis a matrix of shape?": {
        "answer": "matrix withmrows",
        "question": "What is ifinputis a matrix of shape?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what isTrue, samples are drawn with replacement?": {
        "answer": "replacement",
        "question": "If what isTrue, samples are drawn with replacement?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What do rows ofinputdo have to be?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What do rows ofinputdo have to be?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what is true, samples are drawn with replacement?": {
        "answer": "replacement",
        "question": "If what is true, samples are drawn with replacement?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What happens when a sample index is drawn for a row?": {
        "answer": "they are drawn without replacement",
        "question": "What happens when a sample index is drawn for a row?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the name of a sample index that cannot be drawn again for a row?": {
        "answer": "Note",
        "question": "What is the name of a sample index that cannot be drawn again for a row?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What means that when a sample index is drawn for a row, it cannot be drawn again for that row?": {
        "answer": "they are drawn without replacement",
        "question": "What means that when a sample index is drawn for a row, it cannot be drawn again for that row?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the difference between samples drawn with replacement?": {
        "answer": "If replacement isTrue",
        "question": "What is the difference between samples drawn with replacement?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the min number of non-zero elements in each row ofinputif it is a matrix?": {
        "answer": "number of non-zero elements ininput",
        "question": "What is the min number of non-zero elements in each row ofinputif it is a matrix?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is input tensor?": {
        "answer": "input tensor",
        "question": "What is input tensor?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is drawn for a row when it cannot be drawn again for that row?": {
        "answer": "a sample index",
        "question": "What is drawn for a row when it cannot be drawn again for that row?",
        "context": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the input tensor containing probabilities num_samples?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor containing probabilities num_samples?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Modules make it simple to specify what for PyTorch\u2019s Optimizers to update?": {
        "answer": "learnable parameters",
        "question": "Modules make it simple to specify what for PyTorch\u2019s Optimizers to update?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How are Modules easy to work with and transform?": {
        "answer": "Easy to work with and transform",
        "question": "How are Modules easy to work with and transform?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is easy to work with and transform?": {
        "answer": "Easy to work with and transform",
        "question": "What is easy to work with and transform?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the purpose of this note?": {
        "answer": "all PyTorch users",
        "question": "What is the purpose of this note?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Why are modules so fundamental to PyTorch?": {
        "answer": "modules are so fundamental to PyTorch",
        "question": "Why are modules so fundamental to PyTorch?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple custom module module called?": {
        "answer": "Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What is a simple custom module module called?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this note describe?": {
        "answer": "modules",
        "question": "What does this note describe?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a Simple Custom Module Module Module as Building Blocks Neural Network Training with Modules?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What is a Simple Custom Module Module Module as Building Blocks Neural Network Training with Modules?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are Modules as Building Blocks Neural Network Training with?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What are Modules as Building Blocks Neural Network Training with?",
        "context": "Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch'sLinearmodule apply to its input?": {
        "answer": "an affine transformation",
        "question": "What does PyTorch'sLinearmodule apply to its input?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What basic characteristics does PyTorch'sLinearmodule have?": {
        "answer": "module has the following fundamental characteristics of modules",
        "question": "What basic characteristics does PyTorch'sLinearmodule have?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch'sLinearmodule have?": {
        "answer": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What does PyTorch'sLinearmodule have?",
        "context": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a custom version of PyTorch'sLinearmodule?": {
        "answer": "Module Hooks Advanced Features",
        "question": "What is a custom version of PyTorch'sLinearmodule?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the basic features of PyTorch'sLinearmodule?": {
        "answer": "Module Hooks Advanced Features",
        "question": "What are the basic features of PyTorch'sLinearmodule?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple, custom version of PyTorch'sLinearmodule?": {
        "answer": "a simpler, custom version of PyTorch\u2019sLinearmodule",
        "question": "What is a simple, custom version of PyTorch'sLinearmodule?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module inherits from the base Module class?": {
        "answer": "module",
        "question": "What module inherits from the base Module class?",
        "context": "This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the module define in computation?": {
        "answer": "state",
        "question": "What does the module define in computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is each of the state defined as?": {
        "answer": "aParameter",
        "question": "What is each of the state defined as?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be considered the \"learnable\" aspects of the module\u2019s computation?": {
        "answer": "Parameters",
        "question": "What can be considered the \"learnable\" aspects of the module\u2019s computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Parameters are not required to have what?": {
        "answer": "state",
        "question": "Parameters are not required to have what?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What function performs the computation?": {
        "answer": "forward()",
        "question": "What function performs the computation?",
        "context": "It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can perform arbitrary computation involving any number of inputs and outputs?": {
        "answer": "theforward()implementation",
        "question": "What can perform arbitrary computation involving any number of inputs and outputs?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module demonstrates how modules package state and computation together?": {
        "answer": "module",
        "question": "What module demonstrates how modules package state and computation together?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be constructed and called?": {
        "answer": "Instances",
        "question": "What can be constructed and called?",
        "context": "This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are aspects of the module\u2019s computation that should be \u201clearned\u201d?": {
        "answer": "parameters registered by a module",
        "question": "What are aspects of the module\u2019s computation that should be \u201clearned\u201d?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the Optimizer used by a module?": {
        "answer": "PyTorch",
        "question": "What is the name of the Optimizer used by a module?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Before we get to that, let\u2019s first examine what?": {
        "answer": "how modules can be composed with one another",
        "question": "Before we get to that, let\u2019s first examine what?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can modules contain?": {
        "answer": "modules",
        "question": "What can modules contain?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the easiest way to do this?": {
        "answer": "using theSequentialmodule",
        "question": "What is the easiest way to do this?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where does theSequentialmodule feed the output of the firstMyLinearmodule?": {
        "answer": "theReLU",
        "question": "Where does theSequentialmodule feed the output of the firstMyLinearmodule?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is theSequentialmodule limited to?": {
        "answer": "in-order chaining of modules",
        "question": "What is theSequentialmodule limited to?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What doesSequential automatically feed the output of the firstMyLinearmodule?": {
        "answer": "input into theReLU",
        "question": "What doesSequential automatically feed the output of the firstMyLinearmodule?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How is the output of the firstMyLinearmodule limited?": {
        "answer": "in-order chaining of modules",
        "question": "How is the output of the firstMyLinearmodule limited?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is recommended for anything beyond the simplest use cases?": {
        "answer": "a custom module",
        "question": "What is recommended for anything beyond the simplest use cases?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple neural network implemented as?": {
        "answer": "a custom module",
        "question": "What is a simple neural network implemented as?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a custom module?": {
        "answer": "a simple neural network",
        "question": "What is a custom module?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module does s()andnamed_modules() use to go deeper than the immediate children?": {
        "answer": "module",
        "question": "What module does s()andnamed_modules() use to go deeper than the immediate children?",
        "context": "To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What module registers submodules from a list or dict?": {
        "answer": "TheModuleListandModuleDictmodules",
        "question": "What module registers submodules from a list or dict?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is sometimes necessary for a module to do?": {
        "answer": "dynamically define submodules",
        "question": "What is sometimes necessary for a module to do?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the call toparameters()andnamed_parameters() include?": {
        "answer": "child parameters",
        "question": "What does the call toparameters()andnamed_parameters() include?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can elaborate neural networks be formed?": {
        "answer": "module composition",
        "question": "How can elaborate neural networks be formed?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "PyTorch provides a large library of modules that perform computation commonly found within neural networks?": {
        "answer": "thetorch.nnnamespace",
        "question": "PyTorch provides a large library of modules that perform computation commonly found within neural networks?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "In the next section, we give a full example of what?": {
        "answer": "training a neural network",
        "question": "In the next section, we give a full example of what?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does PyTorch.nnnamespace provide?": {
        "answer": "more information",
        "question": "What does PyTorch.nnnamespace provide?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the next section give a full example of?": {
        "answer": "training a neural network",
        "question": "What does the next section give a full example of?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a function that can be used to train a neural network?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is a function that can be used to train a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a function that can be used to a module and its submodules library of PyTorch-provided modules?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is a function that can be used to a module and its submodules library of PyTorch-provided modules?",
        "context": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What function does a module and its submodules library of PyTorch-provided modules:torch.nn have?": {
        "answer": "Recursivelyapply()a function",
        "question": "What function does a module and its submodules library of PyTorch-provided modules:torch.nn have?",
        "context": "Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of a network that needs to be trained?": {
        "answer": "Defining neural net modules",
        "question": "What is the name of a network that needs to be trained?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is present in a network?": {
        "answer": "key parts of training",
        "question": "What is present in a network?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When a network is built, what does it need to be trained?": {
        "answer": "it has to be trained",
        "question": "When a network is built, what does it need to be trained?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the key part of training?": {
        "answer": "A network is created",
        "question": "What is the key part of training?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the network learn to do in a simplified example?": {
        "answer": "zero",
        "question": "What does the network learn to do in a simplified example?",
        "context": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is created by the network?": {
        "answer": "A network",
        "question": "What is created by the network?",
        "context": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What type of optimizer is created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What type of optimizer is created?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does an optimizer do?": {
        "answer": "computes a loss",
        "question": "What does an optimizer do?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is created?": {
        "answer": "A network",
        "question": "What is created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of an optimizer created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the name of an optimizer created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens to the network's parameters' gradients?": {
        "answer": "zeros",
        "question": "What happens to the network's parameters' gradients?",
        "context": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the value of the network's parameters' gradients?": {
        "answer": "zeros",
        "question": "What is the value of the network's parameters' gradients?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "After the above snippet has been run, note that what has changed?": {
        "answer": "the network\u2019s parameters have changed",
        "question": "After the above snippet has been run, note that what has changed?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the value ofl1'sweightparameter closer to?": {
        "answer": "0",
        "question": "What is the value ofl1'sweightparameter closer to?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the function used to train a neural network?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is the name of the function used to train a neural network?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What shows that the value ofl1's weightparameter is now much closer to 0?": {
        "answer": "examining the value ofl1\u2019sweightparameter",
        "question": "What shows that the value ofl1's weightparameter is now much closer to 0?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the network that trains neural networks?": {
        "answer": "Using Optimizers",
        "question": "What is the name of the network that trains neural networks?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the network that is used to train neural networks?": {
        "answer": "Neural network training",
        "question": "What is the name of the network that is used to train neural networks?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What state does a module's _dictcontains state that affects its computation?": {
        "answer": "state",
        "question": "What state does a module's _dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does a module'sstate_dictcontains state that affects its computation?": {
        "answer": "module\u2019s parameters",
        "question": "What does a module'sstate_dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a module'sstate_dictcontains state that affects module computation?": {
        "answer": "beyond parameters",
        "question": "What is a module'sstate_dictcontains state that affects module computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides the concept of \"buffers\"?": {
        "answer": "PyTorch",
        "question": "What provides the concept of \"buffers\"?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the parameters of a module'sstate_dictcontains state that affects its computation?": {
        "answer": "learnable aspects of computation",
        "question": "What are the parameters of a module'sstate_dictcontains state that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a motivating example for the use of buffers?": {
        "answer": "a simple module that maintains a running mean",
        "question": "What is a motivating example for the use of buffers?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the snippet show how to do this?": {
        "answer": "register_buffer()",
        "question": "What does the snippet show how to do this?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are called during the backward pass?": {
        "answer": "Backward hooks",
        "question": "What are called during the backward pass?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed?": {
        "answer": "withregister_full_backward_hook()",
        "question": "How can backward hooks be installed?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When will backward hooks be called?": {
        "answer": "when the backward for this Module has been computed",
        "question": "When will backward hooks be called?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed for all modules?": {
        "answer": "globally",
        "question": "How can backward hooks be installed for all modules?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What will be used throughout the rest of the computation?": {
        "answer": "updated value",
        "question": "What will be used throughout the rest of the computation?",
        "context": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can hooks be used to execute arbitrary code along the regular module forward/backward?": {
        "answer": "modify some inputs/outputs",
        "question": "How can hooks be used to execute arbitrary code along the regular module forward/backward?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides several more advanced features that are designed to work with modules?": {
        "answer": "PyTorch",
        "question": "What provides several more advanced features that are designed to work with modules?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the functionalities that PyTorch provides when writing a new module?": {
        "answer": "inherited",
        "question": "What are the functionalities that PyTorch provides when writing a new module?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where can a detailed discussion of these features be found?": {
        "answer": "links below",
        "question": "Where can a detailed discussion of these features be found?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the feature that PyTorch provides?": {
        "answer": "Profiling",
        "question": "What is the name of the feature that PyTorch provides?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the export of modules to?": {
        "answer": "TorchScript",
        "question": "What is the export of modules to?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "From what language does TorchScript export modules to?": {
        "answer": "C++",
        "question": "From what language does TorchScript export modules to?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a multi-dimensional matrix containing elements of a single data type?": {
        "answer": "Atorch.Tensoris",
        "question": "What is a multi-dimensional matrix containing elements of a single data type?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many tensor types are defined by Torch?": {
        "answer": "10",
        "question": "How many tensor types are defined by Torch?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?": {
        "answer": "Data type dtype",
        "question": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the float32ortorch?": {
        "answer": "32-bit floating point torch",
        "question": "What is the float32ortorch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the floating point torch?": {
        "answer": "torch",
        "question": "What is the name of the floating point torch?",
        "context": "torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits of complex torch is it?": {
        "answer": "8-bit",
        "question": "How many bits of complex torch is it?",
        "context": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch?": {
        "answer": "128",
        "question": "How many -bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the double torch?": {
        "answer": "torch.float64ortorch",
        "question": "What is the name of the double torch?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the floating point1 torch?": {
        "answer": "16-bit",
        "question": "What is the floating point1 torch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.cuda.DoubleTensor?": {
        "answer": "16-bit floating point1 torch",
        "question": "What is the name of the torch.cuda.DoubleTensor?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the floating point1 torch?": {
        "answer": "16-bit floating point1 torch",
        "question": "What is the name of the floating point1 torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.float16ortorch?": {
        "answer": "half torch",
        "question": "What is the name of the torch.float16ortorch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits of complex torch does BFloat16Tensor have?": {
        "answer": "32",
        "question": "How many bits of complex torch does BFloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.cuda.HalfTensor?": {
        "answer": "16-bit floating point2 torch",
        "question": "What is the name of the torch.cuda.HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the bfloat16 torch?": {
        "answer": "16-bit floating point2 torch",
        "question": "What is the name of the bfloat16 torch?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is the BFloat16Tensor 32-bit complex torch.complex64 128-bit complex torch?": {
        "answer": "64-bit",
        "question": "What type of complex torch is the BFloat16Tensor 32-bit complex torch.complex64 128-bit complex torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128": {
        "answer": "32",
        "question": "How many -bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128",
        "context": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is complex32 64-bit complex torch?": {
        "answer": "32-bit",
        "question": "What type of complex torch is complex32 64-bit complex torch?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits of complex torch is torch.complex32?": {
        "answer": "64",
        "question": "How many bits of complex torch is torch.complex32?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is it?": {
        "answer": "64-bit",
        "question": "What type of complex torch is it?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Boolean torch?": {
        "answer": "Boolean torch",
        "question": "What is the name of the Boolean torch?",
        "context": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the unsigned number of torch.uint8 torch?": {
        "answer": "8-bit",
        "question": "What is the unsigned number of torch.uint8 torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int8 torch.ByteTensor torch.cuda.Byte": {
        "answer": "8-bit integer",
        "question": "What is the name of the torch.int8 torch.ByteTensor torch.cuda.Byte",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int8 torch?": {
        "answer": "8-bit integer",
        "question": "What is the name of the torch.int8 torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.": {
        "answer": "16",
        "question": "How many -bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int16ortorch.short torch?": {
        "answer": "16",
        "question": "How many -bit integer (signed) torch.int16ortorch.short torch?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of integers in torch.int32ortorch.int torch.IntTensor torch.cu": {
        "answer": "32-bit",
        "question": "What is the number of integers in torch.int32ortorch.int torch.IntTensor torch.cu",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How large is the torch?": {
        "answer": "16-bit",
        "question": "How large is the torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the torch.int32ortorch.int torch?": {
        "answer": "32-bit integer",
        "question": "What is the name of the torch.int32ortorch.int torch?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits does the torch contain?": {
        "answer": "4-bit integer",
        "question": "How many bits does the torch contain?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many -bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cu": {
        "answer": "4",
        "question": "How many -bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cu",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the indexing and slicing notation used to create a tensor?": {
        "answer": "Python",
        "question": "What is the name of the indexing and slicing notation used to create a tensor?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Python's indexing and slicing notation to get a Python number from a tensor?": {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is the name of Python's indexing and slicing notation to get a Python number from a tensor?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Python's indexing and slicing notation?": {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is the name of Python's indexing and slicing notation?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What can a tensor be created for automatic differentiation?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations",
        "question": "What can a tensor be created for automatic differentiation?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What holds the data of each tensor?": {
        "answer": "associatedtorch.Storage",
        "question": "What holds the data of each tensor?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor class provide?": {
        "answer": "multi-dimensional,stridedview of a storage",
        "question": "What does the tensor class provide?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a tensor class that provides multi-dimensional,stridedview of a storage?": {
        "answer": "Note",
        "question": "What is the name of a tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How can a tensor be created?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "How can a tensor be created?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on tensor views, see what?": {
        "answer": "Tensor Views",
        "question": "For more information on tensor views, see what?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?": {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What can a tensor be created?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "What can a tensor be created?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What holds the data of a tensor?": {
        "answer": "associatedtorch.Storage",
        "question": "What holds the data of a tensor?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on tensor views, seeTensor what?": {
        "answer": "Views",
        "question": "For more information on tensor views, seeTensor what?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that can be created withrequires_grad=Trueso thattorch": {
        "answer": "Note",
        "question": "What is the name of the tensor class that can be created withrequires_grad=Trueso thattorch",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor attributes of atorch.Tensor?": {
        "answer": "Note",
        "question": "What is the name of the tensor attributes of atorch.Tensor?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Note Methods which mutate a tensor are marked with what?": {
        "answer": "underscore suffix",
        "question": "Note Methods which mutate a tensor are marked with what?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does torch.FloatTensor.abs()compute the result in?": {
        "answer": "a new tensor",
        "question": "What does torch.FloatTensor.abs()compute the result in?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a method that mutates a tensor?": {
        "answer": "Warning",
        "question": "What is the name of a method that mutates a tensor?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of atorch.Tensor?": {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What is the name of atorch.Tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Note Methods which mutate a tensor are marked with what suffix?": {
        "answer": "underscore",
        "question": "Note Methods which mutate a tensor are marked with what suffix?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the tensor'storch.deviceand/ortorch.dtype?": {
        "answer": "to()method",
        "question": "What is the tensor'storch.deviceand/ortorch.dtype?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor'storch.device and/ortorch.dtype?": {
        "answer": "Warning",
        "question": "What is the name of the tensor'storch.device and/ortorch.dtype?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do Note Methods which mutate a tensor are marked with an underscore suffix?": {
        "answer": "usingto()method",
        "question": "What do Note Methods which mutate a tensor are marked with an underscore suffix?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the note method that mutates a tensor?": {
        "answer": "Warning",
        "question": "What is the name of the note method that mutates a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What suffix does a method that mutate a tensor have?": {
        "answer": "underscore",
        "question": "What suffix does a method that mutate a tensor have?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor need to change?": {
        "answer": "usingto()method",
        "question": "What does the tensor need to change?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a tensor use to change their tensor'storch.deviceand/ortorch.dtype": {
        "answer": "usingto()method",
        "question": "What does a tensor use to change their tensor'storch.deviceand/ortorch.dtype",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What could a current implementation oftorch.Tensor lead to?": {
        "answer": "unexpectedly high memory usage",
        "question": "What could a current implementation oftorch.Tensor lead to?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor should you use to change existing tensor'storch.deviceand/ortorch.d": {
        "answer": "one large structure",
        "question": "What type of tensor should you use to change existing tensor'storch.deviceand/ortorch.d",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How do you create a tensor with pre-existing data?": {
        "answer": "tensor",
        "question": "How do you create a tensor with pre-existing data?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() do to create a tensor?": {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() do to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() have to do to create a tensor?": {
        "answer": "specific size",
        "question": "What does usetorch.tensor() have to do to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with the same size as another tensor?": {
        "answer": "*tensor creation ops",
        "question": "What is used to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a tensor with the same size as another tensor?": {
        "answer": "usetorch",
        "question": "What is a tensor with the same size as another tensor?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor created by usetorch.tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What is the name of the tensor created by usetorch.tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with its dimensions reversed?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is a Tensor with its dimensions reversed?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with reversed?": {
        "answer": "its dimensions",
        "question": "What is a Tensor with reversed?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the pre-existing data to create a tensor with pre-existing data?": {
        "answer": "usetorch.tensor()",
        "question": "What is the name of the pre-existing data to create a tensor with pre-existing data?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() have to do to create a tensor with?": {
        "answer": "specific size",
        "question": "What does usetorch.tensor() have to do to create a tensor with?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What is the name of the tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor with its dimensions reversed?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is the name of the tensor with its dimensions reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetensor.new_*creation ops have?": {
        "answer": "its dimensions reversed",
        "question": "What does usetensor.new_*creation ops have?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of dimensions inx,x.Tis equivalent tox.permute?": {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x.Tis equivalent tox.permute?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch mean to create a tensor?": {
        "answer": "specific size",
        "question": "What does usetorch mean to create a tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor have its dimensions?": {
        "answer": "reversed",
        "question": "What does the Tensor have its dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor equivalent tox.permute?": {
        "answer": "Tensor.new_tensor",
        "question": "What is the Tensor equivalent tox.permute?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor to create a tensor with the same size as another tensor?": {
        "answer": "usetorch",
        "question": "What is the name of the tensor to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of a tensor with similar type but different size as another tensor?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is the name of a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor with its dimensions?": {
        "answer": "reversed",
        "question": "What is a Tensor with its dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor with what?": {
        "answer": "data",
        "question": "Tensor.new_tensor Returns a new Tensor with what?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_full return?": {
        "answer": "Tensor of sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor.new_empty",
        "question": "What is a Tensor of sizesizefilled withfill_value?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor with what as the tensor data?": {
        "answer": "data",
        "question": "Tensor.new_tensor Returns a new Tensor with what as the tensor data?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor.new_full",
        "question": "What returns a Tensor of sizesizefilled withfill_value?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_empty return a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_empty return a Tensor of sizesizefilled with?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_ones Returns a Tensor of sizesizefilled with what?": {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the dimensions of the Tensor?": {
        "answer": "reversed",
        "question": "What is the dimensions of the Tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_zeros return?": {
        "answer": "a Tensor of sizesizefilled with0",
        "question": "What does Tensor.new_zeros return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Tensor with its dimensions reversed?": {
        "answer": "Tensor.is_cuda",
        "question": "What is the name of the Tensor with its dimensions reversed?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_tensor return a new Tensor withdataas?": {
        "answer": "tensor data",
        "question": "What does Tensor.new_tensor return a new Tensor withdataas?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_empty Returns a Tensor of sizesizefilled with what?": {
        "answer": "uninitialized data",
        "question": "Tensor.new_empty Returns a Tensor of sizesizefilled with what?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor stored?": {
        "answer": "GPU",
        "question": "Where is the Tensor stored?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is stored on the GPU?": {
        "answer": "Tensor.is_quantized",
        "question": "What is true if the Tensor is stored on the GPU?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_tensor Returns a new Tensor withdataas what?": {
        "answer": "tensor data",
        "question": "Tensor.new_tensor Returns a new Tensor withdataas what?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_full Returns a Tensor of what?": {
        "answer": "sizesizefilled withfill_value",
        "question": "Tensor.new_full Returns a Tensor of what?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?": {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What isTrueif the Tensor is quantized?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Tensor?": {
        "answer": "Tensor",
        "question": "What is the name of the Tensor?",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new Tensor withdataas?": {
        "answer": "tensor data",
        "question": "What is a new Tensor withdataas?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Who returns a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor",
        "question": "Who returns a Tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor?": {
        "answer": "quantized",
        "question": "What is the Tensor?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What does Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the meta tensor?": {
        "answer": "meta",
        "question": "What is the name of the meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a meta tensor?": {
        "answer": "Tensor.device",
        "question": "What is a meta tensor?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor of?": {
        "answer": "sizesizefilled withfill_value",
        "question": "What is the Tensor of?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.device?": {
        "answer": "thetorch.device",
        "question": "What is the Tensor.device?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What is the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the meta tensor?": {
        "answer": "meta",
        "question": "What is the meta tensor?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is thetorch.device where this Tensor is?": {
        "answer": "Tensor.grad",
        "question": "What is thetorch.device where this Tensor is?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does a Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros returns a Tensor of sizesizefilled with what?": {
        "answer": "0",
        "question": "Tensor.new_zeros returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_trueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What is the Tensor.is_trueif the Tensor is quantized?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the first call for a Tensor?": {
        "answer": "tobackward()",
        "question": "What is the first call for a Tensor?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor of sizesizefilled with?": {
        "answer": "1",
        "question": "What is the Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor are quantized?": {
        "answer": "quantized",
        "question": "What does the Tensor are quantized?",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What attribute isnoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What attribute isnoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros returns a Tensor of what size?": {
        "answer": "sizesizefilled with0",
        "question": "Tensor.new_zeros returns a Tensor of what size?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the Tensor.is_cuda IsTrueif the Tensor is quantized?": {
        "answer": "quantized",
        "question": "What does the Tensor.is_cuda IsTrueif the Tensor is quantized?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.grad isnoneby default and becomes a Tensor the first time a call?": {
        "answer": "tobackward()",
        "question": "Tensor.grad isnoneby default and becomes a Tensor the first time a call?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does a Tensor return?": {
        "answer": "sizesizefilled with0",
        "question": "What does a Tensor return?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor that is quantized?": {
        "answer": "quantized",
        "question": "What is the name of the tensor that is quantized?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a Tensor the first time a call tobackward()computes gradients forself?": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What is a Tensor the first time a call tobackward()computes gradients forself?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What GPU is Tensor.is_cuda IsTrueif the Tensor is stored on?": {
        "answer": "GPU",
        "question": "What GPU is Tensor.is_cuda IsTrueif the Tensor is stored on?",
        "context": "Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor?": {
        "answer": "quantized",
        "question": "What is the name of the tensor?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Returns a new tensor containing real values of theselftensor?": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What Returns a new tensor containing real values of theselftensor?",
        "context": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new tensor containing real values of theselftensor?": {
        "answer": "real",
        "question": "What is a new tensor containing real values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is IsTrueif the Tensor is stored?": {
        "answer": "GPU",
        "question": "Where is IsTrueif the Tensor is stored?",
        "context": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What meta tensor isTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What meta tensor isTrueif the Tensor is a meta tensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.grad isNoneby default and becomes a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "Tensor.grad isNoneby default and becomes a Tensor the first time a call?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing what?": {
        "answer": "real values of theselftensor",
        "question": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing what?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.imag Returns a new tensor containing what?": {
        "answer": "imaginary values of theselftensor",
        "question": "Tensor.imag Returns a new tensor containing what?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.abs",
        "question": "What is a new tensor containing imaginary values of theselftensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does it mean to be a meta tensor?": {
        "answer": "IsTrueif the Tensor is quantized",
        "question": "What does it mean to be a meta tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the Tensor.is_?": {
        "answer": "meta",
        "question": "What is the Tensor.is_?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of Tensor.abs?": {
        "answer": "Seetorch.abs()",
        "question": "What is the name of Tensor.abs?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What isTrueif the Tensor is a meta tensor?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the olute of the Tensor?": {
        "answer": "In-place version ofabs() Tensor.abs",
        "question": "What is the olute of the Tensor?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What isTrueif the Tensor is?": {
        "answer": "meta tensor",
        "question": "What isTrueif the Tensor is?",
        "context": "IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of abs()?": {
        "answer": "Alias forabs() Tensor.abs",
        "question": "What is the name of the in-place version of abs()?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is Tensor.device?": {
        "answer": "thetorch.device",
        "question": "What is Tensor.device?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the In-place version ofabs() Tensor.absolute?": {
        "answer": "Alias",
        "question": "What is the In-place version ofabs() Tensor.absolute?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor?": {
        "answer": "thetorch.device",
        "question": "Where is the Tensor?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does thetorch.device make a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does thetorch.device make a Tensor the first time a call?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of a Tensor?": {
        "answer": "Alias forabs_()",
        "question": "What is the name of the in-place version of a Tensor?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.grad become a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does Tensor.grad become a Tensor the first time a call?",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the name of the name of the name of the name of the name of the name of the name of the name of the name": {
        "answer": "Tensor.addbmm",
        "question": "What is the name of the name of the name of the name of the name of the name of the name of the name of the name of the name",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does this attribute become a Tensor the first time a call?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What does this attribute become a Tensor the first time a call?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is arccos Seetorch.arccos() Tensor.add?": {
        "answer": "In-place version ofacos() Tensor",
        "question": "What is arccos Seetorch.arccos() Tensor.add?",
        "context": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.real Returns a new tensor containing real values of theselftensor?": {
        "answer": "Alias fordim",
        "question": "Tensor.real Returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the in-place version of arccos() Tensor?": {
        "answer": "In-place version ofarccos() Tensor",
        "question": "What is the name of the in-place version of arccos() Tensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.imag returns a new tensor containing what?": {
        "answer": "imaginary values of theselftensor",
        "question": "Tensor.imag returns a new tensor containing what?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does tensor add toselftensor?": {
        "answer": "scalar",
        "question": "What does tensor add toselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "In what axis does a n-D tensor reverse the order of a n-D tensor?": {
        "answer": "dims",
        "question": "In what axis does a n-D tensor reverse the order of a n-D tensor?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the name of'snp.flip'?": {
        "answer": "NumPy",
        "question": "What is the name of'snp.flip'?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What does torch.flipis expect to be?": {
        "answer": "slower thannp.flip",
        "question": "What does torch.flipis expect to be?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is a listortuple to flip on?": {
        "answer": "axis",
        "question": "What is a listortuple to flip on?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What method returns the reduced singular value decomposition?": {
        "answer": "IfsomeisTrue",
        "question": "What method returns the reduced singular value decomposition?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the returnedUandVmatrices contain if the last two dimensions ofinputaremandn?": {
        "answer": "onlymin(n, m)orthonormal columns",
        "question": "What will the returnedUandVmatrices contain if the last two dimensions ofinputaremandn?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Whencompute_uvisFalse has no effect?": {
        "answer": "argumentsomehas no effect",
        "question": "Whencompute_uvisFalse has no effect?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the returnedUandV be ifcompute_uvisFalse?": {
        "answer": "zero-filled matrices of shape(m, m)and(n, n)respectively",
        "question": "What will the returnedUandV be ifcompute_uvisFalse?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the argumentsome have no effect whencompute_uvisFalse?": {
        "answer": "float, double, cfloat and cdouble data types",
        "question": "What does the argumentsome have no effect whencompute_uvisFalse?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the dtypes ofUandV always be?": {
        "answer": "real-valued",
        "question": "What will the dtypes ofUandV always be?",
        "context": "Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is deprecated in favor oftorch.linalg.svd()?": {
        "answer": "Warning torch.svd()",
        "question": "What is deprecated in favor oftorch.linalg.svd()?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What are the supportsinput of?": {
        "answer": "float, double, cfloat and cdouble data types",
        "question": "What are the supportsinput of?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the dtypes ofUandVare always be?": {
        "answer": "real-valued",
        "question": "What will the dtypes ofUandVare always be?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should U,S,V=torch.svd(A,some=some,compute_uv=True)": {
        "answer": "Note Differences",
        "question": "What should U,S,V=torch.svd(A,some=some,compute_uv=True)",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does whereastorch.linalg.svd()returnsVh?": {
        "answer": "torch.svd()returnsV",
        "question": "What does whereastorch.linalg.svd()returnsVh?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd()return forUandVh?": {
        "answer": "zero-filled tensors",
        "question": "What does torch.svd()return forUandVh?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "In what order are the singular values returned?": {
        "answer": "descending order",
        "question": "In what order are the singular values returned?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of a batch of matrices?": {
        "answer": "Ifinputis",
        "question": "What is the name of a batch of matrices?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Note TheStensor can only be used to what?": {
        "answer": "compute gradients",
        "question": "Note TheStensor can only be used to what?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What can be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What can be used to compute gradients ifcompute_uvisTrue?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd() return forUandVh?": {
        "answer": "zero-filled tensors",
        "question": "What does torch.svd() return forUandVh?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the tensor that can only be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What is the name of the tensor that can only be used to compute gradients ifcompute_uvisTrue?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "WhensomeisFalse, the gradients onU[..., :, min(m, n):]andV[...": {
        "answer": "the backward pass",
        "question": "WhensomeisFalse, the gradients onU[..., :, min(m, n):]andV[...",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What algorithm does the implementation oftorch.linalg.svd()on CPU use instead of?gesvdfor speed?": {
        "answer": "LAPACK",
        "question": "What algorithm does the implementation oftorch.linalg.svd()on CPU use instead of?gesvdfor speed?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Which CPU version of CUDA is a routinegesddon?": {
        "answer": "MAGMA",
        "question": "Which CPU version of CUDA is a routinegesddon?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of CUDA's routinegesdd?": {
        "answer": "Note",
        "question": "What is the name of CUDA's routinegesdd?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the gradients with respect toUandV only be when the input does not have zero nor repeated singular values?": {
        "answer": "finite",
        "question": "What will the gradients with respect toUandV only be when the input does not have zero nor repeated singular values?",
        "context": "Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will the gradients with respect toUandV be if the distance between two singular values is close to zero?": {
        "answer": "numerically unstable",
        "question": "What will the gradients with respect toUandV be if the distance between two singular values is close to zero?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the matrix have?": {
        "answer": "small singular values",
        "question": "What does the matrix have?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens when the matrix has small singular values?": {
        "answer": "Warning",
        "question": "What happens when the matrix has small singular values?",
        "context": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will be numerically unstable if the distance between two singular values is close to zero?": {
        "answer": "the gradients with respect toUandV",
        "question": "What will be numerically unstable if the distance between two singular values is close to zero?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "For what is the singular value decomposition not unique?": {
        "answer": "complex-valuedinput",
        "question": "For what is the singular value decomposition not unique?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens wheninputhas repeated singular values?": {
        "answer": "wheninputhas repeated singular values",
        "question": "What happens wheninputhas repeated singular values?",
        "context": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What may produce differentUandVtensors?": {
        "answer": "Different platforms",
        "question": "What may produce differentUandVtensors?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does a DLPack do to a tensor?": {
        "answer": "Decodes a DLPack to a tensor",
        "question": "What does a DLPack do to a tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is dlpack?": {
        "answer": "PyCapsule object",
        "question": "What is dlpack?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "How often can each dlpack be consumed?": {
        "answer": "once",
        "question": "How often can each dlpack be consumed?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What represents the tensor?": {
        "answer": "a DLPack",
        "question": "What represents the tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is a tensor to be exported?": {
        "answer": "tensor",
        "question": "What is a tensor to be exported?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What does a float tensor convert to?": {
        "answer": "per-channel quantized tensor",
        "question": "What does a float tensor convert to?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is input to quantize scales?": {
        "answer": "float 1D tensor",
        "question": "What is input to quantize scales?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What does a real square matrix compute?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What does a real square matrix compute?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is supported only if eigenvalues and eigenvectors are all real valued?": {
        "answer": "backward pass",
        "question": "What is supported only if eigenvalues and eigenvectors are all real valued?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Wheninputis on CUDA,torch.eig() causes what?": {
        "answer": "host-device synchronization",
        "question": "Wheninputis on CUDA,torch.eig() causes what?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is deprecated in favor oftorch.linalg.eig()?": {
        "answer": "torch.eig()",
        "question": "What is deprecated in favor oftorch.linalg.eig()?",
        "context": "torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What might be complex, but backward pass is supported only if eigenvalues and eigenvectors are all real valued?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "What might be complex, but backward pass is supported only if eigenvalues and eigenvectors are all real valued?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What should be replaced with wheninputis on CUDA,torch.eig()?": {
        "answer": "L,_=torch.eig(A)",
        "question": "What should be replaced with wheninputis on CUDA,torch.eig()?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What should replace L,_=torch.eig(A) with?": {
        "answer": "L,V=torch.eig(A,eigenvectors=True)",
        "question": "What should replace L,_=torch.eig(A) with?",
        "context": "Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the square matrix of shape(nn)(n times n)(nn)?": {
        "answer": "input(Tensor)",
        "question": "What is the square matrix of shape(nn)(n times n)(nn)?",
        "context": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the name of a namedtuple?": {
        "answer": "output tensors",
        "question": "What is the name of a namedtuple?",
        "context": "eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the eigenvalue ofinput?": {
        "answer": "the first element is the real part and the second element is the imaginary part",
        "question": "What is the eigenvalue ofinput?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What are the eigenvalues?": {
        "answer": "not necessarily ordered",
        "question": "What are the eigenvalues?",
        "context": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the output of out(tuple,optional)?": {
        "answer": "tensors",
        "question": "What is the output of out(tuple,optional)?",
        "context": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the second element?": {
        "answer": "the imaginary part",
        "question": "What is the second element?",
        "context": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What are the eigenvalues ordered?": {
        "answer": "not necessarily",
        "question": "What are the eigenvalues ordered?",
        "context": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the name of a namedtuple containing eigenvalues?": {
        "answer": "Shape",
        "question": "What is the name of a namedtuple containing eigenvalues?",
        "context": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the second element in a namedtuple?": {
        "answer": "imaginary part",
        "question": "What is the second element in a namedtuple?",
        "context": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the term for eigenvalues?": {
        "answer": "Shape",
        "question": "What is the term for eigenvalues?",
        "context": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the second element in eigenvalues?": {
        "answer": "imaginary part",
        "question": "What is the second element in eigenvalues?",
        "context": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    }
}